model based policy search approach reinforcement learning rl policies found using model simulator markov decision process dimensional continuous tasks extremely difficult build accurate model algorithm returns policy simulation real life extreme model free rl tends require infeasibly real life trials paper hybrid algorithm requires approximate model real life trials key idea successively ground policy evaluations using real life trials rely approximate model suggest local changes theoretical results algorithm achieves near optimal performance real system model approximate empirical results demonstrate crude model real life trials algorithm obtain near optimal performance real system

experimentally study line investment algorithms proposed agarwal hazan extended hazan et al achieve wealth constant rebalanced portfolio determined hindsight algorithms combine optimal logarithmic regret bounds efficient deterministic computability based newton method offline optimization unlike previous approaches exploits information analyzing algorithm using potential function introduced agarwal hazan extensive experiments actual financial data experiments confirm theoretical advantage algorithms yield returns run considerably faster previous algorithms optimal regret additionally perform financial analysis using mean variance calculations sharpe ratio

recently considerable learning relations i.e unsupervised semi supervised settings hypergraphs tensors proposed natural representing relations corresponding algebra natural tools operating paper argue hypergraphs natural representation relations indeed pairwise relations handled using graphs various formulations semi supervised unsupervised learning hypergraphs result graph theoretic analyzed using existing tools

ranking examples relationships objects goal learn examples real valued ranking function induces ranking object space consider learning ranking function data represented graph vertices correspond objects edges encode similarities objects building recent developments regularization theory graphs corresponding laplacian based methods classification develop algorithmic framework learning ranking functions graph data provide generalization guarantees algorithms via recent results based notion algorithmic stability experimental evidence potential benefits framework

principal components canonical correlations root exploratory data mining techniques provide standard pre processing tools machine learning lately probabilistic reformulations methods proposed roweis 1998 tipping bishop 1999b bach jordan 2005 based gaussian density model probabilistic counterpart sensitive atypical observations paper introduce robust probabilistic principal component analysis robust probabilistic canonical correlation analysis based student density model resulting probabilistic reformulations suitable practice handle outliers natural compute maximum likelihood estimates parameters means em algorithm

address learning kernel supervised learning task approach consists searching convex hull prescribed set basic kernels minimizes convex regularization functional unique feature approach compared literature basic kernels infinite require continuously parameterized example basic kernels isotropic gaussians variance prescribed interval gaussians parameterized multiple continuous parameters builds formulation involving minimax optimization recently proposed greedy algorithm learning kernel optimization convex belongs larger class dc difference convex functions programs apply recent results dc optimization theory create algorithm learning kernel experimental results benchmark data sets algorithm outperforms previously proposed method

introduce relational temporal difference learning effective approach solving multi agent markov decision spaces algorithm temporal difference reinforcement learn distributed value function represented conceptual hierarchy relational predicates experiments using domains game playing repository observe system achieves learning rates relational methods discuss related directions future research

consider clustering basic form local metric data space parametric statistical model assumed clusters learned data introduce analyze demonstrate novel approach clustering data viewed nodes graph pairwise similarities derive transition probability matrix markov random walk algorithm automatically reveals structure increasing scales varying steps random walk represented rows sup sup step distributions walk starting distributions clustered using kl minimizing iterative algorithm clusters steps reveal found optimizing spectral properties

analyze active learning algorithm presence arbitrary forms noise algorithm sup 2 sup agnostic active relies assumption samples drawn i.i.d fixed distribution sup 2 sup achieves exponential improvement i.e requires ln 1 epsilon samples epsilon optimal classifier usual sample complexity supervised learning settings considered realizable include learning threshold classifiers learning homogeneous linear separators respect input distribution uniform unit sphere

kernel functions extremely popular tool machine learning attractive theory theory views kernel implicitly mapping data possibly dimensional space describes kernel function learning data separable margin implicit space elegant theory directly correspond one's intuition kernel similarity function furthermore difficult domain expert theory help design appropriate kernel learning task hand implicit mapping easy calculate finally requirement positive semi definiteness rule natural pairwise similarity functions domain.in develop alternative theory learning similarity functions i.e sufficient conditions similarity function allow learn require reference implicit spaces require function positive semi definite symmetric results generalize standard theory sense kernel function usual definition shown similarity function definition loss parameters provide steps towards theory kernels describes effectiveness kernel function terms natural similarity based properties

bayesian bounds studied machine learning batch online setting arise application simple compression lemma particular derive pac bayesian bounds batch setting ii bayesian log loss bounds iii bayesian bounded loss bounds online setting using compression lemma setting semantics prior posterior loss core bound argument paper simplifies understanding apparently disparate results brings light powerful tool developing similar arguments methods

consider fitting scale covariance matrix multivariate gaussian data inverse sparse providing model selection beginning dense empirical covariance matrix solve maximum likelihood inf 1 inf norm penalty term added encourage sparsity inverse models tens nodes resulting solved using standard interior algorithms convex optimization methods scale poorly size algorithms aimed solving thousand nodes based nesterov's algorithm yields rigorous complexity estimate dependence size interior methods algorithm block coordinate descent updating row columns covariance matrix sequentially experiments genomic data method able uncover biologically interpretable connections genes

tree data structure fast nearest neighbor operations metric spaces data set consists data structure requires space regardless metric's structure maintains performance properties navigating net krauthgamer lee 2004b set bounded expansion constant measure intrinsic dimensionality defined karger ruhl 2002 cover tree data structure constructed sup 6 sup log time furthermore nearest neighbor queries require time logarithmic particular sup 12 sup log time experimental results speedups brute force search varying magnitude natural machine learning datasets

recent proliferation theoretical graph models e.g preferential attachment world models motivated real world graphs internet topology address natural question model particular data set propose model selection criterion graph models model probability distribution graphs suggest using maximum likelihood compare graph models select parameters interestingly graph models computing likelihoods difficult algorithmic task design implement mcmc algorithms computing maximum likelihood popular models power law random graph model preferential attachment model world model uniform random graph model hope novel ml objectify comparisons graph models

family probabilistic time series models developed analyze time evolution topics document collections approach space models natural parameters multinomial distributions represent topics variational approximations based kalman filters nonparametric wavelet regression developed carry approximate posterior inference latent topics addition giving quantitative predictive models sequential corpus dynamic topic models provide qualitative window contents document collection models demonstrated analyzing ocr'ed archives journal science 1880 2000

estimation distribution algorithms edas popular approach learn probability distribution solutions combinatorial optimization consider collection optimization learned distributions characterized vector features define machine learning predict distribution solutions features denotes solution predictive distribution focus search demonstrate utility method compiler optimization task goal sequence code transformations code run fastest results set 12 benchmarks distinct architectures approach consistently leads significant improvements performance

predictive representations psrs powerful models markovian decision processes traditional models e.g hmms pomdps representing using observable quantities psrs learned solely using data interaction process majority existing techniques explicitly implicitly require data gathered using blind policy actions selected independently preceding observations severe limitation practical learning psrs methods fixing limitation existing psr algorithms policy efficient optimization computing exploration policies learning psr exploration policies blind significantly lower amount data build accurate model demonstrating importance blind policies

applications unlabelled examples inexpensive easy obtain semi supervised approaches try utilise examples reduce predictive error paper investigate semi supervised squares regression algorithm based co learning approach similar semi supervised algorithms base algorithm cubic runtime complexity unlabelled examples able handle larger sets unlabelled examples devise semi parametric variant scales linearly unlabelled examples experiments significant error reduction co regularisation runtime improvement semi parametric approximation propose distributed procedure applied collecting data single site

learning mapping input structured interdependent output variables covers sequential spatial relational learning predicting recursive structures joint feature representations input output variables paved leveraging discriminative learners svms class address semi supervised learning joint input output spaces co training approach based principle maximizing consensus multiple independent hypotheses develop principle semi supervised support vector learning algorithm joint input output spaces arbitrary loss functions experiments investigate benefit semi supervised structured models terms accuracy f1 score

revisit gaussian blurring mean shift gbms procedure iteratively sharpens dataset moving data according gaussian mean shift algorithm gms 1 criterion stop procedure soon clustering structure arisen reliably produces image segmentations gms faster 2 prove gbms convergence cubic gaussian clusters faster gms's linear local principal component converges explains powerful clustering denoising properties gbms 3 connection spectral clustering suggests gbms faster 4 accelerate gbms interleaving connected components blurring steps achieving 2x 4x speedups introducing approximation error summary accelerated gbms simple fast nonparametric algorithm achieves segmentations art quality

supervised learning methods introduced decade unfortunately comprehensive empirical evaluation supervised learning statlog project 90 scale empirical comparison ten supervised learning methods svms neural nets logistic regression naive bayes memory based learning random forests decision trees bagged trees boosted trees boosted stumps examine effect calibrating models via platt scaling isotonic regression performance aspect study variety performance criteria evaluate learning methods

derive robust euclidean embedding procedure based semidefinite programming popular classical multidimensional scaling cmds algorithm motivate algorithm arguing cmds particularly robust deficiencies purpose semidefinite programming solvers memory intensive medium sized applications describe fast subgradient based implementation robust algorithm additionally cmds dimensionality reduction provide depth look reducing dimensionality embedding procedures particular np hard optimal low dimensional embeddings variety cost functions

study hierarchical classification instance belong class node underlying taxonomy experiments previous simple hierarchy support vectors machines svm top evaluation scheme surprisingly performance task paper introduce refined evaluation scheme hierarchical svm classifier approximator bayes optimal classifier respect simple stochastic model labels experiments synthetic datasets generated according stochastic model refined algorithm outperforms simple hierarchical svm real world data advantage brought approach bit conjecture due noise rate training labels low levels taxonomy

semi supervised support vector machines sup 3 sup vms appealing method using unlabeled data classification objective function favors decision boundaries cut clusters main optimization convex local minima results suboptimal performances paper propose global optimization technique continuation alleviate compared algorithms minimizing objective function continuation method leads lower test errors

paper focuses kernel methods multi instance learning existing methods require prediction bag identical maximum individual instances restrictive sign classification paper provide complete regularization framework mi learning allowing loss functions outputs bag associated instances especially generalize multi instance regression moreover bag instance information directly optimization instead using heuristics solve resultant linear optimization constrained concave convex procedure studied convergence properties experiments classification regression data sets proposed method leads improved performance

convex learning algorithms support vector machines svms seen highly desirable offer strong practical properties amenable theoretical analysis convexity provide scalability advantages convexity concave convex programming applied produce faster svms training errors support vectors ii faster transductive svms

principal agent principal seeks motivate agent action beneficial principal spending little reward complicated principal agent's utility function type study online setting round principal encounters agent principal sets rewards anew round principal action agent type principal learn set rewards optimally setting generalizes setting selling digital online.we study experimentally compare main approaches apply standard bandit algorithm setting distribution agent types fixed unknown principal introduce gradient ascent algorithm third distribution agents types fixed principal prior belief distribution limited class type distributions study bayesian approach

paper introduce rl cd method solving reinforcement learning stationary environments method based mechanism creating updating selecting partial models environment partial models incrementally built according system's capability predictions regarding sequence observations propose formalize efficiency method simple stationary environment noisy scenario rl cd performs standard reinforcement learning algorithms advantages methods specifically designed cope stationarity finally limitations method future

locally adaptive classifiers usually superior single global classifier major designing locally adaptive classifiers local classifiers combine paper instead placing classifiers based data distribution propose responsibility mixture model uncertainty associated classification training sample using model local classifiers placed near decision boundary effective set local classifiers learned form global classifier maximizing estimate probability samples correctly classified nearest neighbor classifier experimental results artificial real world data sets demonstrate superiority traditional algorithms

receiver operator characteristic roc curves commonly results binary decision machine learning dealing highly skewed datasets precision recall pr curves informative picture algorithm's performance deep connection exists roc space pr space curve dominates roc space dominates pr space corollary notion achievable pr curve properties convex hull roc space efficient algorithm computing curve finally note differences types curves significant algorithm design example pr space incorrect linearly interpolate furthermore algorithms optimize roc curve guaranteed optimize pr curve

clustering widely statistical tools data analysis existing clustering techniques means popular method ease programming accomplishes trade achieved performance computational complexity means prone local minima scale dimensional data sets common approach dealing dimensional data cluster space spanned principal components pc paper benefits clustering low dimensional discriminative space pc space generative particular propose clustering algorithm called discriminative cluster analysis dca dca jointly performs dimensionality reduction clustering toy real examples benefits dca versus traditional pca means clustering additionally matrix formulation proposed connections related techniques spectral graph methods linear discriminant analysis provided

fast gradient based methods maximum margin matrix factorization mmmf recently shown promise rennie srebro 2005 including significantly outperforming previous art methods standard collaborative prediction benchmarks including movielens paper investigate improve performance mmmf casting ensemble approach explore evaluate variety alternative define ensembles resulting ensembles perform significantly single mmmf model multiple evaluation metrics ensembles partially trained mmmf models sometimes predictions total training time comparable single mmmf model

recent decision theoric planning algorithms able optimal solutions using factored markov decision processes fmdps algorithms perfect knowledge structure paper propose sdyna framework addressing reinforcement learning trial error initial knowledge structure sdyna integrates incremental planning algorithms based fmdps supervised learning techniques building structured representations describe spiti instantiation sdyna incremental decision tree induction learn structure combined incremental version structured value iteration algorithm spiti build factored representation reinforcement learning improve policy faster tabular reinforcement learning algorithms exploiting generalization property decision tree induction algorithms

address efficiently learning naive bayes classifiers class conditional classification noise cccn naive bayes classifiers rely hypothesis distributions associated class product distributions data subject ccc noise conditional distributions themselves mixtures product distributions analytical formulas makes identify data subject cccn design learning algorithm based formulas able learn naive bayes classifiers cccn results artificial datasets datasets extracted uci repository database results cccn efficiently successfully handled

preference learning focused pairwise preferences rankings individual items paper method learning preferences sets items learning method takes input collection positive examples sets identified user desirable kernel density estimation estimate value function individual items desired set diversity estimated average set diversity observed collection learning introduce evaluation methodology evaluate learning method data collections synthetic blocks world data real world music data collection gathered

principal component analysis pca minimizes sum squared errors inf 2 inf norm sensitive presence outliers propose rotational invariant inf 1 inf norm pca inf 1 inf pca inf 1 inf pca similar pca 1 unique global solution 2 solution principal eigenvectors robust covariance matrix re weighted soften effects outliers 3 solution rotational invariant properties shared inf 1 inf norm pca subspace iteration algorithm compute inf 1 inf pca efficiently experiments real life datasets inf 1 inf pca effectively handle outliers extend inf 1 inf norm means clustering inf 1 inf norm means leads poor results inf 1 inf means outperforms standard means

dirichlet compound multinomial dcm distribution called multivariate polya distribution model text documents takes account burstiness word occurs document occur repeatedly derive family distributions approximations dcm distributions constitute exponential family unlike dcm distributions called edcm distributions obtain insights properties dcm distributions derive algorithm edcm maximum likelihood training times faster corresponding method dcm distributions investigate expectation maximization edcm components deterministic annealing clustering algorithm documents experiments algorithm competitive methods literature superior view finding models low perplexity

simple statistical model molecular function evolution predict protein function model description encodes knowledge molecular function evolves phylogenetic tree based proteins sequence inputs phylogeny set evolutionarily related protein sequences available function characterizations proteins posterior probabilities protein predict molecular function protein results applying model protein families compare prediction results extant proteins available protein function prediction methods deaminase family method achieves 93.9 related methods blast achieves 72.7 gotcha achieves 87.9 orthostrapper achieves 72.7 prediction accuracy

transition probabilities rewards markov decision process specified exactly solved interaction environment specification available agent's recourse potentially dangerous exploration framework allows expert specify imprecise knowledge transition probabilities terms stochastic dominance constraints algorithm optimal policies qualitatively specified solution available decrease required amount exploration algorithm's behavior demonstrated simulations classic mountain car ascent cart pole balancing

describe framework online multiclass learning based notion hypothesis sharing framework sets classes associated hypotheses classes set share hypothesis framework includes special commonly constructions multiclass categorization allocating unique hypothesis class allocating single common hypothesis classes generalize multiclass perceptron framework derive unifying mistake bound analysis construction naturally extends settings classes advance revealed online learning process demonstrate merits approach comparing previous methods synthetic natural datasets

consider sparse grid combination technique regression regard function reconstruction function space regularised squares approach discretised sparse grids solved using called combination technique sequence conventional grids employed sparse grid solution obtained addition partial solutions combination co efficients dependent involved grids approach instabilities situations guaranteed converge discretisation levels article apply recently introduced optimised combination technique repairs instabilities combination coefficients depend function reconstructed resulting linear approximation method achieves competitive results computational complexity improved method scales linear regard data

report mixtures multinomial logistic regression approximate class smooth probability models multiclass responses bounded derivatives log odds approximation rate sup 2 sup hellinger distance sup 4 sup kullback leibler divergence dim dimension input space predictors availability training data size consistency multiclass regression classification achieved simultaneously classes posterior based inference performed bayesian framework loosely speaking consistency refers performance close consistency achieved taking inf inf taking uniformly distributed 1 inf inf according prior 1 pr inf inf pr sup sup grows isin 0 1

probabilistic modelling text data bag words representation dominated directed graphical models plsi lda nmf discrete pca recently art performance visual object recognition reported using variants models introduce alternative undirected graphical model suitable modelling count data rate adapting poisson rap model shown generate superior dimensionally reduced representations subsequent retrieval classification models trained using contrastive divergence inference latent topical representations efficiently achieved simple matrix multiplication

extend tree based methods prediction structured outputs using kernelization algorithm allows grow trees soon kernel defined output space resulting algorithm called output kernel trees ok3 generalizes classification regression trees tree based ensemble methods principled inherits features methods interpretability robustness irrelevant variables input scalability gram matrix outputs learning sample learns output kernel function inputs proposed algorithm image reconstruction task biological network inference

constructing classifier labeled data assign weight single input feature increase robustness classifier particularly domains nonstationary feature distributions input sensor failures common approach achieving robustness introduce regularization spreads weight features strategy generic induce robustness specifically tailored classification task hand introduce algorithm avoiding single feature weighting analyzing robustness using game theoretic formalization develop classifiers optimally resilient deletion features minimax sense construct classifiers using quadratic programming illustrate applicability methods spam filtering handwritten digit recognition tasks feature deletion indeed realistic noise model

elimination aspects eba probabilistic choice model describing humans decide options options choice characterized binary features associated weights instance choosing mobile phone buy features consider lasting battery color screen existing methods inferring parameters model assume pre specified features features lead observed choices parametric bayesian model infer features options corresponding weights choice data indian buffet process ibp prior features inference using markov chain monte carlo mcmc conjugate ibp models previously described main contribution paper mcmc algorithm eba model inference conjugate ibp models broaden ibp priors considerably

real world sequence learning tasks require prediction sequences labels noisy unsegmented input data speech recognition example acoustic signal transcribed words sub word units recurrent neural networks rnns powerful sequence learners suited tasks require pre segmented training data post processing transform outputs label sequences applicability limited paper novel method training rnns label unsegmented sequences directly thereby solving experiment timit speech corpus demonstrates advantages baseline hmm hybrid hmm rnn

supervised kernel methods observed performance svm classifier poor diagonal entries gram matrix relative diagonal entries referred diagonal dominance occurs kernel functions applied sparse dimensional data text corpora paper investigate implications diagonal dominance unsupervised kernel methods specifically task document clustering propose selection strategies addressing issue evaluate effectiveness producing accurate stable clusterings

kernel based learning algorithms support vector machines svms perceptron rely sequential optimization examples added iteration updating kernel matrix usually requires matrix vector multiplications propose method based transposition speedup computation sparse data instead dot products sparse feature vectors computation incrementally merges lists training examples minimizes access data caching shrinking optimized sparsity natural language tasks tagging translation text classification sparse feature representations 20 80 fold speedup libsvm observed using smo algorithm theory experiments explain type sparsity structure approach adaptation maxent sequential optimization inefficient

consider setting transductive learning vertex labels graphs graph vertices sampled according unknown distribution true labeling vertices vertex assigned exactly classes labels random subset vertices revealed learner task labeling remaining unlabeled vertices agrees true labeling existing algorithms based assumption adjacent vertices usually labeled understand algorithms based assumption derive data dependent bounds fraction mislabeled vertices based total weight edges vertices differing predicted label i.e size cut

sample classification svm considerably enhanced using kernel function learned training data prior discrimination kernel shown enhance retrieval based data similarity specifically describe kernelboost boosting algorithm computes kernel function combination weak space partitions kernel learning method naturally incorporates domain knowledge form unlabeled data i.e semi supervised transductive settings form labeled samples relevant related i.e learning learn scenario latter goal accomplished learning single kernel function classes comparative evaluations method datasets uci repository demonstrate performance enhancement challenging tasks digit classification kernel svm facial image retrieval based image similarity measured learnt kernel

solution inferring hidden sensorimotor experience environment takes form pomdp deterministic transition observation functions environments appear arbitrarily complex deterministic surface actually deterministic respect unobserved underlying exists finite history based representation captures unobserved world allowing perfect prediction action effects representation takes form looping prediction suffix tree pst derive sound complete algorithm learning looping pst sufficient sample sensorimotor experience empirical illustrations advantages conferred approach characterize approximations looping pst existing algorithms variable length markov models utile suffix memory causal splitting reconstruction

goal active learning select informative examples manual labeling previous studies active learning focused selecting single unlabeled example iteration inefficient classification model retrained labeled example paper framework batch mode active learning applies fisher information matrix select informative examples simultaneously key computational challenge efficiently identify subset unlabeled examples result largest reduction fisher information resolve challenge propose efficient greedy algorithm based property submodular functions empirical studies five uci datasets real world medical image classification proposed batch mode active learning algorithm effective art algorithms active learning

paper proposes approaches rank individuals competition results real world type example ranking players team games sports propose exponential model solve estimate individual rankings proposed model introduce convex minimization formulas easy efficient solution procedures experiments real bridge records multi class classification demonstrate viability proposed model

introduce hidden process models hpms class probabilistic models multivariate time series data design hpms motivated challenges modeling hidden cognitive processes brain functional magnetic resonance imaging fmri data fmri data sparse dimensional markovian involves prior knowledge form hidden event occurs times interval prime hpms provide generalization widely linear model approaches fmri analysis hpms viewed subclass dynamic bayes networks

data compression independently obtained hypotheses various tasks algorithmically provide guarantees tasks sufficiently related benefit multitask learning uniform bounds terms empirical average error true average error hypotheses provided deterministic learning algorithms drawing independent samples set unknown computable task distributions finite sets

address automatically constructing basis functions linear approximation value function markov decision process mdp builds results bertsekas casta ntilde 1989 proposed method automatically aggregating speed value iteration propose neighborhood component analysis goldberger et al 2005 dimensionality reduction technique created supervised learning map dimensional space low dimensional space based bellman error temporal difference td error basis function lower dimensional space added features linear function approximator approach applied dimensional inventory control

approach personalized handwriting recognition writer adaptation consists converting generic user independent recognizer personalized user dependent improved recognition rate particular user adaptation step usually involves user specific samples leads fundamental question fuse information captured generic recognizer propose adapting recognizer minimizing regularized risk functional modified svm prior knowledge generic recognizer enters modified regularization term result simple personalization framework practical properties experiments 100 class real world data set errors reduced 40 five user samples character

kernel fisher discriminant analysis kfda carry fisher linear discriminant analysis dimensional feature space defined implicitly kernel performance kfda depends choice kernel paper consider finding optimal kernel convex set kernels optimal kernel selection reformulated tractable convex optimization interior methods solve globally efficiently kernel selection method demonstrated uci machine learning benchmark examples

consider choosing linear classifier minimizes misclassification probabilities class classification bi criterion involving trade objectives assume class conditional distributions gaussian assumption makes computationally tractable pareto optimal linear classifiers classification capabilities inferior linear ones main purpose paper establish robustness properties classifiers respect variations uncertainties distributions extend results kernel based classification finally carry trade analysis empirically finite labeled data

propose efficient particle smoothing methods generalized spaces models particle smoothing expensive sup 2 sup algorithm particles overcome integrating dual tree recursions fast multipole techniques forward backward smoothers generalized filter smoother maximum posteriori map smoother experiments improvements substantially increase practicality particle smoothing

introduce learned shaping rewards reinforcement learning tasks agent prior experience sequence tasks learn portable predictor estimates intermediate rewards resulting accelerated learning tasks related distinct agents trained sequence relatively easy tasks develop informative measure reward transferred improve performance difficult tasks requiring hand coded shaping function rod positioning task significantly improves performance brief training period

unified model traditionally viewed separate tasks data association intensity tracking multiple topics time data association task assign topic class data intensity tracking models bursts changes intensities topics time approach combines extension factorial hidden markov models topic intensity tracking exponential statistics implicit data association experiments text email datasets interplay classification topic intensity tracking improves accuracy classification intensity tracking little noise topic assignments mislead traditional algorithms approach detects correct topic intensities 30 topic noise

kernel learning plays role machine learning tasks algorithms learning kernel matrix scale poorly running times cubic data paper propose efficient algorithms learning low rank kernel matrices algorithms scale linearly data quadratically rank kernel introduce employ bregman matrix divergences rank deficient matrices divergences natural preserve rank positive semi definiteness kernel matrix special framework yield faster algorithms various existing kernel learning experimental results demonstrate effectiveness algorithms learning low rank rank kernels

gaussian process latent variable model gp lvm generative approach nonlinear low dimensional embedding provides smooth probabilistic mapping latent data space linear generalization probabilistic pca ppca tipping bishop 1999 approaches linear dimensionality methods focus preserving local distances data space gp lvm focusses exactly opposite smooth mapping latent data space focusses keeping apart latent space apart data space paper provide overview dimensionality reduction techniques placing emphasis distance relation preserved gp lvm generalized constraints additionally preserve local distances illustrative experiments common data sets

appropriately prior knowledge significantly improve predictive accuracy learning algorithms reduce amount training data paper introduce simple method incorporate prior knowledge support vector machines modifying hypothesis space optimization optimization amenable solution constrained concave convex procedure local optimum paper discusses kinds prior knowledge demonstrates applicability approach characteristic experiments

today's classification results obtained combining responses set base classifiers produce answer query paper explores novel query specific combination rule learning set simple belief network classifiers produce answer query combining individual responses using weights based inversely respective variances responses variances based uncertainty network parameters depend training datasample essence variance quantifies base classifier's confidence response query experimental results mixture using variance belief net classifiers muvs effectively especially base classifiers learned using balanced bootstrap samples results combined using james stein shrinkage found variance based combination rule performed bagging adaboost set base classifiers produced adaboost finally framework extremely efficient learning classification components require straight line code

paper explores kernels context text classification novel view documents created introduced kernels derived framework relations kernels gaussian kernel discussed moreover popular tf idf weighting scheme derived natural consequence finally kernels evaluated reuters corpus volume newswire database assess quality topic classification application

efficient method maximizing energy functions potentials suitable map labeling estimation arise undirected graphical models approach relax integer constraints solution steps efficiently obtain relaxed global optimum following procedure similar iterative power method finding largest eigenvector matrix map relaxed optimum simplex energy obtained optimal bound starting energy follow efficient coordinate ascent procedure guaranteed increase energy step converge solution obeys initial integral constraints sufficient condition ascent procedures guarantees increase energy step

power popularity kernel methods stem ability handle diverse forms structured inputs including vectors graphs strings recently methods proposed combining kernels heterogeneous data sources methods produce stationary combinations i.e relative weights various kernels vary input examples article proposes method combining multiple kernels nonstationary fashion approach margin latent variable generative model maximum entropy discrimination med framework latent parameter estimation rendered tractable variational bounds iterative optimization procedure classifier log ratio gaussian mixtures component implicitly mapped via mercer kernel function support vector machine special model approach discriminative parameter estimation feasible via fast sequential minimal optimization algorithm empirical results synthetic data benchmarks protein function annotation task

approximate region based value iteration rbvi algorithm proposed optimal policy partially observable markov decision process pomdp proposed rbvi approximates true polyhedral partition belief simplex ellipsoidal partition optimal value function linear ellipsoidal regions position shape region gradient alpha vector optimal value function region parameterized explicitly estimated via efficient expectation maximization em variational bayesian em vbem based set selected sample belief rbvi maintains alpha vectors based methods yields parsimonious representation approximates true value function maximum likelihood ml sense results benchmark proposed rbvi comparable performance art algorithms despite alpha vectors

multiclass classification reduced collection binary aid coding matrix quality final solution ensemble base classifiers learned binary affected performance base learner error correcting ability coding matrix coding matrix strong error correcting ability overall optimal binary hard base learner trade error correcting base learning sought paper propose multiclass boosting algorithm modifies coding matrix according learning ability base learner experimentally algorithm efficient optimizing multiclass margin cost outperforms existing multiclass algorithms adaboost.ecc vs improvement especially significant base learner powerful

latent dirichlet allocation lda related topic models increasingly popular tools summarization manifold discovery discrete data lda capture correlations topics paper introduce pachinko allocation model pam captures arbitrary nested possibly sparse correlations topics using directed acyclic graph dag leaves dag represent individual words vocabulary interior node represents correlation children words interior nodes topics pam provides flexible alternative recent blei lafferty 2006 captures correlations pairs topics using text data newsgroups historic nips proceedings research paper corpora improved performance pam document classification likelihood held data ability support finer grained topics topical keyword coherence

clustering multi type relational data attracted attention recent due impact various applications web mining commerce bioinformatics research multi type relational data clustering limited preliminary contribution paper fold propose model collective factorization related matrices multi type relational data clustering model applicable relational data various structures model derive novel algorithm spectral relational clustering cluster multi type interrelated data objects simultaneously algorithm iteratively embeds type data objects low dimensional spaces benefits interactions hidden structures types data objects extensive experiments demonstrate promise effectiveness proposed algorithm third existing spectral clustering algorithms considered special proposed model algorithm demonstrates theoretic generality proposed model algorithm

central subspace clustering methods core segmentation computer vision methods fail correct segmentation practical scenarios e.g data close intersection subspaces cluster centers subspaces spatially close paper address challenges considering clustering set lying union subspaces distributed multiple cluster centers inside subspace propose generalization kmeans ksubspaces clusters data minimizing cost function combines central subspace distances experiments synthetic data compare algorithm favorably clustering methods test algorithm computer vision clustering varying illumination video shot segmentation dynamic scenes

policy evaluation critical step approximate solution markov decision processes mdps typically requiring sup 3 sup directly solve bellman system linear equations space size discrete sample size continuous paper apply recently introduced multiscale framework analysis graphs design faster algorithm policy evaluation fixed policy pi framework efficiently constructs multiscale decomposition random walk pi associated policy pi enables efficiently computing medium term distributions approximation value functions direct computation potential operator gamma sup pi sup sup 1 sup solve bellman's equation preliminary optimized version solver competes highly optimized iterative techniques requiring complexity

novel ensemble pruning method based reordering classifiers obtained bagging selecting subset aggregation classifiers generated bagging makes build subensembles increasing size including classifiers expected perform aggregated ensemble pruning achieved halting aggregation process classifiers generated included ensemble pruned subensembles containing 15 30 initial pool classifiers besides improve generalization performance bagging ensemble classification investigated

paper neighborhood markov random fields learn rich prior models color images approach extends monochromatic fields experts model roth black 2005a color images fields experts model curse dimensionality due clique sizes circumvented parameterizing potential functions according product experts introduce simplifications original approach roth black allow cope increased clique size typically 3x3x3 5x5x3 pixels color images experimental results image denoising evidence improvements art monochromatic image priors

found clustering data set prove unknown clustering sup opt sup data surprisingly answer question sometimes yes goodness measured distortion means clustering paper proves spectral bounds distance sup opt sup bounds exist data admits low distortion clustering

describe family embedding algorithms based nonparametric estimates mutual information mi using parzen window estimates distribution joint input embedding space derive mi based objective function dimensionality reduction optimized directly respect set latent data representatives various types supervision signal introduced framework replacing plain mi forms conditional mi examples semi un supervised algorithms obtain model manifold alignment type embedding method performs conditional dimensionality reduction

discrete spectral framework sparse cardinality constrained solution generalized rayleigh quotient np hard combinatorial optimization central supervised learning tasks sparse lda feature selection relevance ranking classification derive generalized form inclusion principle variational eigenvalue bounds leading exact optimal sparse linear discriminants using branch bound search efficient greedy approximate technique generalization performance sparse lda algorithms demonstrated real world uci ml benchmarks compared leading svm based gene selection algorithm cancer classification

consider alice bob shared secret helps alice identify bob impersonators eve secret eve impersonate bob fool alice eve computationally unbounded observe bob impersonate strategy eve cryptographic functions exist efficient eve impersonate simple bobs exist eve learn impersonate efficient bob formalize questions computational learning model believe captures wide variety natural learning tasks tightly bound observations eve makes terms secret's entropy functions exist efficient eve learn impersonate efficient bob nearly unbounded eve.for version naor rothblum 2006

viterbi algorithm efficient optimal method decoding linear chain markov models entire input sequence observed labels time step generated viterbi directly applied online interactive streaming scenarios incurring significant possibly unbounded latency widely approach break input stream fixed size windows apply viterbi window larger windows lead accuracy result latency.we propose alternative algorithms fixed sized window decoding approach approaches compute certainty measure predicted labels allows trade latency expected accuracy dynamically choose fixed window size front surprisingly principled approach substantial improvement choosing fixed window effectiveness approach task spotting semi structured information documents compared viterbi approach suffers 0.1 percent error degradation average latency 2.6 time steps versus potentially infinite latency viterbi compared fixed windows viterbi achieve 40x reduction error 6x reduction latency

knowledge based planning methods offer benefits classical techniques time consuming costly construct research learning plan knowledge search substantial computer time fail solutions complex tasks describe approach observes sequences operators expert solutions learns hierarchical task networks method similarities previous algorithms explanation based learning differs ability acquire hierarchical structures generality learned conditions increase method's capability transfer learned knowledge supports acquisition recursive procedures learning algorithm report experiments compare abilities techniques planning domains closing review related directions future research

scale empirical application reinforcement learning optimized trade execution modern financial markets experiments based 1.5 millisecond time scale limit data nasdaq demonstrate promise reinforcement learning methods market microstructure learning algorithm introduces exploits natural low impact factorization space

support vector machines svms suffer sup 2 sup training cost denotes training instances paper propose algorithm select boundary instances training data substantially reduce proposed algorithm motivated result burges 1999 removing support vectors training set change svm training results algorithm eliminates instances support vectors concept independent preprocessing step algorithm prepare nearest neighbor lists training instances concept specific sampling step effectively select useful training data target concept empirical studies algorithm effective reducing outperforming competing downsampling algorithms significantly compromising testing accuracy

machine learning tens thousands features dozens hundreds independent training examples dimensionality reduction essential learning performance previous researchers treated learning separate phases algorithm singular value decomposition reduce dimensionality data set classification algorithm na iuml ve bayes support vector machines learn classifier demonstrate combine goals dimensionality reduction classification single learning objective novel efficient algorithm optimizes objective directly experimental results fmri analysis achieve learning performance lower dimensional representations phase approaches

reinforcement learning rl originally proposed framework allow agents learn online fashion interact environment existing rl algorithms short achieving goal amount exploration required costly time consuming online learning result rl offline learning simulated environments propose algorithm called beetle effective online learning computationally efficient minimizing amount exploration bayesian model based approach framing rl partially observable markov decision process main contributions analytical derivation optimal value function upper envelope set multivariate polynomials efficient based value iteration algorithm exploits simple parameterization

applying multiple instance mi learning content based image retrieval cbir goal rank images repository using labeled data set existing mi learning algorithms transductive images repository serve test data learning process missl multiple instance semi supervised learning transforms mi input graph based single instance semi supervised learning method encodes mi aspects simultaneously bag levels unlike prior mi learning algorithms missl makes unlabeled data

applications supervised learning require generalization limited labeled data bayesian setting try achieve goal using informative prior parameters encodes useful domain knowledge focusing logistic regression algorithm automatically constructing multivariate gaussian prior covariance matrix supervised learning task prior relaxes commonly overly simplistic independence assumption allows parameters dependent algorithm similar learning estimate covariance pairs individual parameters semidefinite program combine estimates learn prior current learning task apply methods binary text classification demonstrate 20 40 test error reduction commonly prior

address issue learnability concept classes classification noise models probably approximately correct framework introducing class conditional classification noise cccn model investigate learnability concept classes particular setting concept classes learnable uniform classification noise cn setting cccn learnable cn cccn result prove equality set concept classes cn learnable set concept classes learnable constant partition classification noise cpcn setting words cn cpcn

imitation learning sequential goal directed behavior standard supervised techniques difficult frame learning behaviors maximum margin structured prediction space policies approach learn mappings features cost optimal policy mdp cost mimics expert's behavior demonstrate simple provably efficient approach structured maximum margin learning based subgradient method leverages existing fast algorithms inference technique particularly relevant dynamic programming approaches learning policies tractable beyond limitations qp formulation demonstrate approach applied route planning outdoor mobile robots behavior designer wishes planner execute specifying cost functions engender behavior difficult task

quadratic program relaxations proposed alternative linear program relaxations tree reweighted belief propagation metric labeling map estimation additional convex relaxation quadratic approximation shown additive approximation guarantees apply graph weights mixed sign metric approximations extended manner allows tight variational relaxations map involve convex optimization experiments carried synthetic data quadratic approximations accurate computationally efficient linear programming propagation based alternatives

explore situation documents categorized category system situation refer multiple view categorization particularly address categorizers built based necessarily identical training sets labeled using category system top categorizers considered black boxes propose algorithms able exploit third training set containing examples annotated category systems situation arises example companies incoming mails routed departments relying own category system focus exploiting dependencies category systems refine categorization decisions categorizers trained independently category systems description multiple categorization solutions based categorization reweighting approach compare real data lastly multimedia categorization cast multiple categorization assess methods framework

boosting methods usually overfit training data size generated classifiers schapire et al attempted explain phenomenon terms margins classifier achieves training examples breiman cast serious doubt explanation introducing boosting algorithm arc gv generate margins distribution adaboost performs worse paper close look breiman's compelling puzzling results reproduce main finding poorer performance arc gv explained increased complexity base classifiers explanation supported experiments entirely consistent margins theory maximizing margins desirable necessarily expense factors especially base classifier complexity

propose model probabilistic estimation continuous variables sequence observations tracking position object video mapping modeled product dynamics experts features relating adjacent time steps observation experts features relating image sequence individual features flexible switch time step depending inferred relevance additional information discriminative model generative likelihood data trained conditionally permits inclusion broad range rich features example features relying observations multiple time steps allows relevance features learned labeled sequences

ingcreasingly data mining algorithms deal databases continuously grow time algorithms avoid repeatedly scanning databases database attributes symbolic adtrees shown efficient structures store sufficient statistics main memory accelerate mining process batch environments efficient method sequentially update adtrees suitable incremental environments

introduce controlled predictive linear gaussian model cplg model predictive model discrete time dynamical systems real valued observations vector valued actions extends plg uncontrolled model recently introduced rudary et al 2005 cplg subsumes controlled linear dynamical systems lds called kalman filter models equal dimension requires fewer parameters introduce predictive linear quadratic gaussian cost minimization based cplg equivalent linear quadratic gaussian lqg sometimes called lqr algorithm estimate cplg parameters data algorithm consistent estimation procedure finally empirical results suggesting algorithm performs favorably compared expectation maximization controlled lds models

statistical approach rule learning doing address inherent traditional rule learning computational hardness finding rule sets low training error capacity control avoid fitting chosen representation involves weights attached rules instead optimizing error rate directly optimize rule sets margin low variance formulated convex optimization allowing efficient computation representation optimization procedure effectively yield weighted clauses cnf representation avoid overfitting propose model selection strategy utilizes novel concentration inequality empirical tests system competitive existing rule learning algorithms flexible learning bias adjusted improve predictive accuracy considerably

sequence segmentation flexible highly accurate mechanism modeling applications inference segmentation models involves dynamic programming computations worst cubic length sequence contrast typical sequence labeling models require linear time remove limitation segmentation models vis vis sequential models designing succinct representation potentials common overlapping segments exploit potentials design efficient inference algorithms analytically shown lower complexity empirically found comparable sequential models typical extraction tasks

recent growing classification link prediction structured domains methods crfs lafferty et al 2001 rmns taskar et al 2002 support flexible mechanisms modeling correlations due link structure addition structured domains structure risk cost function associated misclassifications rich tradition cost sensitive learning applied unstructured iid data propose framework capture correlations link structure handle structured cost functions novel cost sensitive structured classifier based maximum entropy principles directly determines cost sensitive classification contrast approach employs standard 0 1 loss structured classifier followed minimization expected cost misclassification demonstrate utility proposed classifier experiments synthetic real world data

medical diagnosis doctors sets medical tests sequence accurate diagnosis patient diseases doing trade cost tests misdiagnosis paper cost sensitive learning model process assume test examples patients contain missing values actual values acquired cost similar doing medical tests reduce misclassification errors misdiagnosis propose novel sequential batch test algorithm acquire sets attribute values sequence similar sets medical tests doctors sequence goal algorithm minimize total cost i.e trade acquiring attribute values misclassifications demonstrate effectiveness algorithm outperforms previous methods significantly algorithm readily applied real world diagnosis tasks study heart disease paper

extend support vector machines input spaces sets ensuring classifier invariant permutations sub elements input permutations include reordering scalars input vector re orderings tuples input matrix re orderings objects hilbert spaces set approach induces permutational invariance classifier directly applied unusual set based representations data permutation invariant support vector machine alternates hungarian method maximum weight matching maximum margin learning procedure effectively estimate apply permutations input data maximize classification margin minimizing data radius procedure strong theoretical justification via established error probability bounds experiments shown character recognition 3d object recognition various uci datasets

bayesian search algorithm learning structure latent variable models continuous variables stress importance applying search operators designed especially parametric family models performed searching subsets observed variables covariance matrix represented sum matrix low rank diagonal matrix residuals resulting search procedure relatively efficient main search operator branch factor grows linearly variables resulting models simpler fit models based generalizations factor analysis derived standard hill climbing methods

reinforcement learning agent act sole purpose efficiently learn optimal policy words explore able exploit formulate markov decision process explicitly modeling internal agent propose principled heuristic solution experimental results domains exploring algorithm's learning policy skill reward function neglected component skill discovery

intuitive approach utilizing unlabeled data kernel based classification algorithms simply treat unknown labels additional optimization variables margin based loss functions view approach attempting learn low density separators hard optimization solve typical semi supervised settings unlabeled data abundant popular transductive svm algorithm label switching retraining procedure susceptible local minima paper global optimization framework semi supervised kernel machines easier parametrically deformed original hard minimizers smoothly tracked approach motivated deterministic annealing techniques involves sequence convex optimization exactly efficiently solved empirical results synthetic real world datasets demonstrate effectiveness approach

feature selection applied dimensional data prior classification learning using training dataset selection learning result called feature subset selection bias bias putatively exacerbate data fitting negatively affect classification performance current practice separate datasets seldom employed selection learning dividing training data datasets feature selection classifier learning respectively reduces amount data task attempts address dilemma formalize selection bias classification learning analyze statistical properties study factors affect selection bias bias impacts classification learning via various experiments research endeavors provide illustration explanation bias cause negative impact classification expected regression

classification multichannel eeg recordings motor imagination exploited successfully brain computer interfaces bci paper consider eeg signals outputs networked dynamical system cortex exploit novel features collective dynamics system classification herein propose framework learning optimal filters automatically data employing fisher ratio criterion experimental evaluations comparing proposed dynamical system features csp ar features reveal competitive performance classification results benefits employing spatial temporal filters optimized using proposed learning approach

investigate conditions clustering learning mixture spherical gaussians computationally tractable statistically using principal component projection greatly aids recovering clustering using em empirical evidence using projection gap samples recover clustering using em samples computational restrictions characterize regime gap exists

investigate learning predict moves board game game records expert players particular obtain probability distribution legal moves professional play position distribution numerous applications computer including serving efficient stand player effective move selector move sorter game tree search training tool players method major components pattern extraction scheme efficiently harvesting patterns size shape expert game records bayesian learning algorithm variants learns distribution values move board position based local pattern context system trained 181,000 expert games excellent prediction performance indicated ability perfectly predict moves professional players 34 test positions

markov decision process finite size action spaces size propose algorithm delayed learning prove pac achieving near optimal performance except otilde sa timesteps using sa space improving otilde sup 2 sup bounds previous algorithms result proves efficient reinforcement learning learning model mdp experience learning takes single continuous thread experience resets nor parallel sampling beyond storage experience requirements delayed learning's experience computation cost previous pac algorithms

formalize associative bandit framework introduced kaelbling learning theory learning environment modeled armed bandit arm payoffs conditioned observable input selected trial payoff functions constrained hypothesis class learning performed efficiently respect vc dimension class formally reduce pac classification associative bandit producing efficient algorithm hypothesis class efficient classification algorithms demonstrate approach empirically scalable concept class

structure bayesian network bn encodes variable independence learning structure bn typically computational complexity paper explore represent variable independence learning conditional probability tables cpts instead learning structure bayesian network structure decision tree learned cpt resulting model called bayesian network classifiers fbcs learning fbc learning decision trees cpts captures essentially variable independence context specific independence novel efficient decision tree learning effective context fbc learning experiments fbc learning algorithm demonstrates performance classification ranking compared art learning algorithms addition reduced effort structure learning makes time complexity low

dimensionality reduction preprocessing steps dimensional data analysis paper consider supervised dimensionality reduction samples accompanied class labels traditional fisher discriminant analysis popular powerful method purpose tends undesired results samples class form separate clusters i.e multimodal paper propose dimensionality reduction method called local fisher discriminant analysis lfda localized variant fisher discriminant analysis lfda takes local structure data account multimodal data embedded appropriately lfda extended linear dimensionality reduction scenarios kernel trick

propose series feature weighting algorithms stemming interpretation relief online algorithm solves convex optimization margin based objective function interpretation explains simplicity effectiveness relief enables identify weaknesses offer analytic solution mitigate extend newly proposed algorithm handle multiclass using multiclass margin definition reduce computational costs online learning algorithm developed convergence theorems proposed algorithms experiments based uci microarray datasets performed demonstrate effectiveness proposed algorithms

established methods reducing support vectors trained binary support vector machine minimal impact accuracy reduced set methods applied multiclass svms binary svms significantly results reducing binary svm independently approach based burges approach constructs reduced set vector pre image vector kernel space extend recomputing svm weights bias optimally using original svm objective function leads accuracy binary reduced set svm allows vectors shared multiple binary svms multiclass accuracy fewer reduced set vectors propose computing pre images using differential evolution found robust gradient descent experimental results variety approach consistently previous multiclass reduced set methods sometimes dramatic difference

string kernels compare set common substrings strings recently proposed vishwanathan smola 2004 surprisingly kernels computed linear time linear space using annotated suffix trees theory suffix tree based algorithm requires space length string practice 40n bytes required 20n bytes storing suffix tree additional 20n bytes annotation memory requirement coupled poor locality memory access inherent due suffix trees means performance suffix tree based algorithm deteriorates strings paper describe linear time space efficient scalable algorithm computing string kernels based suffix arrays algorithm faster easier implement average requires 19n bytes storage exhibits strong locality memory access algorithm extended perform linear time prediction test string experiments validate claims

paper examines dimensional regression noise contaminated input output data goals learning include optimal prediction noiseless query optimal system identification step focus linear regression methods easily cast nonlinear learning locally weighted learning approaches standard linear regression algorithms generate biased regression estimates input noise suffer numerically data contains redundancy irrelevancy inspired factor analysis regression develop variational bayesian algorithm robust ill conditioned data automatically detects relevant features identifies input output noise computationally efficient demonstrate effectiveness techniques synthetic data system identification task rigid body dynamics model robotic vision head algorithm performs 10 70 previously suggested methods

inference markov decision processes recently received means infer goals observed action policy recognition tool compute policies particularly aspect approach existing inference technique dbns available answering behavioral question including continuous factorial hierarchical representations expectation maximization algorithm computing optimal policies unlike previous approaches actually optimizes discounted expected future return arbitrary reward functions assuming ad hoc finite total time algorithm generic inference technique utilized step demonstrate exact inference discrete maze gaussian belief propagation continuous stochastic optimal control

graph data getting increasingly popular e.g bioinformatics text processing main difficulty graph data processing lies intrinsic dimensionality graphs namely graph represented binary feature vector indicators subgraphs dimensionality usual statistical methods propose efficient method learning binomial mixture model feature space combining inf 1 inf regularizer data structure called dfs code tree map estimate zero parameters computed efficiently means em algorithm method applied clustering rna graphs compared favorably graph kernels spectral graph distance

approach automatically driving data collection using information previously acquired data called active learning traditional active learning addresses choosing unlabeled examples class labels queried goal learning classifier contrast address active feature sampling detecting useless features propose strategy actively sample values features class labeled examples objective feature relevance assessment derive active feature sampling algorithm information theoretic statistical formulation experimental results synthetic uci real world datasets demonstrate active sampling algorithm provide accurate estimates feature relevance lower data acquisition costs random sampling previously proposed sampling algorithms

apply stochastic meta descent smd stochastic gradient optimization method gain vector adaptation training conditional random fields crfs data sets resulting optimizer converges quality solution magnitude faster limited memory bfgs leading method reported date report results exact inexact inference techniques

models textual corpora employ text generation methods involving gram statistics latent topic variables inferred using bag words assumption word ignored previously methods combined explore hierarchical generative probabilistic model incorporates gram statistics latent topic variables extending unigram topic model include properties hierarchical dirichlet bigram language model model hyperparameters inferred using gibbs em algorithm data sets 150 documents model exhibits predictive accuracy hierarchical dirichlet bigram language model unigram topic model additionally inferred topics dominated function words topics discovered using unigram statistics potentially meaningful

novel semi supervised learning approach proposed based linear neighborhood model assumes data linearly reconstructed neighborhood algorithm named linear neighborhood propagation lnp propagate labels labeled dataset using linear neighborhoods sufficient smoothness derive easy extend lnp sample data promising experimental results synthetic data digit text classification tasks

recently appealing approach proposed compute entire solution path support vector classification svc low extra computational cost approach extended support vector regression svr model called epsilon svr method requires error parameter epsilon set priori desired accuracy approximation specified advance paper solution path epsilon svr piecewise linear respect epsilon propose efficient algorithm exploring dimensional solution space defined regularization error parameters opposed algorithm svc proposed algorithm epsilon svr initializes support vectors zero increases gradually algorithm proceeds regression function possessing sparseness property obtained iterations

consider boosting algorithms maintain distribution set examples iteration weak hypothesis received distribution updated motivate updates minimizing relative entropy subject linear constraints example adaboost constrains edge hypothesis w.r.t updated distribution gamma 0 sense adaboost corrective w.r.t hypothesis cleaner boosting method totally corrective edges past hypotheses constrained gamma gamma suitably adapted.using techniques prove iteration bounds totally corrective algorithms corrective versions moreover adaptive gamma algorithms provably maximizes margin experimentally totally corrective versions return convex combinations weak hypotheses corrective ones competitive lpboost totally corrective boosting algorithm regularization iteration bound

paper study framework introduced vapnik 1998 vapnik 2006 alternative capacity concept margin approach particular binary classification set labeled examples collection examples belong class collection called universum allows encode prior knowledge representing meaningful concepts domain hand describe algorithm leverage universum maximizing observed contradictions experimentally approach delivers accuracy improvements using labeled data

recent predictive linear gaussian model plg improves traditional linear dynamical system models using predictive representation makes consistent parameter estimation loss modeling power using fewer parameters paper extend plg model stochastic nonlinear dynamical systems using kernel methods gaussian kernel model admits closed form solutions update equations due conjugacy dynamics representation explore efficient sigma approximation updates model parameters learned directly data learned line kernel recursive squares algorithm empirically compare model approximation original plg discuss relative advantages

recent predictive representation psr models focused using predictions outcomes loop action sequences predictions answer questions form probability seeing observation sequence inf 1 inf inf 2 inf agent takes action sequence inf 1 inf inf 2 inf history expressive questions representation behave according policy terminate observation extend linear psr framework answer questions options temporally extended closed loop courses action bounding size linear psr model questions class options introduce hierarchical psr hpsr predictions options primitive action sequences empirical results learning hpsrs simple domains

algorithms proposed time series classification nearest neighbor dynamic time warping dtw distance exceptionally difficult beat approach weakness computationally demanding realtime applications mitigate speed dtw calculations nonetheless limit help propose additional technique numerosity reduction speed nearest neighbor dtw idea numerosity reduction nearest neighbor classifiers history leverage original observation relationship dataset size dtw constraints produce extremely compact dataset little loss accuracy test ideas comprehensive set experiments efficiently produce extremely fast accurate classifiers

unified duality view recently emerged spectral methods nonlinear dimensionality reduction including isomap locally linear embedding laplacian eigenmaps maximum variance unfolding discuss duality theory maximum variance unfolding methods directly related primal formulation dual formulation interpreted optimality conditions duality framework reveals close connections seemingly algorithms particular resolves myth methods using top eigenvectors dense matrix bottom eigenvectors sparse matrix eigenspaces exactly aligned primal dual optimality

uncovering haplotypes single nucleotide polymorphisms population demography essential biological medical applications methods haplotype inference developed including methods based coalescence finite infinite mixtures maximal parsimony ignore underlying population structure genotype data noted pritchard 2001 populations share portion genetic ancestors own genetic components migration diversification paper address multi population haplotype inference capture cross population structure using nonparametric bayesian prior hierarchical dirichlet process hdp teh et al 2006 conjoining prior recently developed bayesian methodology haplotype phasing dp haplotyper xing et al 2004 develop efficient sampling algorithm hdp based level nested oacute lya urn scheme model outperforms extant algorithms simulated real biological data

unsupervised algorithm training structured predictors discriminative convex avoids em idea formulate unsupervised version structured learning methods maximum margin markov networks trained via semidefinite programming result discriminative training criterion structured predictors hidden markov models remains unsupervised create local minima reduce training cost reformulate training procedure mitigate dependence semidefinite programming finally propose heuristic procedure avoids semidefinite programming entirely experimental results convex discriminative procedure produce conditional models conventional baum welch em training

nonlinear dimensionality reduction considered focus prior information available namely semi supervised dimensionality reduction shown basic nonlinear dimensionality reduction algorithms locally linear embedding lle isometric feature mapping isomap local tangent space alignment ltsa modified taking account prior information exact mapping data sensitivity analysis algorithms prior information improve stability solution insight prior information improves solution demonstrate usefulness algorithm synthetic real life examples

dimensionality reduction pre processing step applications linear discriminant analysis lda methods supervised dimensionality reduction classical lda formulation requires nonsingularity scatter matrices involved undersampled data dimension larger sample size scatter matrices singular classical lda fails extensions including null space based lda nlda orthogonal lda olda proposed past overcome paper computational theoretical analysis nlda olda main result mild condition holds applications involving dimensional data nlda equivalent olda performed extensive experiments various types data results consistent theoretical analysis analysis experimental results provide insight lda based algorithms

paper considers selecting informative experiments measurements learning regression model propose novel simple concept active learning transductive experimental design explores available unmeasured experiments i.e unlabeled data scalability comparison classic experimental design methods depth analysis method tends favor experiments hard predict representative rest experiments efficient optimization design achieved alternating optimization sequential greedy search extensive experimental results synthetic real world tasks including questionnaire design preference learning active learning text categorization spatial sensor placement highlight advantages proposed approaches

ordinal regression effective learning user preferences research focuses single regression paper introduce collaborative ordinal regression multiple ordinal regression tasks handled simultaneously modeling task individually explore dependency ranking functions hierarchical bayesian model assign common gaussian process gp prior individual functions empirical studies collaborative model outperforms individual counterpart preference learning applications

eigendecomposition kernel matrix indispensable procedure learning vision tasks cubic complexity sup 3 sup impractical data size paper propose efficient approach solve eigendecomposition kernel matrix idea approximate composed sup 2 sup constant blocks eigenvectors solved sup 3 sup time recover eigenvectors original kernel matrix complexity method mn sup 3 sup scales favorably art low rank approximation sampling based approaches sup 2 sup sup 3 sup approximation quality controlled conveniently method demonstrates encouraging scaling behaviors experiments image segmentation spectral clustering kernel principal component analysis

describe statistical approach software debugging presence multiple bugs due sparse sampling issues complex interaction program predicates generic shelf algorithms fail select useful bug predictors taking inspiration bi clustering algorithms propose iterative collective voting scheme program runs predicates demonstrate successful debugging results real world programs debugging benchmark suite

semi naive bayesian classifiers seek retain numerous strengths naive bayes reducing error relaxing attribute independence assumption backwards sequential elimination bse wrapper technique attribute elimination proved effective task explore technique lazy elimination le eliminates highly related attribute values classification time computational overheads inherent wrapper techniques analyze effect le bse art semi naive bayesian algorithm averaged dependence estimators aode experiments le significantly reduces bias error undue computation bse significantly reduces bias error training time complexity context aode le significant advantage bse computational efficiency error


stationarity unrealistic prior assumption gaussian process regression solution predefine explicit nonstationary covariance function covariance functions difficult specify require detailed prior knowledge nonstationarity propose gaussian process product model gppm models data pointwise product latent gaussian processes nonparametrically infer nonstationary variations amplitude approach differs nonparametric approaches covariance function inference operates outputs inputs resulting significant reduction computational cost required data inference approximate inference scheme using expectation propagation variational approximation yields convenient gp hyperparameter selection compact approximate predictive distributions

identifying minimal gene set required sustain life crucial importance understanding cellular mechanisms designing therapeutic drugs describes kernel based solutions predicting essential genes outperform existing models using training data solution based semi manually designed kernel derived pfam database includes pfam domains novel domain based sequence kernels capture sequence similarity respect domains sets protein sequences deal size thousands domains individual domains sometimes containing thousands sequences representing efficiently computing kernels using automata report results extensive experiments demonstrating compare favorably pfam kernel predicting protein essentiality requiring manual tuning

kernel stick breaking process ksbp employed segment imagery imposing condition patches blocks pixels spatially proximate associated cluster segment clusters set priori inferred hierarchical bayesian model ksbp integrated shared dirichlet process prior simultaneously model multiple images inferring inter relationships latter application useful sorting learning relationships multiple images bayesian inference algorithm based hybrid variational bayesian analysis local sampling addition providing details model associated inference framework example results image analysis

clouds sets dimensions kernel methods learning sets dealt specific geometrical invariances practical constraints associated clouds computer vision graphics paper extensions graph kernels clouds allow kernel methods objects shapes line drawings dimensional clouds design rich numerically efficient kernels free parameters kernels covariance matrices factorizations probabilistic graphical models derive polynomial time dynamic programming recursions applications recognition handwritten digits chinese characters training examples

consider square linear regression regularization l1 norm usually referred lasso paper detailed asymptotic analysis model consistency lasso various decays regularization parameter compute asymptotic equivalents probability correct model selection i.e variable selection specific rate decay lasso selects variables enter model probability tending exponentially fast selects variables strictly positive probability property implies run lasso bootstrapped replications sample intersecting supports lasso bootstrap estimates leads consistent model selection novel variable selection algorithm referred bolasso compared favorably linear regression methods synthetic data datasets uci machine learning repository

describe algorithm learning presence multiple criteria technique generalizes previous approaches learn optimal policies linear preference assignments multiple reward criteria algorithm viewed extension standard reinforcement learning mdps instead repeatedly maximal expected rewards set expected rewards maximal set linear preferences weight vector algorithm proof correctness solution optimal policy linear preference function solution reduces standard value iteration algorithm specific weight vector

paper introduces novel machine learning model called multiple instance ranking mirank enables ranking performed multiple instance learning setting motivation mirank stems hydrogen abstraction computational chemistry predicting hydrogen atoms hydrogen abstracted removed metabolism model predicts preferred hydrogen molecule ranking ambiguity knowing hydrogen atom preferred actually abstracted paper formulates mirank context proposes algorithm solving mirank using successive linear programming method outperforms multiple instance classification models real synthetic datasets

address learning classifiers tasks derive solution produces resampling weights match pool examples target distribution task motivated predicting outcome therapy attempt patient carries hiv virus set observed genetic properties predictions hundreds combinations drugs similar biochemical mechanisms multi task learning enables predictions drug combinations training examples substantially improves overall prediction accuracy

nonnegative matrix factorization nmf popularized tool data mining lee seung 1999 nmf attempts approximate matrix nonnegative entries product low rank matrices nonnegative entries propose algorithm called rank downdate r1d computing nmf partly motivated singular value decomposition algorithm computes dominant singular values vectors adaptively determined sub matrices matrix iteration r1d extracts rank submatrix original matrix according objective function establish theoretical result maximizing objective function corresponds correctly classifying articles nearly separable corpus provide computational experiments success method identifying features realistic datasets method faster nmf routines

typically agent evaluation monte carlo estimation stochastic agent decisions stochastic outcomes approach inefficient requiring samples accurate estimate technique simultaneously evaluate strategies playing single strategy context extensive game technique based importance sampling utilizes mechanisms significantly reducing variance estimates demonstrate effectiveness domain poker stochasticity makes traditional evaluation problematic

scientists frequently multiple types experiments data sets test validity parameterized models locate plausible regions model parameters examining multiple data sets scientists obtain inferences typically informative deductions derived data sources independently standard data combination techniques result target functions weighted sum observed data sources computing constraints plausible regions model parameter space formulated finding level set target function sum observable functions propose active learning algorithm selects sample parameter space observable function compute sample empirical tests synthetic functions real data eight parameter cosmological model algorithm significantly reduces samples required identify desired level set

common machine learning statistics consists estimating mean response beta vector observations assuming beta epsilon beta vector parameters epsilon vector stochastic errors particularly dimension beta dimension propose flexible bayesian models yield sparse estimates beta rarr infin models closely related class eacute vy processes simulations demonstrate models outperform significantly range popular alternatives

paper perform empirical evaluation supervised learning dimensional data evaluate performance metrics accuracy auc squared loss study effect increasing dimensionality performance learning algorithms findings consistent previous studies relatively low dimension suggest dimensionality increases relative performance learning algorithms changes surprise method performs consistently dimensions random forests followed neural nets boosted trees svms

recent developments programmable highly parallel graphics processing units gpus enabled performance implementations machine learning algorithms describe solver support vector machine training running gpu using sequential minimal optimization algorithm adaptive set selection heuristic achieves speedups 9 35x libsvm running traditional processor gpu based system svm classification achieves speedups 81 138x libsvm 5 24x own cpu based svm classifier

data structure enabling efficient nearest neighbor nn retrieval bregman divergences family bregman divergences includes popular dissimilarity measures including kl divergence relative entropy mahalanobis distance itakura saito divergence divergences challenge efficient nn retrieval metrics nn data structures designed data structure introduced shares basic structure popular metric ball tree employs convexity properties bregman divergences triangle inequality experiments demonstrate speedups brute force search magnitude

dimensional classification infeasible include training samples cover class regions densely irregularities resulting sparse sample distributions cause local classifiers nearest neighbors nn kernel methods irregular decision boundaries solution fill holes building convex model region spanned training samples class classifying examples based distances approximate models methods based affine convex hulls bounding hyperspheres studied propose method based bounding hyperdisk class intersection affine hull bounding hypersphere training samples argue hyperdisks preferable affine convex hulls hyperspheres bound classes tightly affine hulls hyperspheres avoiding sample overfitting computational complexity inherent dimensional convex hulls hyperdisk method kernelized provide nonlinear classifiers based euclidean distance metrics experiments classification promising results

novel commentator system learns language sportscasts simulated soccer games system learns parse generate commentaries engineered knowledge english language training using ambiguous supervision form textual human commentaries simulation soccer games system simultaneously tries establish correspondences commentaries simulation build translation model novel algorithm iterative generation strategy learning igsl deciding events comment human evaluations generated commentaries indicate reasonable quality compared human commentaries

similarity matrices generated applications positive semidefinite hence can't fit kernel machine framework paper study training support vector machines indefinite kernel consider regularized svm formulation indefinite kernel matrix treated noisy observation unknown positive semidefinite proxy kernel support vectors proxy kernel computed simultaneously propose semi infinite quadratically constrained linear program formulation optimization solved iteratively global optimum solution propose employ additional pruning strategy significantly improves efficiency algorithm retaining convergence property algorithm addition close relationship proposed formulation multiple kernel learning experiments collection benchmark data sets demonstrate efficiency effectiveness proposed algorithm

consider learning follow desired trajectory demonstrations sub optimal expert algorithm extracts initially unknown desired trajectory sub optimal expert's demonstrations ii learns local model suitable control learned trajectory apply algorithm autonomous helicopter flight autonomous helicopter's performance exceeds expert helicopter pilot's demonstrations stronger results significantly extend art autonomous helicopter aerobatics particular results include autonomous tic tocs loops hurricane vastly superior performance previously performed aerobatic maneuvers flips rolls complete airshow requires autonomous transitions various maneuvers

clustering advice constrained clustering recent focus data mining community success achieved incorporating advice means spectral clustering frameworks theory community explored inconsistent advice incorporated spectral clustering extending de bie cristianini set framework finding minimum normalised cuts subject inconsistent advice

describe single convolutional neural network architecture sentence outputs host language processing predictions speech tags chunks named entity tags semantic roles semantically similar words likelihood sentence makes sense grammatically semantically using language model entire network trained jointly tasks using weight sharing instance multitask learning tasks labeled data except language model learnt unlabeled text represents novel form semi supervised learning shared tasks multitask learning semi supervised learning improve generalization shared tasks resulting art performance

errors map tasks using computer vision sparse demonstrate considering construction digital elevation models employ stereo matching algorithms triangulate real world sparsity coupled geometric theory errors recently developed authors allows autonomous agents calculate own precision independently ground truth connect developments recent advances mathematics sparse signal reconstruction compressed sensing theory extends autonomy 3 model reconstructions discovered 1990s errors

paper notion algorithmic stability derive novel generalization bounds families transductive regression algorithms using convexity closed form solutions analysis helps compare stability algorithms suggests existing algorithms stable prescribes technique stable reports results experiments local transductive regression demonstrating benefit stability bounds model selection particular determining radius local neighborhood algorithm

class classification seek rule coherent subset instances similar positive examples pool instances formulated analyzed naturally rate distortion framework leading efficient algorithm compares previous class methods model extended remove background clutter clustering improve cluster purity

processes difficult analyze provide sparse noisy observation intensity function driving process gaussian processes offer attractive framework infer underlying intensity functions result inference continuous function defined time typically amenable analytical efforts naive implementation computationally infeasible reasonable size memory run time requirements demonstrate specific methods class renewal processes eliminate memory burden reduce solve time magnitude

paper focuses clustering task called self taught clustering self taught clustering instance unsupervised transfer learning aims clustering collection target unlabeled data help amount auxiliary unlabeled data target auxiliary data topic distribution target data sufficient allow effective learning quality feature representation learn useful features help auxiliary data target data clustered effectively propose co clustering based self taught clustering algorithm tackle clustering target auxiliary data simultaneously allow feature representation auxiliary data influence target data common set features data representation clustering target data improved experiments image clustering algorithm greatly outperform art clustering methods utilizing irrelevant unlabeled auxiliary data

active learning scheme exploits cluster structure data

classifier trained using machine learning algorithm real world system noise appear training data particularly subset features missing corrupted novel machine learning techniques robust type classification time noise solve approximation learning using linear programming analyze tightness approximation prove statistical risk bounds approach define online learning variant address variant using modified perceptron obtain statistical learning algorithm using online batch technique conclude set experiments demonstrate effectiveness algorithms

propose rule induction algorithm solving classification via probability estimation main advantage decision rules simplicity interpretability approaches rule induction based sequential covering follow approach single decision rule treated base classifier ensemble ensemble built greedily minimizing negative loglikelihood results estimating class conditional probability distribution introduced approach compared decision rule induction algorithms slipper lri rulefit

address learning decision functions training data attribute values unobserved arise instance training data aggregated multiple sources sources record subset attributes derive generic joint optimization distribution governing missing values free parameter optimal solution concentrates density mass finitely imputations provide corresponding algorithm learning incomplete data report empirical results benchmark data email spam application motivates

rich representations reinforcement learning studied purpose enabling generalization learning feasible spaces introduce object oriented mdps oo mdps representation based objects interactions natural modeling environments offers generalization opportunities introduce learning algorithm deterministic oo mdps prove polynomial bound sample complexity illustrate performance gains representation algorithm taxi domain plus real life videogame

learning rank becoming increasingly popular research machine learning ranking aims induce preference relations set instances input space collecting labeled data growing burden rank applications labeling requires eliciting relative set alternatives paper propose novel active learning framework svm based boosting based rank learning approach suggests sampling based maximizing estimated loss differential unlabeled data experimental results benchmark corpora proposed model substantially reduces labeling effort achieves superior performance rapidly 30 relative improvement margin based sampling baseline

partially observable markov decision processes pomdps succeeded planning domains require balancing actions increase agent's knowledge actions increase agent's reward unfortunately pomdps defined parameters difficult specify domain knowledge paper approximation approach allows treat pomdp model parameters additional hidden model uncertainty pomdp coupled model directed queries planner actively learns policies demonstrate approach pomdp

introduce confidence weighted linear classifiers add parameter confidence information linear classifiers online learners setting update classifier parameters estimate confidence particular online algorithms study maintain gaussian distribution parameter vectors update mean covariance distribution instance empirical evaluation range nlp tasks algorithm improves art online batch methods learns faster online setting lends classifier combination parallel training

describe efficient algorithms projecting vector onto l1 ball methods projection performs exact projection expected time dimension space vectors elements perturbed outside l1 ball projecting log time setting especially useful online learning sparse feature spaces text categorization applications demonstrate merits effectiveness algorithms numerous batch online learning tasks variants stochastic gradient projection methods augmented efficient projection procedures outperform interior methods considered art optimization techniques online settings gradient updates l1 projections outperform exponentiated gradient algorithm obtaining models degrees sparsity

cost curves recently introduced alternative complement roc curves visualize binary classifiers performance importance cost roc curves computation confidence intervals curves themselves reliability classifier's performance assessed computing confidence intervals difference performance classifiers allows determination classifier performs significantly simple procedure obtain confidence intervals costs difference costs various operating conditions perform bootstrap resampling test set paper derive exact bootstrap distributions values dstributions obtain confidence intervals various operating conditions performances confidence intervals measured terms coverage accuracies simulations excellent results

study introduce novel algorithm learning polyhedron describe target class proposed approach takes advantage limited subclass information available negative samples jointly optimizes multiple hyperplane classifiers designed classify positive samples subclass negative samples flat polyhedron provides robustness whereas multiple contributes flexibility required deal complex datasets apart improving prediction accuracy system proposed polyhedral classifier provides run time speedups product executed cascaded framework real time evaluate performance proposed technique real world colon dataset terms prediction accuracy online execution speed

transition probabilities rewards markov decision process mdp agent obtain optimal policy interaction environment exact transition probabilities difficult experts specify option left agent potentially costly exploration environment paper propose alternative initial possibly inaccurate specification mdp agent determines sensitivity optimal policy changes transitions rewards focuses exploration regions space optimal policy sensitive proposed exploration strategy performs control planning

discriminative training e.g crf structural svm holds promise machine translation image segmentation clustering complex inference applications require exact training intractable leads approximate training methods unfortunately knowledge perform efficient effective approximate training limited focusing structural svms provide explore algorithms classes approximate training algorithms call undergenerating e.g greedy overgenerating e.g relaxations algorithms provide theoretical empirical analysis types approximate trained structural svms focusing connected pairwise markov random fields models trained overgenerating methods theoretic advantages undergenerating methods empirically robust relative undergenerating brethren relaxed trained models favor fractional predictions relaxed predictors

hierarchical dirichlet process hidden markov model hdp hmm flexible nonparametric model allows spaces unknown size learned data demonstrate limitations original hdp hmm formulation teh et al 2006 propose sticky extension allows robust learning smoothly varying dynamics using dp mixtures formulation allows learning complex multimodal emission distributions develop sampling algorithm employs truncated approximation dp jointly resample sequence greatly improving mixing rates via extensive experiments synthetic data nist speaker diarization database demonstrate advantages sticky extension utility hdp hmm real world applications

developed linear support vector machine svm training algorithm called ocas computational effort scales linearly sample size extensive empirical evaluation ocas significantly outperforms current art svm solvers svm sup light sup svm sup perf sup bmrm achieving speedups 1,000 datasets svm sup light sup 20 svm sup perf sup obtaining precise support vector solution ocas optimization steps faster convergence domain prevailing approximative methods sgd pegasos effectively parallelizing ocas able train dataset size 15 million examples 32gb size 671 competing string kernel svm required 97,484 train 10 million examples sub sampled dataset

propose stopping condition support vector machine svm solver precisely reflects objective leave error computation stopping condition guarantees output intermediate svm solution identical output optimal svm solution data excluded training set simple augmentation svm training algorithm allows stopping criterion equivalent proposed sufficient condition comprehensive experimental evaluation method consistent speedup exact loo computation method factor 13 linear kernel algorithm seen example constructive guidance optimization algorithm towards achieving attainable expected risk optimal computational cost

consider task reinforcement learning environment rare significant events occur independently actions selected controlling agent events sampled according natural probability occurring convergence conventional reinforcement learning algorithms slow learning algorithms exhibit variance assume access simulator rare event probabilities artificially altered importance sampling learn simulation data introduce algorithms policy evaluation using tabular function approximation representations value function prove reinforcement learning algorithms converge tabular analyze bias variance approach compared td learning evaluate empirically performance algorithm random markov decision processes network planning task

type algorithms statistical techniques support learning datasets stretches time address question memory bounded version variational em algorithm approximates inference topic model algorithm alternates phases model building model compression satisfy memory constraint model building phase expands internal representation topics data arrives bayesian model selection compression achieved merging data items clumps caching sufficient statistics empirically resulting algorithm able handle datasets magnitude larger standard batch version

recently instead selecting single kernel multiple kernel learning mkl proposed convex combination kernels weight kernel optimized training mkl assigns weight kernel input space paper develop localized multiple kernel learning lmkl algorithm using gating model selecting appropriate kernel function locally localizing gating model kernel based classifier coupled optimization joint manner empirical results ten benchmark bioinformatics data sets validate applicability approach lmkl achieves statistically similar accuracy results compared mkl storing fewer support vectors lmkl combine multiple copies kernel function localized example lmkl multiple linear kernels accuracy results using single linear kernel bioinformatics data sets

bit minimizing kinds regret experts regret types relate types equilibria multiagent setting repeated matrix games kinds regret online convex programming ocps equilibria analogous multiagent setting repeated convex games gap unfortunate convex games expressive matrix games machine learning expressed ocps paper close gap analyze spectrum regret types lie external swap regret corresponding equilibria lie coarse correlated correlated equilibrium analyze algorithms minimizing regret types examples framework derive algorithms learning correlated equilibria polyhedral convex games extensive form correlated equilibria extensive form games former exponentially efficient previous algorithms latter type

real world machine learning common input feature vector incomplete available missing corrupted paper boosting approach integrates features incomplete information complete information form strong classifier introducing hidden variables model missing information form loss functions combine labeled data partially labeled data effectively learn normalized unnormalized models primal proposed optimization loss functions provided close relationship motivations auxiliary functions bound change loss functions derive explicit parameter update rules learning algorithms demonstrate encouraging results real world visual object recognition computer vision named entity recognition natural language processing effectiveness proposed boosting approach

paper propose discriminant learning framework data consist linear subspaces instead vectors treating subspaces basic elements learning algorithms adapt naturally linear invariant structures propose unifying view subspace based learning method formulating grassmann manifold set fixed dimensional linear subspaces euclidean space previous methods typically adopt inconsistent strategy feature extraction performed euclidean space euclidean distances approach treat sub space grassmann space perform feature extraction classification space feasibility approach using grassmann kernel functions projection kernel binet cauchy kernel experiments real image databases proposed method performs compared art algorithms

paper common speech recognition training criteria minimum phone error criterion maximum mutual information criterion extended incorporate margin term margin based training algorithms proposed refine existing training algorithms machine learning speech recognition special addressed approaches proposed lack practical applicability inclusion margin term enforces significant changes underlying model e.g optimization algorithm loss function parameterization model approach conventional training criteria modified incorporate margin term allows margin training speech recognition using efficient algorithms accumulation optimization software conventional discriminative training proposed criteria equivalent support vector machines suitable smooth loss functions approximating smooth hinge loss function hard error e.g phone error experimental results tasks simple digit string recognition task sietill severely suffers overfitting vocabulary european parliament plenary sessions english task supposed dominated risk generalization issue

principled bayesian framework modeling partial memberships data clusters unlike standard mixture model assumes data belongs mixture component cluster partial membership model allows data fractional membership multiple clusters algorithms assign data partial memberships clusters useful tasks clustering genes based microarray data gasch eisen 2002 bayesian partial membership model bpm exponential family distributions model cluster product distibtutions weighted parameters model datapoint weights correspond degree datapoint belongs cluster parameters bpm continuous hybrid monte carlo perform inference learning discuss relationships bpm latent dirichlet allocation mixed membership models exponential family pca fuzzy clustering lastly experimental results discuss nonparametric extensions model

identifying appropriate kernel function matrix dataset essential kernel based learning techniques kernel learning algorithms proposed learn kernel functions matrices information e.g labeled examples pairwise constraints previous studies limited passive kernel learning information provided beforehand paper framework active kernel learning akl actively identifies informative pairwise constraints kernel learning key challenge active kernel learning measure informativeness example pair class label unknown propose min max approach active kernel learning selects example pair results classification margin regardless assigned class label furthermore approximate related optimization convex programming evaluate effectiveness proposed algorithm comparing implementations active kernel learning empirical study nine datasets semi supervised data clustering proposed algorithm effective competitors

applications data appear huge instances features linear support vector machines svm popular tools deal scale sparse data paper novel dual coordinate descent method linear svm l1 l2 loss functions proposed method simple reaches epsilon accurate solution log 1 epsilon iterations experiments indicate method faster art solvers pegasos tron svm sup perf sup recent primal coordinate descent implementation

markov logic networks mlns expressive representation statistical relational learning generalizes logic graphical models existing methods learning logical structure mln discriminative relational learning involve specific target predicates inferred background information found existing mln methods perform poorly ilp benchmark improved discriminative methods learning mln clauses weights outperform existing mln traditional ilp methods

causal analysis continuous valued variables typically autoregressive models linear gaussian bayesian networks instantaneous effects estimation gaussian bayesian networks poses serious identifiability recently proposed gaussian models combine gaussian instantaneous model autoregressive models gaussian model identifiable prior knowledge network structure propose estimation method shown consistent approach neglecting instantaneous effects lead completely wrong estimates autoregressive coefficients

hierarchical decomposition promises help scale reinforcement learning algorithms naturally real world exploiting underlying structure model based algorithms provided finite time convergence guarantees reinforcement learning play role coping relative scarcity data environments paper introduce algorithm integrates modern hierarchical model learning methods standard reinforcement learning setting algorithm scp maxq scp inherits efficient model based exploration scp max scp algorithm opportunities abstraction provided maxq framework analyze sample complexity algorithm experiments standard simulation environment illustrate advantages combining hierarchies models

paper introduces banditron variant perceptron rosenblatt 1958 multiclass bandit setting multiclass bandit setting models wide range practical supervised learning applications learner receives partial feedback referred bandit feedback spirit multi armed bandit models respect true label e.g web applications users provide positive click feedback necessarily disclose true label banditron ability learn multiclass classification setting bandit feedback reveals prediction algorithm correct necessarily reveal true label provide relative mistake bounds banditron enjoys favorable performance experiments demonstrate practicality algorithm furthermore paper pays close attention special data linearly separable exhaustively studied information setting novel bandit setting

regularizer transductive support vector machines tsvm trained stochastic gradient descent linear models multi layer architectures resulting methods trained online vastly superior training testing speed existing tsvm algorithms encode prior knowledge network architecture obtain competitive error rates propose natural generalization tsvm loss function takes account neighborhood manifold information directly unifying stage low density separation method single criterion leading art results

policy gradient approaches powerful instrument learning interact environment existing approaches focused propositional continuous domains extensive feature engineering difficult impossible apply structured domains e.g varying objects relations paper describe parametric policy gradient approach called nppg overcomes limitation key idea apply friedmann's gradient boosting policies represented weighted sum regression models grown stage wise optimization employing shelf regression learners nppg deal propositional continuous relational domains unified experimental results improve established results

propose algorithm independent component independent subspace analysis algorithm contrast based schweizer wolff measure pairwise dependence schweizer wolff 1981 parametric measure computed pairwise ranks variables algorithm frequently outperforms art ica methods normal setting significantly robust outliers mixed signals performs presence noise method solve independent subspace analysis isa signals recovered ica methods provide extensive empirical evaluation using simulated sound image data

meaningfully combine sets rankings comes deals ranked data heuristic supervised learning approaches rank aggregation exist require domain knowledge supervised ranked data expensive acquire address limitations propose mathematical algorithmic framework learning aggregate partial rankings supervision instantiate framework combining permutations combining top lists propose novel metric latter experiments scenarios demonstrate effectiveness proposed formalism

consider optimizing multilabel mrfs np hard ubiquitous low level computer vision approach solution formulate integer linear programming relax integrality constraints approach consider paper convert multi label mrf equivalent binary label mrf relax resulting relaxation efficiently solved using maximum flow algorithm solution provides partially optimal labelling binary variables partial labelling easily transferred multi label study theoretical properties relaxation compare standard specifically compare tightness characterize subclass relaxations coincide propose combined algorithms based technique demonstrate performance challenging computer vision

consider task learning accurately follow trajectory vehicle car helicopter dynamic programming algorithms differential dynamic programming ddp policy search dynamic programming psdp efficiently compute stationary policies tasks policies suited trajectory following easily generate control actions times follow trajectory weakness algorithms policies time indexed apply policies depending current time problematic 1 current time correspond trajectory 2 uncertainty prevent algorithms finding policies paper propose method space indexed dynamic programming overcomes difficulties begin dynamical system rewritten terms spatial index variable i.e trajectory function time space indexed dynamical systems derive space indexed version ddp psdp algorithms finally algorithms perform variety control tasks simulation real systems

central issue representing graph structured data instances learning algorithms designing features invariant permuting numbering vertices system invariant graph features call skew spectrum graphs skew spectrum based mapping adjacency matrix weigted directed unlabeled graph function symmetric computing bispectral invariants reduced form skew spectrum computable sup 3 sup time experiments benchmark datasets outperform art graph kernels

inductive logic programming theta subsumption widely coverage test unfortunately testing theta subsumption np complete represents crucial efficiency bottleneck relational learners paper probabilistic estimator clause coverage based randomized restarted search strategy distribution assumption algorithm estimate clause coverage decide subsumption examples implement algorithm program recover generated graph data real world datasets recover provides reasonably accurate estimates achieving dramatic runtimes improvements compared art algorithm

paper concerned generalization ability learning rank algorithms information retrieval ir key addressing learning look viewpoint query define concepts including query level loss query level risk query level stability analyze generalization ability learning rank algorithms giving query level generalization bounds using query level stability tool analysis helpful derive advanced algorithms ir apply proposed theory existing algorithms ranking svm irsvm experimental results algorithms verify correctness theoretical analysis

hidden markov models assume observations time series data stem hidden process compactly represented markov chain generalize model assuming observed data stems multiple hidden processes outputs interleave form sequence observations exact inference model np hard tractable effective inference algorithm obtained extending structured approximate inference methods factorial hidden markov models proposed model evaluated activity recognition domain multiple activities interleave generate stream sensor observations shown accurate standard hidden markov model domain

examine evaluating policy contextual bandit setting using observations collected execution policy policy evaluation impossible exploration policy chooses actions based information provided time step propose prove correctness principled method policy evaluation exploration policy deterministic action explored sufficiently apply technique offline evaluation internet advertising policies theoretical results hold exploration policy chooses ads independent information assumption typically violated commercial systems clever theory provide trivial realistic applications provide empirical demonstration effectiveness techniques real ad placement data

recently applications restricted boltzmann machines rbms developed variety learning rbms usually feature extractors learning algorithm provide initialization deep feed forward neural network classifiers considered standalone solution classification paper argue rbms provide self contained framework deriving competitive linear classifiers evaluation learning algorithms rbms aim introducing discriminative component rbm training improve performance classifiers approach simple rbms directly build classifier stepping stone finally demonstrate discriminative rbms successfully employed semi supervised setting

main objective transfer reinforcement learning reduce complexity learning solution target task effectively reusing knowledge retained solving set source tasks paper introduce novel algorithm transfers samples i.e tuples lang rang source target tasks assumption tasks similar transition models reward functions propose method select samples source tasks similar target task input batch reinforcement learning algorithms result samples agent collect target task learn solution reduced empirically following proposed approach transfer samples effective reducing learning complexity source tasks significantly target task

temporal text data generated time changing process distribution drift underlying distribution captured stationary likelihood techniques consider application local likelihood methods generative conditional modeling temporal document sequences examine asymptotic bias variance experimental study using rcv1 dataset containing temporal sequence reuters news stories

residual gradient rg proposed alternative td 0 policy evaluation function approximation exists little formal analysis comparing except limited paper employs techniques online learning linear functions provides worst probabilistic analysis compare types algorithms linear function approximation statistical assumptions sequence observations analysis applies markovian adversarial domains particular results suggest rg result temporal differences td 0 yield prediction errors phenomena observed simple markov chain examples adversarial

introduce learning framework combines elements pac mistake bound models kwik framework designed particularly utility learning settings active exploration impact training examples learner exposed true reinforcement learning active learning catalog kwik learnable classes

consider learning pairwise constraints unlabeled data pairwise constraints specify objects belong class link constraints link constraints propose learn mapping smooth data graph maps data onto unit hypersphere link objects mapped link objects mapped orthogonal mapping achieved formulating semidefinite programming convex solved globally approach effectively propagate pairwise constraints data set directly applied multi class classification handle data labels pairwise constraints mixture unified framework promising experimental results classification tasks variety synthetic real data sets

statistical computational concerns motivated parameter estimators based various forms likelihood e.g joint conditional pseudolikelihood paper unified framework studying estimators allows compare relative statistical efficiencies asymptotic analysis suggests modeling data tends reduce variance cost sensitive model misspecification experiments validating analysis

structured models achieve excellent performance slow test time investigate structure compilation replace structure features computationally simpler unfortunately statistically complex analyze tradeoff theoretically empirically natural language processing tasks introduce simple method transfer predictive power structure features via unlabeled data incurring minimal statistical penalty

describe manifold learning framewor naturally accommodates supervised learning partially supervised learning unsupervised clustering particular method chooses function minimizing loss subject manifold regularization penalty augmented cost minimized using greedy stagewise functional minimization procedure gradientboost stage boosting fast efficient demonstrate approach using radial basis function approximations trees performance method art standard semi supervised learning benchmarks produce results scale datasets

broad class boosting algorithms interpreted performing coordinate wise gradient descent minimize potential function margins data set class includes adaboost logitboost widely studied boosters paper broad class convex potential functions boosting algorithm highly susceptible random classification noise booster nonzero random classification noise rate eta simple data set examples efficiently learnable booster noise learned accuracy 1 2 random classification noise rate eta negative result contrast branching program based boosters fall convex potential function framework provably learn accuracy presence random classification noise

tensorial data frequently encountered various machine learning tasks dimensionality reduction applications paper extends classical principal component analysis pca multilinear version proposing novel unsupervised dimensionality reduction algorithm tensorial data named uncorrelated multilinear pca umpca umpca seeks tensor vector projection captures variation original tensorial input producing uncorrelated features successive variance maximization evaluate umpca tensorial recognition experimental results superiority especially low dimensional spaces comparison pca based algorithms

distance measure time series properly incorporate temporal structure applicable sequences unequal lengths paper propose distance measure principled solution requirements unlike conventional feature vector representation approach represents time series summarizing smooth curve reproducing kernel hilbert space rkhs translate distance time series distances curves moreover propose learn kernel rkhs population time series discrete observations using gaussian process based parametric mixed effect models experiments vastly real world proposed distance measure leads improved classification accuracy conventional distance measures

algorithm line incremental discovery temporal difference td networks key contribution establishment criteria expand node td network node expanded node independent prediction error requires explanation none criteria requires centralized calculation operations easily computed parallel distributed manner scalable bigger compared discovery methods predictive representations computer experiments demonstrate empirical effectiveness algorithm

positive definite kernels probability measures recently applied structured data classification kernels related classic information theoretic quantities mutual information jensen shannon divergence meanwhile driven recent advances tsallis statistics nonextensive generalizations shannon's information theory proposed paper bridges trends introduce jensen tsallis difference generalization jensen shannon divergence define family nonextensive mutual information kernels allow weights assigned arguments includes boolean jensen shannon linear kernels particular illustrate performance kernels text categorization tasks

algorithm hi mat hierarchy induction via models trajectories discovers maxq task hierarchies applying dynamic bayesian network models successful trajectory source reinforcement learning task hi mat discovers subtasks analyzing causal temporal relationships actions trajectory appropriate assumptions hi mat induces hierarchies consistent observed trajectory compact value function tables employing safe abstractions demonstrate empirically hi mat constructs compact hierarchies comparable manually engineered hierarchies facilitate significant speedup learning transferred target task

minimum rank arise frequently machine learning applications notoriously difficult solve due convex nature rank objective paper online learning approach rank minimization matrices polyhedral sets particular online learning algorithms rank minimization algorithm multiplicative update method based generalized experts framework algorithm novel application online convex programming framework zinkevich 2003 latter flip role decision maker decision maker search constraint space instead feasible usually online convex programming salient feature online learning approach allows provable approximation guarantees rank minimization polyhedral sets demonstrate effectiveness methods synthetic examples real life application low rank kernel learning

address computing optimal function markov decision infinite space analyze convergence properties variations learning combined function approximation extending analysis td learning tsitsiklis van roy 1996a stochastic control settings identify conditions approximate methods converge probability 1 conclude brief discussion applicability results compare related

sampling popular scaling machine learning algorithms datasets question samples adaptive stopping algorithms monitor performance online fashion stop saving valuable resources consider probabilistic guarantees desired demonstrate recently introduced empirical bernstein bounds design stopping rules efficient provide upper bounds sample complexity rules empirical results model selection boosting filtering setting

obtaining maximum posteriori map estimate discrete random field fundamental importance computer science build tree reweighted message passing trw framework kolmogorov 2006 wainwright et al 2005 trw iteratively optimizes lagrangian dual linear programming relaxation map estimation dual formulation trw extended include cycle inequalities barahona mahjoub 1986 recently proposed cone soc constraints kumar et al 2007 propose efficient iterative algorithms solving resulting duals similar method described kolmogorov 2006 algorithms guaranteed converge test approach set synthetic data real data experiments additional constraints i.e cycle inequalities soc constraints provide results trw framework fails namely map estimation submodular energy functions

address question finding symmetries mdp isomorphism complete polynomially equivalent verifying graphs isomorphic apart theoretical importance result practical application reduction shelf graph isomorphism solver performs average symmetries mdp results using nauty graph isomorphism solver currently available symmetries mdps

algorithm exact bayes optimal classification hypothesis space decision trees satisfying leaf constraints contribution reduce classification finding rule based classifier appropriate weights rules weights computed linear time output modified frequent itemset mining algorithm means compute classifier practice despite exponential worst complexity experiments compare bayes optimal predictions maximum posteriori hypothesis

recent trend exemplar based unsupervised learning formulate learning convex optimization convexity achieved restricting set prototypes training exemplars particular clustering vector quantization mixture model density estimation paper propose novel algorithm theoretically practically superior convex formulations posing unsupervised learning single convex master convex subproblems learning tasks subproblems extremely behaved solved efficiently

class classification common classify setting threshold class probability estimates threshold determined roc curve analysis analog multi class classification learning class partitioning multiclass probability simplex minimize empirical misclassification costs analyze interplay systematic errors class probability estimates cost matrices multiclass classification explore effect class partitioning five transformations cost matrix experiments benchmark datasets naive bayes quadratic discriminant analysis effectiveness learning partition matrix compared previously proposed methods

discriminative online algorithm bounded memory growth based kernel based perceptron required memory kernel based perceptron storing online hypothesis bounded previous focused discarding instances memory bounded proposed algorithm instances discarded projected onto space spanned previous online hypothesis derive relative mistake bound compare algorithm analytically empirically art forgetron algorithm dekel et al 2007 variant algorithm called projectron outperforms forgetron variant called projectron outperforms perceptron

consider learning dissimilarities via formulations preserve specified numerical values dissimilarities dissimilarity ranking ranking learns instances similar distance larger formulations ranking algorithms semidefinite programming sdp quadratic programming qp novel capabilities approaches sample prediction scalability

modeling term dependencies time series proved difficult achieve traditional machine learning methods occurs considering music data paper introduce model rhythms based distributions distances subsequences specific implementation model considering hamming distances simple rhythm representation described proposed model consistently outperforms standard hidden markov model terms conditional prediction accuracy music databases

provide theoretical analysis chance accuracies collections classifiers examples classifier perform random chance derive theorem explicitly calculate accuracy theorem provide principled feature selection criterion sparse dimensional evaluate method microarray fmri datasets performs close optimal accuracy obtained oracle fmri dataset technique chooses relevant features successfully art method false discovery rate fdr completely fails standard significance levels

linear value function approximation equivalent form linear model approximation derive relationship model approximation error bellman error relationship guide feature selection model improvement value function improvement results insight behavior existing feature selection algorithms

absence explicit queries alternative try infer users implicit feedback signals clickstreams eye tracking formulated implicit query searches formulate task probabilistic model interpreted transfer meta learning probabilistic model demonstrated outperform earlier kernel based method scale information retrieval task

compressive sensing cs emerging pound eld appropriate conditions signi pound cantly reduce measurements required signal applications multiple signals measured multiple cs type measurements signal corresponds sensing task paper propose novel multitask compressive sensing framework based bayesian formalism dirichlet process dp prior employed yielding principled means simultaneously inferring appropriate sharing mechanisms cs inversion task variational bayesian vb inference algorithm employed estimate posterior model parameters

consider following sets unlabeled observations set label proportions predict labels set observations label proportions appears commerce spam filtering improper content detection consistent estimators reconstruct correct labels probability uniform convergence sense experiments method practice

algorithms learning rank web documents usually assume document's relevance independent documents leads learned ranking functions produce rankings redundant results contrast user studies shown diversity ranks preferred online learning algorithms directly learn diverse ranking documents based users clicking behavior algorithms minimize abandonment alternatively maximize probability relevant document found top positions ranking moreover algorithms asymptotically achieves optimal worst performance users change

finding representations text documents crucial information retrieval classification systems popular document representation based vector word counts document representation neither captures dependencies related words nor handles synonyms polysemous words paper propose algorithm learn text document representations based semi supervised autoencoders stacked form deep network model trained efficiently partially labeled corpora producing compact representations documents retaining class information joint word statistics advantageous exploit labeled samples training

body past focused tree based lp relaxation map markov random fields paper develops family super linearly convergent lp solvers based proximal minimization schemes using bregman divergences exploit underlying graphical structure scale algorithms double loop character outer loop corresponding proximal sequence inner loop cyclic bregman divergences compute proximal update inner loop updates distributed respect graph structure cast message passing algorithms establish various convergence guarantees algorithms illustrate performance rounding schemes provable optimality guarantees

propose novel bayesian multiple instance learning mil algorithm algorithm automatically identifies relevant feature subset utilizes inductive transfer learning multiple conceptually related classifiers experimental results indicate proposed mil method accurate previous mil algorithms selects set useful features inductive transfer improves accuracy classifier compared learning task individually

kernel based bayesian methods reinforcement learning rl gaussian process temporal difference gptd particularly promising rigorously treat uncertainty value function easy specify prior knowledge choice prior distribution significantly affects empirical performance learning agent little extending existing methods prior model selection online setting paper develops replacing kernel rl online model selection method gptd using sequential monte carlo methods replacing kernel rl compared standard gptd tile coding rl domains shown yield significantly asymptotic performance kernel families furthermore resulting kernels capture intuitively useful notion prior covariance nevertheless difficult capture manually

dynamic hierarchical dirichlet process dhdp developed model time evolving statistical properties sequential data sets data collected time represented via mixture associated appropriate underlying model framework hdp statistical properties data collected consecutive time linked via random parameter controls probabilistic similarity sharing mechanisms time evolving data derived relatively simple markov chain monte carlo sampler developed experimental results demonstrate model

propose family supervised dimensionality reduction sdr algorithms combine feature extraction dimensionality reduction learning predictive model unified optimization framework using data class appropriate generalized linear models glms handling classification regression approach simple closed form update rules provably convergent promising empirical results demonstrated variety dimensional datasets

modeling conditional quantiles requires specification quantile estimated viewed parameterized predictive modeling quantile loss typically indeed parameterized quantile parameter paper follow path cross validated solutions regularized kernel quantile regression bi level optimization encounter quantile convex manner optimal cross validated solution evolves parameter loss function allows tracking solution prove property construct resulting algorithm demonstrate data algorithm allows efficiently solve family bi level

lasso method finding explanatory factors suffers potential uniqueness solutions computational costs formulate conditions uniqueness lasso solutions lead easily implementable test procedure allows identify potentially active results derive efficient algorithm deal input dimensions millions approximate solution path efficiently derived methods applied scale learning exhibit excellent performance testing procedure helps avoid misinterpretations solutions

success kernel methods including support vector machines svms strongly depends design appropriate kernels initially kernels designed handle fixed length data extension unordered variable length data real pattern recognition object recognition bioinformatics focus paper object recognition using type kernel referred context dependent objects seen constellations local features regions matched minimizing energy function mixing 1 fidelity term measures quality feature matching 2 neighborhood criterion captures object geometry 3 regularization term fixed energy context dependent kernel cdk satisfies mercer condition experiments conducted object recognition plugging kernel svms outperform svms context free kernels

consider distributed reinforcement learning drl private perceptions setting agents perceptions rewards actions distributed kept private conventional drl algorithms handle multiple agents necessarily guarantee privacy preservation guarantee optimality design cryptographic solutions achieve optimal policies requiring agents share private information

deep belief networks dbn's generative models contain layers hidden variables efficient greedy algorithms learning approximate inference allowed models applied successfully application domains main building block dbn bipartite undirected graphical model called restricted boltzmann machine rbm due presence partition function model selection complexity control exact maximum likelihood learning rbm's intractable annealed importance sampling ais efficiently estimate partition function rbm novel ais scheme comparing rbm's architectures ais estimator approximate inference estimate lower bound log probability dbn model multiple hidden layers assigns test data knowledge step towards obtaining quantitative results allow directly assess performance deep belief networks generative models data

low rank matrix approximation methods provide simplest effective approaches collaborative filtering models usually fitted data finding map estimate model parameters procedure performed efficiently datasets unless regularization parameters tuned carefully approach prone overfitting single estimate parameters paper bayesian treatment probabilistic matrix factorization pmf model model capacity controlled automatically integrating model parameters hyperparameters bayesian pmf models efficiently trained using markov chain monte carlo methods applying netflix dataset consists 100 million movie ratings resulting models achieve significantly prediction accuracy pmf models trained using map estimation

tsochantaridis et al 2005 proposed formulations maximum margin training structured spaces margin scaling slack scaling margin scaling extensively requires map inference normal structured prediction slack scaling believed accurate behaved efficient variational approximation slack scaling method solves inference bottleneck retaining accuracy advantage margin scaling argue existing scaling approaches separate true labeling comprehensively generating violating constraints propose max margin trainer poslearn generates violators ensure separation position decomposable loss function empirical results real datasets illustrate poslearn reduce test error 25 margin scaling 10 slack scaling poslearn violators generated efficiently slack violators structured tasks time required twice map inference

paper investigate aspects ranking graphs augment deterministic pruning algorithm sarkar moore 2007 sampling techniques compute approximately correct rankings probability random walk based proximity measures query time prove surprising locality properties proximity measures examining short term behavior random walks proposed algorithm answer queries fly caching information entire graph empirical results 600 000 node author word citation graph citeseer domain single cpu machine average query processing time 4 quantifiable link prediction tasks techniques outperform personalized pagerank diffusion based proximity measure

probabilistic grammatical formalisms hidden markov models hmms stochastic context free grammars scfgs extensively studied widely applied fields introduce algorithmic hmms scfgs arises naturally protein rna design previously studied viewed inverse solved viterbi algorithm hmms cky algorithm scfgs study theoretically obtain algorithmic results prove np complete 3 letter emission alphabet via reduction 3 sat result implications hardness rna secondary structure design develop approaches tractable particular hmms develop branch bound algorithm shown fixed parameter tractable worst running time exponential hmm linear length structure cast mixed integer linear program

relate compressed sensing cs bayesian experimental design provide novel efficient approximate method latter based expectation propagation comparative study linearly measuring natural images simple standard heuristic measuring wavelet coefficients top systematically outperforms cs methods using random measurements sequential projection optimisation approach ji carin 2007 performs worse own approximate bayesian method able learn measurement filters images efficiently outperform wavelet heuristic knowledge ours successful attempt learning compressed sensing images realistic size contrast common cs methods framework restricted sparse signals readily applied notions signal complexity noise models concrete ideas method scaled signal representations

derive generalization bound multi classification schemes based grid clustering categorical parameter product spaces grid clustering partitions parameter space form cartesian product partitions parameters derived bound provides means evaluate clustering solutions terms generalization power built classifier classification based single feature bound serves globally optimal classification rule comparison generalization power individual features feature ranking experiments role bound precise mutual information normalized correlation indices

discuss runtime svm optimization decrease size training data increases theoretical empirical results demonstrating simple subgradient descent approach indeed displays behavior linear kernels

paper develop spectral framework estimating mixture distributions specifically gaussian mixture models physics spectroscopy identification substances spectrum treating kernel function light sampled data substance spectrum interaction eigenvalues eigenvectors kernel matrix unveils aspects underlying parametric distribution parameters gaussian mixture approach extends intuitions analyses underlying existing spectral techniques spectral clustering kernel principal components analysis kpca construct algorithms estimate parameters gaussian mixture models including mixture components means covariance matrices practical applications provide theoretical framework encouraging experimental results

haussler's convolution kernel provides successful framework engineering positive semidefinite kernels applied wide range data types applications framework data object represents finite set finer grained components haussler's convolution kernel takes pair data objects input returns sum return values predetermined primitive positive semidefinite kernel calculated pairs components input data objects hand mapping kernel introduce paper natural generalization haussler's convolution kernel input primitive kernel moves predetermined subset entire cross product plural instances mapping kernel literature positive semidefiniteness investigated manners worse sometimes incorrectly concluded exists simple easily checkable sufficient condition generic sense enables investigate positive semidefiniteness arbitrary instance mapping kernel paper proves validity condition addition introduce instances mapping kernel refer size index structure distribution kernel editcost distribution kernel naturally derived dis similarity measurements literature e.g maximum agreement tree edit distance reasonably expected improve performance existing measures evaluating distributional features peak maximum minimum features

traditional methods analyzing population structure structure program ignore influence mutational effects propose mstruct admixture population specific mixtures inheritance models addresses task structure inference mutation estimation jointly hierarchical bayesian framework variational algorithm inference validated method synthetic data analyze hgdp ceph cell line panel microsatellites rosenberg et al 2002 hgdp snp data conrad et al 2006 comparison structural maps world populations estimated mstruct structure report potentially mutation patterns world populations estimated mstruct structure

study finding dominant eigenvector sample covariance matrix additional constraints vector cardinality constraint limits zero elements negativity forces elements equal sign sparse negative principal component analysis pca applications including dimensionality reduction feature selection based expectation maximization probabilistic pca algorithm combination constraints complexity quadratic dimensions data demonstrate significant improvements performance computational efficiency compared constrained pca algorithms data sets biology computer vision finally usefulness negative sparse pca unsupervised feature selection gene clustering task

reinforcement learning architecture dyna 2 encompasses sample based learning sample based search generalises learning search apply dyna 2 performance computer domain successful planning methods based sample based search algorithms uct treated individually successful learning methods based temporal difference learning algorithms sarsa linear function approximation estimate value function formed transient computed discarded move whereas permanent slowly accumulating moves games idea dyna 2 transient planning memory permanent learning memory remain separate based linear function approximation updated sarsa apply dyna 2 9x9 computer million binary features function approximator based templates matching fragments board using transient memory dyna 2 performed uct using memories combined significantly outperformed uct program based dyna 2 achieved rating computer online server handcrafted traditional search based program

inspired co training multi view semi supervised kernel methods implement following idea function multiple reproducing kernel hilbert spaces rkhss chosen functions similar predictions unlabeled examples average prediction chosen functions performs labeled examples paper construct single rkhs data dependent co regularization norm reduces approaches standard supervised learning reproducing kernel rkhs explicitly derived plugged kernel method greatly extending theoretical algorithmic scope coregularization particular development rademacher complexity bound co regularization rosenberg bartlett 2007 follows easily wellknown results furthermore refined bounds localized rademacher complexity easily applied propose co regularization based algorithmic alternative manifold regularization belkin et al 2006 sindhwani et al 2005a leads major empirical improvements semi supervised tasks unlike recently proposed transductive approach yu et al 2008 rkhs formulation truly semi supervised naturally extends unseen test data

semi supervised learning aims taking advantage unlabeled data improve efficiency supervised learning procedures discriminative models challenging task contribution introduce original methodology using unlabeled data design simple semi supervised objective function prove corresponding semi supervised estimator asymptotically optimal practical consequences result discussed logistic regression model

moment matching popular means parametric density estimation extend technique nonparametric estimation mixture models approach embedding distributions reproducing kernel hilbert space performing moment matching space allows tailor density estimators function class i.e compute expectations density estimation approach useful applications message compression graphical models image classification retrieval

discovering additive structure step towards understanding complex multi dimensional function allows function expressed sum lower dimensional components variables interact effects additive modeled interpreted simultaneously approach interaction detection method based comparing performance unrestricted restricted prediction models restricted models prevented modeling interaction question additive model based regression ensemble additive groves restricted appropriately framework properties accurately detecting variable interactions

paper consider smoothing kernel based classification rule propose algorithm optimizing performance rule learning bandwidth smoothing kernel data dependent distance metric data dependent distance metric obtained learning function embeds arbitrary metric space euclidean space minimizing upper bound resubstitution estimate error probability kernel classification rule restricting embedding function reproducing kernel hilbert space reduce solving semidefinite program resulting kernel classification rule variation nearest neighbor rule compare performance kernel rule using learned data dependent distance metric art distance metric learning algorithms designed nearest neighbor classification benchmark datasets results proposed rule classification accuracy metric learning algorithms

bayesian network classifiers widely classification fixed bayesian network structure parameters learning approaches generative discriminative learning generative parameter learning efficient discriminative parameter learning effective paper propose simple efficient effective discriminative parameter learning method called discriminative frequency estimate dfe learns parameters discriminatively computing frequencies data empirical studies dfe algorithm integrates advantages generative discriminative learning performs art discriminative parameter learning method elr accuracy significantly efficient

canonical correlation analysis cca technique finding correlations sets multi dimensional variables projects sets variables lower dimensional space maximally correlated cca commonly applied supervised dimensionality reduction multi dimensional variables derived class label shown cca formulated squares binaryclass relationship setting remains unclear paper mild condition tends hold dimensional data cca multi label classifications formulated squares based equivalence relationship propose cca extensions including sparse cca using 1 norm regularization experiments multi label data sets confirm established equivalence relationship results demonstrate effectiveness proposed cca extensions

apprenticeship learning goal learn policy markov decision process policy demonstrated expert difficulty arises mdp's true reward function assumed unknown frame apprenticeship learning linear programming using shelf lp solver solve results substantial improvement running time existing methods magnitude faster experiments additionally approach produces stationary policies existing methods apprenticeship learning output policies mixed i.e randomized combinations stationary policies technique convert mixed policy stationary policy

support vector machine svm acknowledged powerful tool building classifiers lacks flexibility sense kernel chosen prior learning multiple kernel learning mkl enables learn kernel ensemble basis kernels combination optimized learning process propose composite kernel learning address situation distinct components rise structure kernels formulation learning encompasses setups putting emphasis structure characterize convexity learning provide wrapper algorithm computing solutions finally illustrate behavior method multi channel data correpond channels

exploration exploitation dilemma intriguing unsolved framework reinforcement learning optimism uncertainty model building play central roles advanced exploration methods integrate concepts obtain fast simple algorithm proposed algorithm near optimal policy polynomial time experimental evidence robust efficient compared ascendants

nu support vector classification nu svc algorithm shown provide intuitive interpretations e.g parameter nu roughly specifies fraction support vectors nu corresponds fraction entire range 0 1 original form settled convex extension nu svc extended method experimentally shown generalize original nu svc generalization performance convergence properties optimization algorithm studied paper provide theoretical insights issues propose novel nu svc algorithm guaranteed generalization performance convergence properties

algorithm training restricted boltzmann machines introduced algorithm named persistent contrastive divergence standard contrastive divergence algorithms aims draw samples exactly model distribution compared standard contrastive divergence pseudo likelihood algorithms tasks modeling classifying various types data persistent contrastive divergence algorithm outperforms algorithms equally fast simple

reinforcement learning rl methods based squares temporal difference lstd developed recently shown practical performance quality estimation elucidated article discuss lstd based policy evaluation view semiparametric statistical inference estimator obtained particular estimating function guarantees convergence true value asymptotically specifying model environment based observations 1 analyze asymptotic variance lstd based estimator 2 derive optimal estimating function minimum asymptotic estimation variance 3 derive suboptimal estimator reduce computational burden obtaining optimal estimating function

dimensionality reduction approaches data typically embedded euclidean latent space data sets inappropriate example human motion data expect latent spaces cylindrical toroidal poorly captured euclidean space paper range approaches embedding data euclidean latent space focus gaussian process latent variable model context human motion modeling allows learn models interpretable latent directions enabling example style content separation generalise beyond data set enabling learn transitions motion styles transitions data

infinite hidden markov model parametric extension widely hidden markov model paper introduces inference algorithm infinite hidden markov model called beam sampling beam sampling combines slice sampling limits considered time step finite dynamic programming samples trajectories efficiently algorithm typically outperforms gibbs sampler robust applications ihmm inference using beam sampler changepoint detection text prediction

previous shown difficulties learning deep generative discriminative models overcome initial unsupervised learning step maps inputs useful intermediate representations introduce motivate training principle unsupervised learning representation based idea learned representations robust partial corruption input pattern approach train autoencoders denoising autoencoders stacked initialize deep architectures algorithm motivated manifold learning information theoretic perspective generative model perspective comparative experiments surprising advantage corrupting input autoencoders pattern classification benchmark suite

brier game prediction mixable optimal learning rate substitution function resulting prediction algorithm applied predict results football tennis matches theoretical performance guarantee tight data sets especially extensive tennis data

existing sparse gaussian process g.p models seek computational advantages basing computations set basis functions covariance function g.p inputs fixed generalise gaussian covariance function basing computations gaussian basis functions arbitrary diagonal covariance matrices length scales fixed basis functions criteria additional flexibility permits approximations worse typically previously perform gradient based optimisation marginal likelihood costs sup 2 sup time data compare method various sparse g.p methods focus g.p regression central idea applicable kernel based algorithms provide results support vector machine s.v.m kernel ridge regression k.r.r approach outperforms methods particularly basis functions sparsity ratio

paper introduce novel approach manifold alignment based procrustes analysis approach differs semi supervised alignment results mapping defined suitable dimensionality reduction method training data describe evaluate approach theoretically experimentally providing results useful knowledge transfer domain novel applications method including cross lingual information retrieval transfer learning markov decision processes

consider feature extraction dimensionality reduction compositional data data vectors constrained positive constant sum real world data components variables usually complicated correlations total huge scenario demands feature extraction de correlate components reduce dimensionality traditional techniques principle component analysis pca suitable due unique statistical properties satisfy constraints compositional data paper novel approach feature extraction compositional data method identifies family dimensionality reduction projections preserve relevant constraints optimal projection maximizes estimated dirichlet precision projected data reduces compositional data lower dimensionality components lower dimensional space de correlated develop theoretical foundation approach validate effectiveness synthetic real world datasets

multiple instance learning mil instances determine bag labels essential issue algorithmically intrinsically paper mechanism instances determine bag labels application domains necessarily obey traditional assumptions mil propose adaptive framework mil adapts application domains learning domain specific mechanisms merely labeled bags approach especially attractive encountered novel application domains mechanisms unknown specifically exploit mixture models represent composition bag adaptable kernel function represent relationship bags validate synthetic mil datasets kernel function automatically adapts mechanisms instances determine bag labels compare approach art mil techniques real world benchmark datasets

graph transduction methods label input data learning classification function regularized exhibit smoothness graph labeled unlabeled samples practice algorithms sensitive initial set labels provided user instance classification accuracy drops training set contains weak labels imbalances exist label classes labeled portion data chosen random paper introduces propagation algorithm reliably minimizes cost function function graph binary label matrix cost function generalizes prior graph transduction introduces node normalization terms resilience label imbalances demonstrate global minimization function intractable instead provide alternating minimization scheme incrementally adjusts function labels towards reliable local minimum unlike prior methods resulting propagation labels prematurely commit erroneous labeling obtains consistent labels experiments shown synthetic real classification tasks including digit text recognition substantial improvement accuracy compared art semi supervised methods achieved advantage dramatic labeled instances limited

multi view learning hot topic past paper characterize sample complexity multi view active learning alpha expansion assumption exponential improvement sample complexity usual otilde 1 epsilon otilde log 1 epsilon requiring neither strong assumption data distribution data distributed uniformly unit sphere sup sup nor strong assumption hypothesis class linear separators origin upper bound error rate alpha expansion assumption hold analyze combination multi view active learning semi supervised learning improvement sample complexity finally study empirical behavior paradigms verifies combination multi view active learning semi supervised learning efficient

paper study improve nearest neighbor classification learning mahalanobis distance metric build recently proposed framework distance metric learning margin nearest neighbor lmnn classification paper makes contributions describe highly efficient solver particular instance semidefinite programming arises lmnn classification solver handle billions margin constraints hours reduce training testing times using metric ball trees speedups ball trees magnified learning low dimensional representations input space third learn mahalanobis distance metrics input space data sets locally adaptive distance metrics leads lower error rates

nonlinear embedding algorithms popular shallow semi supervised learning techniques kernel methods applied deep multilayer architectures regularizer output layer layer architecture provides simple alternative existing approaches deep learning whilst yielding competitive error rates compared methods existing shallow semi supervised techniques

exponential family psr efpsr models capture stochastic dynamical systems representing parameters exponential family distribution shortterm window future observations appealing learning perspective observed meaning expressions maximum likelihood involve hidden quantities expressive capture existing models predict models maximum likelihood learning algorithms efpsrs exist computationally feasible computationally efficient learning algorithm based approximate likelihood function algorithm interpreted attempting induce stationary distributions observations features match empirically observed counterparts approximate likelihood idea matching stationary distributions apply models

em related algorithms step computations distribute easily data items independent parameters data sets storing parameters single node step impractical framework distributes entire em procedure node interacts parameters relevant data sending messages nodes junction tree topology demonstrate improvements mapreduce topology tasks word alignment topic modeling

paper aims conduct study listwise approach learning rank listwise approach learns ranking function taking individual lists instances minimizing loss function defined predicted list ground truth list existing approach mainly focused development algorithms methods rankcosine listnet proposed performances observed unfortunately underlying theory sufficiently studied amend paper proposes conducting theoretical analysis learning rank algorithms investigations properties loss functions including consistency soundness continuity differentiability convexity efficiency sufficient condition consistency ranking result obtained related research paper conducts analysis loss functions likelihood loss cosine loss cross entropy loss latter rankcosine listnet likelihood loss leads development listwise method called listmle loss function offers properties leads experimental results

previous algorithms learning lexicographic preference models lpms produce guess lpm consistent observations approach democratic commit single lpm instead approximate target using votes collection consistent lpms variations method variable voting model voting empirically democratic algorithms outperform existing methods introduce intuitive powerful learning bias prune lpms demonstrate learning bias variable model voting learning bias improves learning curve significantly especially observations

paper extends recent popular policy evaluation algorithms generalized framework includes squares temporal difference lstd learning squares policy evaluation lspe variant incremental lstd ilstd basis extension preconditioning technique solves stochastic model equation paper studies significant issues framework rule step size computed online provides iterative apply preconditioning reduces complexity related algorithms near temporal difference td learning

extend bfgs quasi newton method limited memory variant lbfgs optimization smooth convex objectives rigorous fashion generalizing components bfgs subdifferentials local quadratic model identification descent direction wolfe line search conditions apply resulting sublbfgs algorithm l2 regularized risk minimization binary hinge loss direction finding component l1 regularized risk minimization logistic loss settings generic algorithms perform comparable counterparts specialized art solvers

retrieval tasks goal involves retrieving diverse set results e.g documents covering wide range topics search query reduces redundancy effectively information results secondly queries ambiguous level example query jaguar refer topics car feline set documents topic diversity ensures fewer users abandon query results relevant unlike existing approaches learning retrieval functions method explicitly trains diversify results particular formulate learning predicting diverse subsets derive training method based structural svms

low rank matrix approximation effective tool alleviating memory computational burdens kernel methods sampling mainstream algorithms drawn considerable attention theory practice paper detailed studies nystr ouml sampling scheme particular error analysis directly relates nystr ouml approximation quality encoding powers landmark summarizing data resultant error bound suggests simple efficient sampling scheme means clustering algorithm nystr ouml low rank approximation compare art approaches range greedy schemes probabilistic sampling algorithm achieves significant performance gains supervised unsupervised learning tasks including kernel pca squares svm

em algorithm popular iteration based method estimate parameters gaussian mixture model observation set em algorithm guaranteed converge global optimum instead stops local optimums worse global optimum usually required run multiple procedures em algorithm initial configurations return solution improve efficiency scheme propose method estimate upper bound logarithm likelihood local optimum based current configuration em iteration accomplished deriving region bounding locations local optimum followed upper bound estimation maximum likelihood estimation terminate em algorithm procedure estimated local optimum definitely worse solution seen extensive experiments method effectively efficiently accelerate conventional multiple restart em algorithm

paper cutting plane algorithm multiclass maximum margin clustering mmc proposed algorithm constructs nested sequence successively tighter relaxations original mmc optimization sequence efficiently solved using constrained concave convex procedure cccp experimental evaluations real world datasets algorithm converges faster existing mmc methods guaranteed accuracy handle larger datasets efficiently

propose laplace max margin markov networks lapm sup 3 sup class bayesian sup 3 sup bm sup 3 sup lapm sup 3 sup special sparse structural bias robust structured prediction bm sup 3 sup generalizes extant structured prediction rules based estimator bayes predictor using learnt distribution rules novel structured maximum entropy discrimination smed formalism combining bayesian max margin learning markov networks structured prediction approach subsumes conventional sup 3 sup special efficient learning algorithm based variational inference standard convex optimization solvers sup 3 sup generalization bound offered method outperforms competing ones synthetic real ocr data

