acceptance talk curious mixture personal history developing ideas context growing field ir covering decades concentrate models theories interpreted loosely try insight thinking ideas believe going.in development coined design language ir takes inspiration quantum mechanics analogy mathematical objects represent documents objects vectors density operators dimensional vector space usually hilbert space request information query observable represented linear operator space linear operators expressed matrices operator hermitian set eigenvectors forming basis space interpret view perspective understand space document vector located respect basis calculate inner product vector basis vector interpreted probability relevance probability observing eigenvector square inner product assuming vectors normalised hence connect probability observation geometry space furthermore subspaces space lattice structure equivalent logic makes entire mathematical structure language handling structure linear algebra vectors matrices projections inner products neatly captured dirac notation quantum mechanics probability slightly classical probability logic quantum logic quantum probability.a commitment mathematical structure model objects processes ir depends critical assumptions distances space objects source relationships respect relevance aboutness observation property relevance aboutness user dependent sense potential interaction specified user operator measured achieves outcomes probability determined geometry space geometry mathematical structure probability defined closely connected following theorem due gleason 1957 summarise theorem saying probability subspace simple algorithm derived projection onto subspace special operator namely statistical operator density matrix conversely probability measure subspaces encode measure uniquely algorithm powerful theorem consequences remain explored.so form abstraction research divided contributions following clustering evaluation probabilistic models logic models geometry attempted search underlying mathematical structures lead computations topics common depend construction measures space sense determines usefulness effectiveness structure clustering considers mapping metric spaces ultrametic spaces measure closeness fit evaluation starts relational conjoint structure imposes constraints measured constructs numerical representation structure leading measures probabilistic models main difficulty concerned deciding appropriate event space define probability measures significant example context attempt construct logical uncertainty principle formulated measure uncertainty incomplete logical constructs attempt left unspecified exact form measure geometry ir finally managed formulate measure projection valued measure.this thinking appear heavily influenced fairthorne 1961 brouwerian logic intuitionistic logic picked salton book ir earlier stage mackay 1950 wrote paper paper relates borderline linking experimental theoretical physics mathematical logic covers ground common theory communication goes define information operator similar scope intent hermitian operator maron collaborated mackay stated 1965 paper argued index descriptions viewed properties documents function relate documents users development ideas continued construction geometry ir.what leave attempt design language build ir system theoretical front worth considering start transition probability space hilbert space von neumann 1937 translated 1981 assumption closed linear subspaces elements logic challenged construction elements obvious form conditional probability spaces agreeing form conditionalisation intimately tied model contextuality evidence suggest contextuality plays role modelling conjuncton concepts widdows 2004 contexts modelled quantum theory beginning example gleason's theorem precludes noncontextual hidden variable theories

evaluating user preferences web search results crucial search engine development deployment maintenance real world study modeling behavior web search users predict web search result preferences accurate modeling interpretation user behavior applications ranking click spam detection web search personalization tasks key insight improving robustness interpreting implicit feedback model query dependent deviations expected noisy user behavior model clickthrough interpretation improves prediction accuracy art clickthrough methods generalize approach model user behavior beyond clickthrough results preference prediction accuracy models based clickthrough information report results scale experimental evaluation substantial improvements published implicit feedback interpretation methods

recent studies demonstrated type improvements information retrieval system effectiveness reported forums sigir trec translate benefit users studies instance recall task third question answering task unsurprising precision based measures ir system effectiveness shot query evaluation correlate user performance tasks study evaluate information retrieval tasks trec web track data precision based user task measured length time users single document relevant trec topic simple recall based task represented total relevant documents users identify five minutes users employ search engines controlled mean average precision map 55 95 results significant relationship system effectiveness measured map precision based task significant weak relationship precision document returned metric weak relationship map simple recall based task

incorporating user behavior data significantly improve top results real web search setting examine alternatives incorporating feedback ranking process explore contributions user feedback compared common web search features report results scale evaluation 3,000 queries 12 million user interactions popular web search engine incorporating implicit feedback augment features improving accuracy competitive web search ranking algorithms 31 relative original performance

similarity measures text historically tool solving information retrieval settings documents closely connected documents textual objects instance email messages connected messages via header information paper consider extended similarity metrics documents objects embedded graphs facilitated via lazy graph walk provide detailed instantiation framework email data content social networks timeline integrated structural graph suggested framework evaluated email related disambiguating names email documents threading reranking schemes based graph walk similarity measures outperform baseline methods improvements obtained appropriate learning methods

text message stream newly emerging type web data produced enormous quantities popularity instant messaging internet relay chat beneficial detecting threads contained text stream various applications including information retrieval expert recognition crime prevention despite importance research conducted due characteristics data messages usually short incomplete paper stringent definition thread detection task preliminary solution propose variations single pass clustering algorithm exploiting temporal information streams algorithm based linguistic features forward exploit discourse structure information conducted experiments compare approaches existing algorithms real dataset results variations single pass algorithm outperform basic single pass algorithm proposed algorithm based linguistic features improves performance relatively 69.5 9.7 compared basic single pass algorithm variation algorithm terms f1 respectively

searching organization's document repositories experts provides cost effective solution task expert finding strategies expert searching document collection formalized using generative probabilistic models directly models expert's knowledge based documents associated whilst locates documents topic associated expert forming reliable associations crucial performance expert finding systems consequently evaluation compare approaches exploring variety associations operational parameters topicality using trec enterprise corpora strategy consistently outperforms comparison unsupervised techniques reveals model delivers excellent performance

retrieving information conversational speech corpora call center data data comprises spontaneous speech conversations low recording quality makes automatic speech recognition asr highly difficult task typical call center data art vocabulary continuous speech recognition systems produce transcript word error rate 30 addition output transcript advanced systems provide word confusion networks wcns compact representation word lattices associating word hypothesis posterior probability exploits information provided wcns improve retrieval performance paper mean average precision map improved using wcns compared raw word transcripts finally analyze effect increasing asr word error rate search effectiveness map reasonable extremely error rate

automated singer identification organising browsing retrieving data music databases paper propose novel scheme called ybrid inger dentifier hsi automated singer recognition hsi effectively multiple low level features extracted vocal vocal music segments enhance identification process hybrid architecture build profiles individual singer characteristics based statistical mixture models extensive experimental results conducted music database demonstrate superiority method art approaches

paper proposes novel framework music content indexing retrieval music structure information i.e timing harmony music region content represented layers music structure pyramid begin extracting layered structure information analyze rhythm music segment signal proportional inter beat intervals timing information incorporated segmentation process call beat space segmentation describe harmony events propose layer hierarchical approach model music chords model progression instrumental vocal content acoustic events information extraction propose vector space modeling approach events indexing terms query example music retrieval query represented vector statistics gram events propose effective retrieval models hard indexing scheme soft indexing scheme experiments vector space modeling effective representing layered music information achieving 82.5 top 5 retrieval accuracy using 15 sec music clips queries soft indexing outperforms hard indexing

website organizational structures web effectively rank websites essential web applications web search crawling ranks websites researchers describe inter connectivity websites called hostgraph nodes denote websites edges denote linkages websites hyperlinks pages website pages edge websites adopted random walk model hostgraph paper random walk hostgraph reasonable accordance browsing behavior web surfers derivate rank represent true probability visiting corresponding website.in mathematically proved probability visiting website random web surfer equal sum pagerank values pages inside website nevertheless web pages larger websites feasible base calculation ranks websites calculation pagerank tackle proposed novel method named aggregaterank rooted theory stochastic complement approximate sum pagerank accurately lower computational complexity pagerank theoretical analysis experimental evaluation aggregaterank method ranking websites previous methods

approach improving precision initial document ranking wherein utilize cluster information graph based framework main idea perform reranking based centrality bipartite graphs documents clusters premise mutually reinforcing entities links entities created via consideration language models induced them.we cluster document graphs rise retrieval performance previously proposed document graphs example authority based reranking documents via hits style cluster based approach outperforms previously proposed pagerank inspired algorithm applied solely document graphs moreover computing authority scores clusters constitutes effective method identifying clusters containing percentage relevant documents

traditional web link based ranking schemes single score measure page's authority concern community authority derived result resource highly popular topic dominate results topic authoritative address suggest calculating score vector page distinguish contribution topics using random walk model probabilistically combines page topic distribution link structure incorporate topical model pagerank hits affecting overall property render insight topic level transition experiments multiple datasets indicate technique outperforms ranking approaches incorporate textual analysis

despite intuitive appeal hypothesis retrieval level concepts outperform purely term based approaches remains unverified empirically addition knowledge consistently resulted performance gains identifying reasons previous negative results novel framework conceptual retrieval articulates types knowledge information seeking instantiate framework domain clinical medicine based principles evidence based medicine ebm experiments ebm based scoring algorithm dramatically outperforms art baseline employs term statistics ablation studies yield understanding performance contributions components finally discuss domains benefit knowledge based approaches

paper investigates stringent athematical formalism parallel derivation grand probabilistic retrieval models binary independent retrieval bir poisson model pm language modelling lm investigation motivated questions firstly sharing origin namely probability relevance models respect event spaces captured consistent notation relate event spaces secondly bir pm closely related lm fit thirdly tf idf probabilistic models related parallel investigation models leads formalised results bir pm assume collection set relevant documents whereas lm assumes collection set terms relevant documents pm viewed bridge connecting bir lm bir lm equivalence explains bir special lm pm explains tf idf bir lm probabilities express tf idf dual

common limitation retrieval models including recently proposed axiomatic approaches retrieval scores solely based exact i.e syntactic matching terms queries documents allowing distinct semantically related terms match contribute retrieval score paper semantic term matching naturally incorporated axiomatic retrieval model defining primitive weighting function based semantic similarity function terms define desirable retrieval constraints semantic term matching constraints extend axiomatic model directly support semantic term matching based mutual information terms computed document set extension efficiently implemented query expansion experiment results representative data sets mutual information computed documents target collection retrieval external collection web semantic expansion consistently substantially improves retrieval accuracy baseline axiomatic retrieval model pseudo feedback method method outperforms art language modeling feedback method

set independently developed spam filters combined simple provide substantially filtering individual filters results fifty spam filters evaluated trec 2005 spam track combined post hoc simulate parallel line operation filters combined results evaluated using trec methodology yielding factor improvement filter simplest method averaging binary classifications returned individual filters yields remarkably result method averaging log odds estimates based scores returned individual filters yields somewhat result provides input svm logistic regression based stacking methods stacking methods appear provide improvement corpora stacking methods logistic regression yields result finally select priori subsets filters combined outperform individual filter substantial margin

web query classification qc aims classify web users queries short ambiguous set target categories qc applications including page ranking web search targeted advertisement response queries personalization paper novel approach qc outperforms winning solution acm kddcup 2005 competition objective classify 800,000 real user queries approach build bridging classifier intermediate taxonomy offline mode classifier online mode map user queries target categories via intermediate taxonomy major innovation leveraging similarity distribution intermediate taxonomy retrain classifier set target categories bridging classifier trained addition introduce category selection method narrowing scope intermediate taxonomy based classify queries category selection improve efficiency effectiveness online classification combining algorithm winning solution kddcup 2005 improvement 9.7 3.8 terms precision f1 respectively compared results kddcup 2005

data fusion combination results independent searches document collection single output result set shown past greatly improve retrieval effectiveness individual results.this paper probfuse probabilistic approach data fusion probfuse assumes performance individual input systems training queries indicative future performance fused result set based probabilities relevance calculated training process retrieval experiments using data trec ad hoc collection demonstrate probfuse achieves results superior popular combmnz fusion algorithm

study effect user supplied relevance feedback improving web search results using query refinement document similarity measures rerank results web graph distance documents robust measure relative relevancy demonstrate metric improve rankings result urls user rates document dataset research suggests interactive systems significantly improve search results

information retrieval algorithms leverage various collection statistics improve performance statistics computed relatively evaluation corpus believe using larger evaluation corpora improve performance specifically advocate incorporating external corpora based language modeling refer process external expansion compared traditional pseudo relevance feedback techniques external expansion stable topics 10 effective terms mean average precision results using quality corpus comparable evaluation corpus effective using web results external expansion outperforms simulated relevance feedback addition propose method predicting extent external expansion improve retrieval performance measure demonstrates positive correlation improvements mean average precision

pseudo relevance feedback proven effective strategy improving retrieval accuracy retrieval models performance existing pseudo feedback methods affected significantly parameters feedback documents relative weight original query terms parameters set trial error guidance paper robust method pseudo feedback based statistical language models main idea integrate original query feedback documents single probabilistic mixture model regularize estimation language model parameters model information feedback documents gradually added original query unlike existing feedback methods method parameter tune experiment results representative data sets method significantly robust art baseline language modeling approach feedback comparable retrieval accuracy

semantic smoothing incorporates synonym sense information language models effective potentially significant improve retrieval performance implemented semantic smoothing models translation model statistically maps document terms query terms followed shown experimental results models unable incorporate contextual information resulting translation mixed fairly overcome limitation propose novel context sensitive semantic smoothing method decomposes document query set weighted context sensitive topic signatures translate topic signatures query terms detail solve 1 choosing concept pairs topic signatures adopting ontology based approach extract concept pairs 2 estimating translation model topic signature using em algorithm 3 expanding document query models based topic signature translations smoothing method evaluated trec 2004 05 genomics track collections significant improvements obtained map mean average precision achieves 33.6 maximal gain simple language model 7.8 gain language model context insensitive semantic smoothing

search algorithms incorporating form topic model history information retrieval example cluster based retrieval studied 60s recently produced results language model framework approach building topic models based formal generative model documents latent dirichlet allocation lda heavily cited machine learning literature feasibility effectiveness information retrieval unknown paper study efficiently lda improve ad hoc retrieval propose lda based document model language modeling framework evaluate trec collections gibbs sampling employed conduct approximate inference lda computational complexity analyzed improvements retrieval using cluster based models obtained reasonable efficiency

paper concerned applying learning rank document retrieval ranking svm typical method learning rank factors consider applying ranking svm learning rank method document retrieval correctly ranking documents top result list crucial information retrieval system conduct training ranked results accurate relevant documents vary query query avoid training model biased queries relevant documents previously existing methods include ranking svm applied document retrieval none factors consideration modifications conventional ranking svm document retrieval specifically modify hinge loss function ranking svm deal described employ methods conduct optimization loss function gradient descent quadratic programming experimental results method referred ranking svm ir outperform conventional ranking svm existing methods document retrieval datasets

paper study statistical query translation models units translation begin review word based translation model co occurrence statistics resolving translation ambiguities translation selection formulated framework graphic model resorting modeling assumptions limitations co occurrence model discussed research finding translation units motivated models larger linguistically motivated translation units i.e noun phrase dependency triple model modeling training methods described detail query translation models evaluated using trec collections results larger translation units lead specific models usually achieve translation cross language information retrieval results

paper introduces framework translation probabilities cross language information retrieval based notion information retrieval fundamentally requires matching searcher means author document meant perspective yields computational formulation provides natural combining query document translation recognized techniques shown special model restrictive assumptions cross language search results reported statistically indistinguishable strong monolingual baselines french chinese documents

role network structure grown significance past ten field information retrieval stimulated extent importance link analysis development web search techniques 4 body focused primarily network visible web network hyperlinks connecting documents documents web contained network explicit equally social network users latent person person links encoding variety relationships including friendship information exchange influence developments past including emergence social networking systems rich social media availability scale mail instant messenging datasets highlighted crucial role played line social networks time easier uncover analyze considerable opportunity exploit information content inherent networks prospect raises research challenge.within context focus recent efforts formalize searching social network goal capture issues underlying variety related scenarios social networking system myspace seeks piece information held friend friend 27 28 employee company searches network colleagues expertise particular subject 9 node decentralized peer peer file sharing system queries file hops 2 6 16 17 user distributed ir federated search setting traverses network distributed resources connected links informational economic contractual 3 5 7 8 13 18 21 basic forms scenarios essential features common node network global knowledge short path desired target node target nodes frame underlying pieces empirical social network analysis stanley milgram's research world phenomenon six degrees separation 19 24 25 form milgram's experiments randomly chosen starters forward letter designated target individual established short chains connecting flung pairs people abundant social networks individuals networks operating purely local information own friends acquaintances able actually chains 10 milgram experiments constituted earliest indication scale social networks structured support type decentralized search family random graph models proposed watts strogatz 26 shown ability network support type decentralized search depends subtle range connections correlated underlying spatial organizational structure embedded 10 11 recent studies using data communication organizations 1 friendships line communities 15 established striking real social networks closely match structural features predicted mathematical models.if looks line settings provide initial motivation issues directions term economic implications essentially consequences follow viewing distributed information retrieval applications peer peer systems social networking sites providing marketplaces information services decentralized search network change participants simply agents following fixed algorithm strategic actors decisions own self demand compensation taking protocol considerations bring realm algorithmic game theory active current research game theoretic notions quantify performance systems participants follow own self 20 23 simple model decentralized search presence incentives performance depends crucially rarity information richness network topology 12 network structurally impoverished enormous investment required produce path query answer

paper proposes probabilistic model definitional question answering qa reflects characteristics definitional question intention definitional question request definition question target answer definitional question contain content relevant topic target representation form definition style modeling definitional qa topic definition viewpoints proposed probabilistic model converts task answering definitional questions estimating language models topic language model definition language model language model proposed model systematically combines evidences probabilistic framework experimental results definitional qa system based proposed probabilistic model comparable art systems

novel framework answering complex questions relies question decomposition complex questions decomposed procedure operates markov chain following random walk bipartite graph relations established concepts related topic complex question subquestions derived topic relevant passages manifest relations decomposed questions discovered random walk submitted art question answering system retrieve set passages merged comprehensive answer multi document summarization mds system evaluations access decompositions generated using method significantly enhance relevance comprehensiveness summary length answers complex questions

types document collections developed various web services service providers track textual features click counts paper framework textual features predict quality documents quality measure successfully incorporated language modeling based retrieval model test approach collection question answer pairs gathered community based question answering service people answer questions experimental results using quality measure significant improvement baseline

co occurrence data common real applications latent semantic analysis lsa successfully identify semantic relations data lsa handle single co occurrence relationship types objects practical applications multiple types objects exist pair objects pairwise co occurrence relation co occurrence relations exploited alleviate data sparseness represent objects meaningfully paper propose novel algorithm lsa conducts latent semantic analysis incorporating pairwise co occurrences multiple types objects based mutual reinforcement principle lsa identifies salient concepts co occurrence data represents objects unified semantic space lsa variants lsa special algorithm experiment results lsa outperforms lsa multiple applications including collaborative filtering text clustering text categorization

paper studies identifying comparative sentences text documents related sentiment opinion sentence identification classification sentiment classification studies classifying document sentence based subjective opinion author application sentiment opinion identification business intelligence product manufacturer consumers opinions products comparisons hand subjective objective furthermore comparison concerned object isolation instead compares object example opinion sentence sound quality cd player poor example comparative sentence sound quality cd player cd player sentences information language constructs identifying comparative sentences useful practice direct comparisons convincing evaluation opinions individual object paper proposes study comparative sentence identification categorizes comparative sentences types novel integrated pattern discovery supervised learning approach identifying comparative sentences text documents experiment results using types documents news articles consumer reviews products internet forum postings precision 79 recall 81 detailed results paper

machine learning mainstay text classification successful techniques defeated real world applications strong time varying component advance research challenging promote natural experimental framework daily classification task applied time based datasets reuters rcv1 paper dissect concept drift main subtypes demonstrate via novel visualization recurrent themes subtype rcv1 understanding led develop learning model transfers induced knowledge time benefit future classifier learning tasks method avoids main existing inductive transfer scalability risk negative transfer empirical tests consistently 10 measure improvement reuters categories tested

standard information retrieval ir metrics assume simple model documents understood independent units assumption adapted paradigms xml web ir retrievable informations documents sets related documents moreover classical hypotheses assumes user ignores structural logical context document elements hence possibility navigation units eprum generalisation precision recall pr aims allowing user navigate browse corpus structure cumulated gain metrics able handle continuous valued relevance apply compare eprum context xml retrieval active field evaluation metrics explain eprum ir paradigms

accurate estimation information retrieval evaluation metrics average precision require sets relevance judgments building sets evaluation real world implementations inefficient worst infeasible link evaluation test collection construction gain understanding minimal judging effort confidence outcome evaluation looking average precision leads natural algorithm selecting documents judge allows estimate degree confidence defining distribution document judgments study annotators method researchers rank set systems hours 95 confidence information retrieval metrics average precision require sets relevance judgments accurately estimated building sets infeasible inefficient real world retrieval implementations looking average precision allows estimate confidence evaluation based size test collection build algorithm selecting documents judge maximum confidence evaluation minimal relevance judgments study annotators algorithm researchers quickly rank set systems 95 confidence

existing methods measuring quality search algorithms static collection documents set queries mapping queries relevant documents allow experimenter search engines engine configurations retrieve correct answers methodology assumes document set set relevant documents unchanging paper abandon static collection requirement begin recent trec collection created web crawl analyze documents collection changed time determine decay document collection affects trec systems results experiment using decayed collection measure live web search system employ novel measures search effectiveness robust despite incomplete relevance information lastly propose methodology collection maintenance supports measuring search performance single system systems run time

broder et al 3 shingling algorithm charikar's 4 random projection based approach considered art algorithms finding near duplicate web pages algorithms developed popular web search engines compare algorithms scale namely set 1.6b distinct web pages results neither algorithms finding near duplicate pairs site achieve precision near duplicate pairs sites charikar's algorithm near duplicate pairs sites achieves precision overall namely 0.50 versus 0.38 broder et al algorithm combined algorithm achieves precision 0.79 79 recall algorithms

web ir digital library applications require crawling process collect pages ultimate goal taking advantage useful information available web sites applications criteria determine page collection related page content situations inner structure pages provides criteria guide crawling process content paper structure driven approach generating web crawlers requires minimum effort users idea input sample page entry web site generate structure driven crawler based navigation patterns sequences patterns links crawler follow reach pages structurally similar sample page experiments carried structure driven crawlers generated approach able collect pages match samples including pages added generation

objective web forums create shared space communications discussions specific topics issues tremendous information forum sites utilized links forum pages automatically created means link based ranking algorithm applied efficiently paper proposed novel ranking algorithm tries introduce content information link based methods implicit links basic idea derived focused random surfer surfer jump page similar reading currently manner allowed introduce content similarities link graph personalization bias method named fine grained rank fgrank efficiently computed based automatically generated topic hierarchy topic sensitive pagerank method compute single pagerank score page contribution paper efficient algorithm automatically generating topic hierarchy map page scale collection onto computed hierarchy experimental results proposed method improve retrieval performance reveal content based link graph compared hyper link graph

paper introduces family link based ranking algorithms propagate page importance links algorithms damping function decreases distance direct link implies endorsement link path pagerank widely ranking function family.the main objective paper determine family ranking techniques se choices damping function impact rank quality convergence speed results suggest pagerank approximated simpler forms rankings computed efficiently focus speculative nature aims separating kernel pagerank link based importance propagation propagation decays paths.we focus damping functions linear exponential hyperbolic decay lengths paths exponential decay corresponds pagerank functions presentation includes algorithms analysis comparisons experiments study behavior parameters real web graph data.among results calculate linear approximation induces page identical pagerank's using fixed iterations comparisons performed using kendall's 964 domain datasets

modern distributed information retrieval techniques require accurate knowledge collection size cooperative environments detailed collection statistics available size underlying collections estimated approaches estimation collection size proposed accuracy thoroughly evaluated empirical analysis past estimation approaches variety collections demonstrates prediction accuracy low motivated ecological techniques estimation animal populations propose approaches estimation collection size approaches significantly accurate previous methods efficient resources required perform estimation

combining output multiple retrieval sources document collection importance retrieval tasks multimedia retrieval web retrieval meta search merge retrieval sources adaptively according query topics propose series approaches called probabilistic latent query analysis plqa associate identical combination weights latent classes underlying query space compared previous query independent query class based combination methods proposed approaches advantage able discover latent query classes automatically using prior human knowledge assign query mixture query classes determine query classes model selection principle experimental results retrieval tasks i.e multimedia retrieval meta search demonstrate proposed methods uncover sensible latent classes training data achieve considerable performance gains

user modeling information retrieval studied improve effectiveness information access centralized repositories paper explore user modeling context text federated search peer peer networks approach models user's persistent term based past queries model improve search efficiency future queries represent similar past queries approach enables queries representing user's transient ad hoc automatically recognized search queries rely relatively search radius avoid sacrificing effectiveness efficiency experimental results demonstrate approach significantly improve efficiency text federated search degrading accuracy furthermore proposed approach require amount training data robust range parameter values

adaptive distributed query sampling framework quality conscious extracting quality text database samples framework divides query based sampling process initial seed sampling phase quality aware iterative sampling phase phase sampling process dynamically scheduled based estimated database size quality parameters derived previous sampling process unique characteristic adaptive query based sampling framework self learning self configuring ability based overall quality text databases consideration introduce quality conscious sampling schemes estimating database quality initial results proposed framework supports quality document sampling existing approaches

scale web text retrieval systems deal amounts data greatly exceed capacity single machine handle data volumes query throughput rates parallel systems document index data split tightly clustered distributed computing systems index data distributed document term paper examine methods load balancing term distributed parallel architectures propose suite techniques reducing net querying costs combination techniques describe allow 30 improvement query throughput tested eight node parallel computer system

family hybrid index maintenance strategies line index construction monotonically growing text collections strategies improve recent results hybrid index maintenance dynamic text retrieval systems previous techniques method distinguishes short posting lists short lists maintained using merge strategy lists kept separate updated costly relocations posting lists avoided.we discuss shortcomings previous hybrid methods experimental evaluation technique index maintenance performance superior earlier methods especially amount main memory available indexing system complexity analysis proves zipfian term distribution asymptotical disk accesses performed hybrid maintenance strategy linear size text collection implying asymptotical optimality proposed strategy

consider following text search autocompletion feature imagine user search engine typing query letter typed instant display completions query word lead hits time hits completions displayed indexing data structures apply incur processing times substantial class queries lot space indexing data structure space art compressed inverted index 10 times faster query processing times trec terabyte collection comprises 25 million documents achieve single machine index disk average response times tenth built fledged interactive search engine realizes proposed autocompletion feature combined support proximity search semi structured xml text subword phrase completion semantic tags

exhaustive evaluation ranked queries expensive particularly subset overall ranking required queries contain common terms concern rise techniques dynamic query pruning methods eliminating redundant usual exhaustive evaluation generating demonstrably set answers query propose pruning methods impact sorted indexes compared exhaustive evaluation methods reduce amount computation performed reduce amount memory required accumulators reduce amount data transferred disk time allow performance guarantees terms precision mean average precision strong claims experiments using trec terabyte collection queries

noabstract

classical query expansion techniques local context analysis lca term co occurrence statistics incorporate additional contextual terms enhancing passage retrieval relevant contextual terms co occur frequently query terms vice versa hence methods brings noise leads reduced precision previous studies demonstrated importance relationship analysis natural language queries passage retrieval found query expansion performance satisfactory short queries paper novel query expansion techniques dependency relation analysis extract contextual terms relations external corpuses techniques enhance performance density based relation based passage retrieval frameworks respectively compare performance resulting systems lca density based passage retrieval system dbs relation based system query expansion rbs using factoid questions trec 12 qa task results terms mrr scores relation based term expansion method dbs outperforms lca 9.81 relation expansion method outperforms rbs 17.49

tries answer question makes query difficult addresses novel model captures main components topic relationship components topic difficulty components topic textual expression describing information query queries set documents relevant topic qrels entire collection documents experimentally topic difficulty strongly depends distances components absence knowledge model components model useful approximating missing component based components demonstrate applicability difficulty model predicting query difficulty predicting topic aspects expected covered search results analyzing findability specific domain

growing estimating effectiveness search approaches typically considered examining search queries examining retrieved document sets paper latter approach measures characterize retrieved document sets estimate quality search measures clustering tendency measured cox lewis statistic ii sensitivity document perturbation iii sensitivity query perturbation iv local intrinsic dimensionality experimental results task ranking 200 queries according search effectiveness trec discs 4 5 dataset ranking queries compared ranking based average precision using kendall statistic individual estimator sensitivity document perturbation yields kendall 0.521 combined clustering tendency based cox lewis statistic query perturbation measure results kendall 0.562 knowledge correlation average precision reported date

document clustering tool text analysis applications propose incorporate prior knowledge cluster membership document cluster analysis develop novel semi supervised document clustering model method models set documents weighted graph document represented vertex edge connecting pair vertices weighted similarity value corresponding documents prior knowledge indicates pairs documents belong cluster prior knowledge transformed set constraints document clustering task accomplished finding cuts graph constraints apply model normalized cut method demonstrate idea concept experimental evaluations proposed document clustering model reveals remarkable performance improvements limited training samples hence effective semi supervised classification tool

text clustering commonly treated automated task user feedback variety researchers explored mixed initiative clustering methods allow user interact advise clustering algorithm mixed initiative approach especially attractive text clustering tasks user trying organize corpus documents clusters particular purpose e.g clustering email folders reflect various activities involved paper introduces approach mixed initiative clustering handles natural types user feedback introduce probabilistic generative model text clustering speclustering model outperforms commonly mixture multinomials clustering model autonomous mode user input describe incorporate distinct types user feedback clustering algorithm provide experimental evidence substantial improvements text clustering user feedback incorporated

task near duplicated document detection traditional fingerprinting techniques database community bag word comparison approaches information retrieval community sufficiently accurate due characteristics near duplicated documents identical documents data cleaning task relevant documents search task paper instance level constrained clustering approach near duplicate detection framework incorporates information document attributes content structure clustering process form near duplicate clusters gathered collections public comments sent u.s government agencies proposed regulations experimental results demonstrate approach outperforms near duplicate detection algorithms effective human assessors

noabstract

precision top ranks focus research information retrieval paper multiple nested ranker approach improves accuracy top ranks iteratively re ranking top scoring documents iteration approach ranknet learning algorithm re rank subset results splits easier tasks generates distribution results learned algorithm evaluate approach using settings data set labeled degrees relevance normalized discounted cumulative gain ndcg measure performance depends position relevance score document ranked list experiments learning algorithm concentrate top scoring results improves precision top ten documents terms ndcg score

ir applications desirable adopt precision search strategy return set documents highly focused relevant user's information applications mind investigate semantic search using xml fragments query language text corpora automatically pre processed encode semantic information useful retrieval identify xml fragment operations applied query conceptualize restrict relate terms query demonstrate operations address query time semantic specify target information type disambiguate keywords specify search term context relate select terms query demonstrate effectiveness semantic search technology series experiments using applications embed technology yields significant improvement precision search results

term relevance feedback history information retrieval research interactive term relevance feedback yielded mixed results paper investigate aspects related elicitation term relevance feedback display document surrogates technique identifying selecting terms sources expansion terms conduct subjects experiment 61 term relevance feedback interfaces using 2005 trec hard collection evaluate interface respect query length retrieval performance results demonstrate queries created experimental interface significantly outperformed corresponding baseline queries differences performance interface conditions results demonstrate pseudo relevance feedback runs outperformed baseline experimental runs assessed recall oriented measures user generated terms improved precision

search systems time provided users ability request documents similar document interfaces provide feature via link button document search results call feature similar similarity browsing examined similar search tool relevance feedback improving retrieval performance investigation focused similar's document document similarity reexamination documents search user's browsing pattern similar query biased similarity avoiding reexamination documents breadth browsing pattern achieved 23 increase arithmetic mean average precision 66 increase geometric mean average precision baseline retrieval performance matched traditionally styled iterative relevance feedback technique

single iteration clarification dialogs implemented trec hard track represent attempt introduce interaction ad hoc retrieval preserving benefits scale evaluations previous experiments conclusively demonstrated performance gains resulting interactions unclear findings speak nature clarification dialogs simply limitations current systems probe limits interactions employed human intermediary formulate clarification questions exploit user responses addition establishing plausible upper bound performance able induce ontology clarifications characterize human behavior ontology serves input regression model attempts determine types clarification questions helpful serve inform design interactive systems initiate user dialogs

scale learning realistic semi supervised setting set labeled examples available collection unlabeled data information retrieval data mining applications linear classifiers strongly preferred ease implementation interpretability empirical performance family semi supervised linear support vector classifiers designed handle partially labeled sparse datasets possibly examples features core algorithms employ recently developed modified finite newton techniques contributions paper follows provide implementation transductive svm tsvm significantly efficient scalable currently dual techniques linear classification involving sparse datasets propose variant tsvm involves multiple switching labels experimental results variant provides magnitude improvement training efficiency algorithm semi supervised learning based deterministic annealing da approach algorithm alleviates local minimum tsvm optimization procedure computationally attractive conduct empirical study document classification tasks confirms value methods scale semi supervised settings

automatic classification data items based training samples boosted considering neighborhood data items graph structure e.g neighboring documents hyperlink environment co authors publications bibliographic data entries paper method graph based classification particular emphasis hyperlinked text documents broader applicability approach based iterative relaxation labeling combined bayesian svm classifiers feature spaces data items graph neighborhood consideration exploit locality patterns time avoiding overfitting contrast prior lines approach employs novel techniques dynamically inferring link class pattern graph run iterative relaxation labeling judicious pruning edges neighborhood graph based node dissimilarities node degrees weighting influence edges based distance metric classification labels weighting edges content similarity measures techniques considerably improve robustness accuracy classification outcome shown systematic experimental comparisons previously published methods real world datasets

supervised learning approaches text classification practice required unsystematically collected training sets alternative supervised learning usually viewed building classifiers hand using domain expert's understanding features text related class expensive requires degree sophistication linguistics classification makes difficult combinations weak predictors propose instead combining domain knowledge training examples bayesian framework domain knowledge specify prior distribution parameters logistic regression model labeled training data produce posterior distribution mode final classifier text categorization data sets approach rescue otherwise disastrously bad training situations producing effective classifiers

memory based methods collaborative filtering predict ratings averaging weighted ratings respectively pairs similar users items practice ratings similar users similar items available due sparsity inherent rating data consequently prediction quality poor paper re formulates memory based collaborative filtering generative probabilistic framework treating individual user item ratings predictors missing ratings final rating estimated fusing predictions sources predictions based ratings item users predictions based item ratings user third ratings predicted based data similar users rating similar items existing user based item based approaches correspond simple framework complete model robust data sparsity types ratings concert additional ratings similar users towards similar items employed background model smooth predictions experiments demonstrate proposed methods indeed robust data sparsity recommendations

propose information access behavior people modeled information flow issue people intentionally unintentionally influence inspire creating retrieving getting specific information product information flow models information propagated social network real social network interactions people reside moreover virtual social network people influence unintentionally instance collaborative filtering leverage users access patterns model information flow generate effective personalized recommendations adoption based information flow eabif network describes influential relationships people based adoption typically category specific propose topic sensitive eabif teabif network access patterns clustered respect categories item accessed adopters personalized recommendations achieved estimating whom information propagated probabilities experiments online document recommendation system results demonstrate eabif teabif respectively achieve improved precision recall 91.0 87.1 108.5 112.8 compared traditional collaborative filtering adopter exists

collaborative filtering techniques popular past decade effective help people deal information overload recent research identified significant vulnerabilities collaborative filtering techniques shilling attacks attackers introduce biased ratings influence recommendation systems shown effective memory based collaborative filtering algorithms examine effectiveness popular shilling attacks random attack average attack model based algorithm singular value decomposition svd learn low dimensional linear model results svd based algorithm resistant shilling attacks memory based algorithms furthermore develop attack detection method directly built svd based algorithm method detects random shilling attacks detection rates low false alarm rates

paper describes bootstrap approach statistics applied evaluation ir effectiveness metrics argue bootstrap hypothesis tests deserve attention ir community based fewer assumptions traditional statistical significance tests describe straightforward methods comparing sensitivity ir metrics based bootstrap hypothesis tests unlike heuristics based swap method proposed voorhees buckley method estimates performance difference required achieve significance level directly bootstrap hypothesis test results addition describe simple examining accuracy rank correlation metrics based bootstrap estimate standard error demonstrate usefulness methods using test collections runs ntcir clir track comparing seven ir metrics including handle graded relevance based geometric mean

introduce validate bootstrap techniques compute confidence intervals quantify effect test collection variability average precision ap mean average precision map ir effectiveness measures consider test collection ir evaluation representative population materially similar collections documents drawn infinite pool similar characteristics model accurately predicts degree concordance system results randomly selected halves trec 6 ad hoc corpus advance framework statistical evaluation framework model sources chance variation source input meta analysis techniques

consider scale retrieval evaluation propose statistical method evaluating retrieval systems using incomplete judgments unlike existing techniques 1 rely effectively complete prohibitively expensive relevance judgment sets 2 produce biased estimates standard performance measures 3 produce estimates standard measures correlated standard measures proposed statistical technique produces unbiased estimates standard measures themselves.our proposed technique based random sampling estimates unbiased statistical design variance dependent sampling distribution employed derive sampling distribution yield low variance estimates test proposed technique using benchmark trec data demonstrating sampling pool derived set runs efficiently effectively evaluate runs sampling pools generalize unseen runs experiments indicate highly accurate estimates standard performance measures obtained using relevance judgments 4 typical trec style judgment pool

content targeted advertising task automatically associating ads web page constitutes key web monetization strategy nowadays introduces challenging technical raises questions instance design ranking functions able satisfy conflicting goals selecting advertisements ads relevant users suitable profitable publishers advertisers paper propose framework associating ads web pages based genetic programming gp gp method aims learning functions select appropriate ads contents web page ranking functions designed optimize overall precision minimize misplacements using real ad collection web pages newspaper obtained gain art baseline method 61.7 average precision evolving individuals provide ranking estimations gp able discover ranking functions effective placing ads web pages avoiding irrelevant ones

searches web transactional intent argue pages satisfying transactional distinguished common pages information links execute transaction based hypothesis provide recipe constructing transaction annotator constructing annotator corpus demonstrating classification performance establish robustness finally experimentally search procedure exploits pre annotation greatly outperforms traditional search retrieving transactional pages

today's data rich networked world people express aspects lives online common segregate aspects write opinionated rants movies blog pseudonym participating forum web site scholarly discussion medical ethics real name link separate identities movies journal articles authors mention sparse relation space properties e.g items related users allow re identification re identification violates people's intentions separate aspects life negative consequences allow privacy violations obtaining stronger identifier name address.this paper examines specific setting re identification users public web movie forum private movie ratings dataset major results develop algorithms re identify proportion public users sparse relation space evaluate private dataset owners protect user privacy hiding data requires extensive undesirable changes dataset impractical third evaluate methods users public forum protect own privacy suppression misdirection suppression doesn't simple misdirection strategy mention popular items haven't rated

usual approach automatic summarization sentence extraction key sentences input documents selected based suite features word frequency feature summarization impact system performance isolated paper study contribution summarization factors related frequency content word frequency composition functions estimating sentence importance word frequency adjustment frequency weights based context carry analysis using datasets document understanding conferences studying impact features automatic summarizers role human summarization research frequency based summarizer achieve performance comparable art systems composition function context sensitivity improves performance significantly reduces repetition

information graphics pictorial graphics bar charts line graphs depict attributes entities relations entities information graphics appearing popular media communicative goal intended message consequently information graphics constitute form language paper argues information graphics valuable knowledge resource retrievable digital library graphics account summarizing multimodal document subsequent indexing retrieval accomplish information graphic understood message recognized paper bayesian system recognizing primary message information graphic simple bar charts discusses potential role information graphic's message indexing graphics summarizing multimodal documents

evaluation novel hierarchical text summarization method allows users view summaries web documents mobile devices unlike previous approaches ours require documents html infers hierarchical structure automatically currently method summarize news articles sent web mail account plain text format subjects web enabled mobile phone emulator access account's inbox view summarized news articles summaries complete information seeking tasks involved answering factual questions stories comparing hierarchical text summary setting subjects text articles significant difference task accuracy time complete task hierarchical summarization setting bytes transferred user request half text finally comparing method summarization methods subjects achieved significantly accuracy tasks using hierarchical summaries

clustering search results feature today's information retrieval applications notion hit list clustering appears web search engines enterprise search engines mechanism allows users explore coverage query little exposing temporal attributes constructing presentation clusters attributes appear documents textual content e.g date time token temporal reference sentence paper outline model describe prototype main ideas

developed prototype integrated retrieval aggregation diverse information contained scanned paper documents complex document information processing combines forms image processing textual linguistic processing enable effective analysis complex document collections necessity wide range applications system attempt integrated retrieval complex documents report current capabilities

consider evaluating retrieval systems using limited relevance judgments recent demonstrated accurately estimate average precision via judged pool corresponding relatively random sample documents demonstrate values estimates average precision accurately infer relevances unjudged documents combined efficiently accurately infer judged pool relatively judged documents permitting accurate efficient retrieval evaluation scale

noabstract

noabstract

noabstract

noabstract

primary aim xml element retrieval return users xml elements documents poster describes study elicited users expectations i.e anticipated experience interacting xml retrieval system compared traditional flat document retrieval system

poster investigates theoretical benchmarks describe matching functions xml retrieval systems properties specificity exhaustivity xml retrieval theoretical benchmarks concern formal representation qualitative properties ir models situation theory framework meta evaluation xml retrieval

question classification crucial step modern question answering systems previous demonstrated effectiveness statistical machine learning approaches paper approach building question classifier using log linear models evidence rich diverse set syntactic semantic features evaluated approaches exploit hierarchical structure question classes

describe evaluate approach personalizing web search involves post processing results returned underlying search engine re ect community minded searchers.to leverage search experiences community mining title snippet texts results selected community response queries approach seeks build community based snippet index re ects evolving searchers index sed re rank results returned underlying search engine boosting ranking key results freq ently selected similar eries community past

modern retrieval test collections built process called pooling sample entire document set judged topic idea pooling relevant documents unjudged documents assumed nonrelevant resulting judgment set sufficiently complete unbiased document sets grow larger constant size pool represents increasingly percentage document set assumption approximately complete judgments invalid.this paper demonstrates aquaint 2005 test collection exhibits bias caused pools shallow document set size despite diverse runs contribute pools existing judgment set favors relevant documents contain topic title words relevant documents containing topic title words exist document set paper concludes suggested modifications traditional pooling evaluation methodology allow reusable test collections built

propose integration term proximity scoring okapi bm25 relative retrieval effectiveness retrieval method compared pure bm25 varies collection collection.we experimental evaluation method gains achieved bm25 size underlying text collection increases stemmed queries impact term proximity scoring larger unstemmed queries

web search multitasking study based automatic task session detection procedure described results study 1 multitasking rare 2 usually covers 2 task sessions 3 frequently formed temporal inclusion interrupting task session interrupted session 4 quantitative characteristics multitasking greatly characteristics sequential execution tasks searcher minimizes task switching costs avoids multitasking multitasking cheapest manner task switching

vector space model vsm core information retrieval past decades vsm considers documents vectors dimensional space.in vector space techniques latent semantic indexing lsi support vector machines svm naive bayes applied indexing classification dimensionality document space extremely makes techniques infeasible due curse dimensionality paper propose novel tensor space model document analysis represent documents tensors matrices correspondingly novel indexing algorithm called tensor latent semantic indexing tensorlsi developed tensor space theoretical analysis tensorlsi computationally efficient conventional latent semantic indexing makes applicable extremely scale data set experimental results standard document data sets demonstrate efficiency effectiveness algorithm

results scale turkish information retrieval experiments performed trec test collection test bed created study contains 95.5 million words 408,305 documents 72 ad hoc queries size 800mb documents turkish newspaper milliyet implement apply simple sophisticated stemmers various query document matching functions truncating words prefix length 5 creates effective retrieval environment turkish lemmatizer based stemmer provides significantly effectiveness variety matching functions

introduce novel approach combining rankings multiple retrieval systems logistic regression model svm learn ranking pairwise document preferences approach requires training data relevance scores outperforms popular voting algorithm

interactive question answering qa users systems questions provide answers interactive setting user questions depend answers provided system question user follow questions provide feedback system automatically assess performance e.g assess correct answer delivered self awareness qa systems intelligent information seeking example adapting strategies cope problematic situations paper describes initial investigation addressing results indicate interaction context provide useful cues automated performance assessment interactive qa

paper focuses method stylistic segmentation text documents technique involves mapping change feature throughout text linguistic features conjunction modality taxonomies systemic functional linguistics segmentation applications automated summarization particularly documents

web catalog integration current digital content management past studies shown using flattened structure auxiliary information extracted source catalog improve integration results nature flattened structure ignores hierarchical relationships performance improvement catalog integration reduced paper propose enhanced hierarchical catalog integration ehci approach conceptual thesauri extracted source catalog results enhanced hierarchical integration approach effectively boosts accuracy hierarchical catalog integration

rpref generalization bpref evaluation metric assessing quality search engine results graded binary user relevance judgments

paper novel multi webpage summarization algorithm adds graph based ranking algorithm framework maximum marginal relevance mmr method capture main topic web pages eliminate redundancy existing sentences summary result experiment result indicates approach performance previous methods

paper plsi nmf optimize objective function plsi nmf algorithms verified experiments addition propose hybrid method runs plsi nmf alternatively achieve solutions

rank aggregation pervading operation ir technology hypothesize performance score based aggregation affected artificial usually meaningless deviations consistently occurring input score distributions distort combined result individual biases propose score based rank aggregation model source scores normalized common distribution combined experiments available data trec collections shown support proposal

technical professionals spend 25 time searching information specialized information served generic enterprise search tools study investigated software engineers workplace search system identify patterns search behaviour specific distinct web intranet search patterns design recommendations search systems serve

study investigates effectiveness retrieval systems human users generating terms query expansion compare sources terms system generated terms terms users select top ranked sentences user generated terms results demonstrate overall system generated effective expansion terms users users selection terms improved precision top retrieved document list

noabstract

paper employs conceptnet covers rich set commonsense concepts retrieve images text descriptions focusing spatial relationships evaluation test data 2005 imageclef integrating commonsense knowledge information retrieval feasible

noabstract

practical constrains user interfaces user's judgment feedback loop deviate real document read overlooked evaluation relevance feedback.this paper quantitatively analyze impact judging inconsistency performance relevance feedback

extracting morphemes words nontrivial task rule based stemming approaches porter's algorithm encountered success restricted ability identify limited affixes language dependent dealing languages affixes rule based approaches require rules deal word forms deriving rules requires larger effort linguists instances simply impractical propose unsupervised ngram based approach named swordfish using ngram probabilities corpus morphemes identified look methods identifying candidate morphemes using joint probabilities ngrams based log odds prefix probabilities initial results indicate joint probability approach english prefix ratio approach finnish turkish

paper blog corpus demonstrate identify author anonymous text thousands candidate authors approach combines standard information retrieval methods text categorization meta learning scheme determines venture guess

explore interactive methods improve performance pseudo relevance feedback studies citeria suggest methods tackling difficult queries required approach gather information query user simple questions equally simple responses modify original query experiments using trec robust track queries obtain significant improvement mean average precision averaging 5 pseudo relevance feedback improvement spread queries compared ordinary pseudo relevance feedback suggested geometric mean average precision

aim study investigate element retrieval opposed text retrieval meaningful useful searchers carrying information seeking tasks results suggest searchers structural breakdown documents useful browsing retrieved documents provide support usefulness element retrieval interactive settings

research development information access technology scanned paper documents hampered lack public test collections realistic scope complexity project create prototype system search mining masses document images assembling 1.5 terabyte dataset support evaluation complex document information processing cdip tasks e.g text retrieval data mining component technologies optical character recognition ocr document structure analysis signature matching authorship attribution

paper propose strategy time granularity reasoning utilizing temporal information topic tracking compared previous ones distinguished characteristics firstly try determine set topic times target topic topic stories helps avoid negative influence irrelevant times secondly account time granularity variance deciding coreference relationship exists times thirdly publication time times texts considered finally time attribute topic increase similarity story target topic related temporally semantically experiments tdt corpora method makes temporal information news stories

study investigates impact search feature designs dls user search experience results indicate impact significant terms queries issued search steps zero hits pages returned search errors

noabstract

speech retrieval experiments focused news broadcasts adequate automatic speech recognition asr accuracy obtained newspapers news broadcasts manually selected arranged set stories evaluation designs reflected using story boundaries basis evaluation substantial advances asr accuracy build search systems types spontaneous conversational speech evaluation designs continue rely topic boundaries matched nature materials propose class measures speech retrieval based manual annotation user specific topical wish replay begin

emails examples structured documents various fields fields exploited enhance retrieval effectiveness information retrieval ir system mailing list archives recent experiments trec2005 enterprise track various fields applied varying degrees success participants using field based weighting model investigate retrieval performance attainable field examine fields evidence combined

simple improve document retrieval question answering systems method biases retrieval system documents contain words appeared documents containing answers type question method virtually retrieval system exhibits statistically significant performance improvement strong baseline

shot level video browsing method based semantic visual features e.g car mountain fire proposed facilitate content based retrieval video's binary semantic feature vector utilized calculate score similarity shot keyframes score browse similar keyframes terms semantic visual features pilot user study conducted understand users behaviors video retrieval context video retrieval browsing systems compared temporal neighbor semantic visual feature fused browsing system initial results indicated semantic visual feature browsing effective efficient visual centric tasks visual centric tasks

novel language modeling approach capturing query reformulation behavior web search users based framework categorizes eight types user moves adding removing query terms treat search sessions sequence data build gram language models capture user behavior evaluated models prediction task results suggest useful patterns activity extracted user histories furthermore examining prediction performance gram models gained insight amount history context associated types user actions serves basis refined user models

propose method rate credibility news articles using clues 1 commonality contents articles news publishers 2 numerical agreement versus contradiction numerical values reported articles 3 objectivity based subjective speculative phrases news sources tested method news stories seven news sites web average agreement system produced credibility manual judgments human assessors 52 sample articles 69.1 limitations current approach future directions discussed

consider relationship training set size parameter nearest neighbors knn classifier examples available observe accuracy sensitive tends increase training size explore subsequent risk tuned partitions suboptimal aggregation re training risk found severe little data available larger training sizes accuracy increasingly stable respect risk decreases

methods detecting sentences input document set relevant novel respect information direct benefit systems extractive text summarizers satisfactory levels agreement judges performing task manually demonstrated leaving researchers conclude task subjective previous experiments judges identify sentences relevant topic eliminate sentences list contain information currently task proposed annotators perform procedure context specific factual information experiment satisfactory levels agreement independent annotators achieved step identifying sentences containing relevant information relevant results indicate judges agree sentences contain novel information

exponential growth web increasing ability web search engines index data led plenty results returned query typically millions documents common queries benefit added coverage query ranking documents giving results worse difficult temporal ambiguous queries try address using feedback user query logs leverage technology called units generating query refinements shown try queries yahoo search consider refinements sub concepts help define user intent improve search relevance results obtained via live testing yahoo search encouraging

evaluate methods diversifying search results improve personalized web search common personalization approach involves reranking top search results documents preferred user usefulness reranking limited diversity results considered propose methods increase diversity top results evaluate effectiveness methods

xml elements estimated relevant retrieval model desirable retrieval units paper generic model exploits information obtained elements identify relationships relevant elements linking information reinforce relevance elements removing ones experiments using inex testbed effectiveness approach

introduce evaluation metric called measure task retrieving lt ione highly relevant document models user behaviour practical tasks item search stable sensitive reciprocal rank handle graded relevance

performance document clustering systems depends employing optimal text representations difficult determine beforehand vary clustering step towards building robust document clusterers strategy based feature diversity cluster ensembles experiments conducted binary clustering method robust near optimal model selection able detect constructive interactions document representations test bed

hypothesized language modeling retrieval improve reduced document smoothing provide inverse document frequency idf effect created inverse collection frequency icf weighted query models tool partially separate idf role document smoothing compared maximum likelihood estimated mle queries icf weighted queries achieved 6.4 improvement mean average precision description queries icf weighted queries performed document smoothing required mle queries language modeling retrieval benefit means separately incorporate idf behavior outside document smoothing

recently growing topical text classification tasks genre classification sentiment analysis authorship profiling study extent ocr errors affect stylistic text classification scanned documents relatively level errors ocred documents substantially affect stylistic classification accuracy

thanks ubiquity internet search engine search box users depend search engines re information re finding behavior significantly addressed look re finding queries issued yahoo search engine 114 users

report statistically significant mean impacts blind feedback implemented 7 participants 2003 reliable information access ria workshop 30 retrieval measures including primary recall measures originally reported blind feedback detrimental measures focused relevant item boosted precision measures mean precision 10 implying conventional reporting ad hoc precision enhancement

noabstract

structural hints xml retrieval queries specify granularity search result target element document search support elements hints interpreted strictly vaguely matter xml search engine interprets user performance runs submitted inex 2005 content structure cas tasks measured interpretations cas runs perform interpretation target elements regardless interpretation support elements interpret target element matter suggests perform cas queries target structure specification interpreted extend nexi query language include hypothesize using increase overall performance search engines

commonly held user adds structural hints query improve precision element retrieval search inex 2005 conducted experiment test assumption unexpected result structural hints queries improve precision analysis topics judgments suggests users particularly bad giving structural hints

noabstract

poster describe study interface technique provides list suggested additional query terms searcher types search query effect offering interactive query expansion iqe options query formulated analysis results offering iqe query formulation leads quality initial queries increased uptake query expansion findings implications iqe offered retrieval interfaces

label propagation exploits structure unlabeled documents propagating label information training documents unlabeled documents limitation existing label propagation approaches deal single type objects propose framework named relation propagation allows information propagated multiple types objects empirical studies multi label text categorization proposed algorithm effective semi supervised learning algorithms capable exploring correlation categories structure unlabeled documents simultaneously

study similarity measures text centric xml documents based extended vector space model considers document content structure experimental results based benchmark superior performance proposed measure baseline ignores structural knowledge xml documents

discuss information retrieval methods aim serving diverse stream user queries propose methods emphasize importance taking consideration query difference learning effective retrieval functions formulate multi task learning using risk minimization framework particular calibrate empirical risk incorporate query difference terms introducing nuisance parameters statistical models propose alternating optimization method simultaneously learn retrieval function nuisance parameters illustrate effectiveness proposed methods using modeling data extracted commercial search engine

challenging biomedical text retrieval accurate synonyms name variants biomedical entities paper propose concept based approach tackle approach set concepts instead keywords extracted query concepts retrieval purpose experiment results proposed approach boost retrieval performance generates results 2005 trec genomics data sets

noabstract

noabstract

text web consists opinionated evaluative text opposed directly informative text field sentiment analysis seeks characterize aspects natural language text opposed bare suggest appraisal expression extraction viewed fundamental task sentiment analysis define appraisal expression piece text expressing evaluative stance towards particular object task elements characterize type orientation positive negative evaluative stance target possibly source potential applications methods include approaches traditional tasks sentiment classification pinion mining possibly adversarial textual analysis intention detection intelligence applications

extensible java based platform contextual retrieval based probabilistic information retrieval model modules dual indexes relevance feedback blind machine learning approaches query expansion context integrated okapi system deal contextual information platform allows easy extension include types contextual information

personal project planner prototype extension file manager provide people rich text overlays information folders files email web pages notes rich text document project plans created provide context create reference email messages electronic documents web pages complete plan user locate information item email message reference plan e.g alternative context free search inbox sent mail planner explores possibility effective organization project related information emerge natural product efforts plan structure project

noabstract

noabstract

noabstract

noabstract

shot level video retrieval system supports semantic visual features e.g car mountain fire browsing developed facilitate content based retrieval video's binary semantic feature vector utilized calculate score similarity shot keyframes score browse similar keyframes terms semantic visual features

noabstract

strategic management debate 1962 alfred chandler stated structure follows strategy nineteen eighties michael porter modified chandler's dictum structure following strategy introducing level structure organizational structure follows strategy follows structure question leading technology debate seen structure organisation development environment organisation tries survive adapting notion technological advancement change paradigmas organisational strategy development mainly impact technological changes workflow procedures organisations profound days technological change affects levels strategic development examples changes occurring occurred sound vision introduction rfid transmitters admission rings sound vision experience setup office media asset management storage distribution structure public broadcasters third development archive towards becoming media application service provider

noabstract

noabstract

inherent ambiguity short keyword queries demands enhanced methods web retrieval paper propose improve web queries expanding terms collected user's personal information repository implicitly personalizing search output introduce five broad techniques generating additional query keywords analyzing user data increasing granularity levels ranging term compound level analysis global co occurrence statistics using external thesauri extensive empirical analysis scenarios approaches perform especially ambiguous queries producing strong increase quality output rankings subsequently move personalized search framework step propose expansion process adaptive various features query separate set experiments indicates adaptive algorithms bring additional statistically significant improvement static expansion approach

user query element specifies information studies literature found contextual factors strongly influence interpretation query recent studies tried consider user's creating user profile single profile user sufficient variety queries user study propose query specific contexts instead user centric ones including context query context query former specifies environment query domain latter refers context words query particularly useful selection relevant term relations paper types context integrated ir model based language modeling experiments trec collections context factors brings significant improvements retrieval effectiveness

personal information management pim rapidly growing research concerned people store manage refind information feature pim research systems designed assist users manage refind information evaluated noted scholars explained difficulties involved performing pim evaluations difficulties include people re information unique personal collections researchers little tasks cause people re information numerous privacy issues concerning personal information paper aim facilitate pim evaluations addressing difficulties diary study information re finding tasks study examines tasks require users refind information produces taxonomy refinding tasks email messages web pages propose task based evaluation methodology based findings examine feasibility approach using methods task creation

paper examines approach information distillation temporally documents proposes novel evaluation scheme framework combines strengths extends beyond conventional adaptive filtering novelty detection redundant passage ranking respect lasting information tasks multiple queries approach supports fine grained user feedback via highlighting arbitrary spans text leverages information utility optimization adaptive settings experiments defined hypothetical tasks based news events tdt4 corpus multiple queries task answer keys nuggets generated query semi automatic procedure acquiring rules allow automatically matching nuggets system responses propose extension ndcg metric assessing utility ranked passages combination relevance novelty results encouraging utility enhancements using approach compared baseline systems incremental learning novelty detection components

memory based collaborative filtering algorithms widely adopted popular recommender systems approaches suffer data sparsity poor prediction quality usually user item matrix sparse directly leads inaccurate recommendations paper focuses memory based collaborative filtering crucial factors 1 similarity computation users items 2 missing data prediction algorithms enhanced pearson correlation coefficient pcc algorithm adding parameter overcomes potential decrease accuracy computing similarity users items propose effective missing data prediction algorithm information users items account algorithm set similarity threshold users items respectively prediction algorithm determine predicting missing data address predict missing data employing combination user item information finally empirical studies dataset movielens shown newly proposed method outperforms art collaborative filtering algorithms robust data sparsity

content based personalized recommendation system learns user specific profiles user feedback deliver information tailored individual user's system serving millions users learn user profile user user little feedback borrowing information users bayesian hierarchical model learning model parameters optimize joint data likelihood millions users computationally expensive commonly em algorithm converges slowly due sparseness data ir applications paper proposes fast learning technique learn individual user profiles efficacy efficiency proposed algorithm justified theory demonstrated actual user data netflix movielens

low cost methods acquiring relevance judgments boon researchers evaluate retrieval tasks topics resources thousands judgments judgments useful time evaluation trusted re evaluate systems formally define means judgments reusable confidence evaluation systems accurately assessed existing set relevance judgments method augmenting set relevance judgments relevance estimates require additional assessor effort using method practically guarantees reusability five judgments topic systems reliably evaluate larger set ten systems sets judgments useful evaluation systems

information retrieval evaluation based pooling method inherently biased systems contribute pool judged documents distort results obtained relative quality systems evaluated lead incorrect conclusions performance particular ranking technique examine magnitude effect explore countered automatically building unbiased set judgements original biased judgements obtained pooling compare performance method approaches incomplete judgements bpref proposed method leads evaluation accuracy especially set manual judgements rich documents highly biased systems

recently trec tracks adopted retrieval effectiveness metric called bpref designed evaluation environments incomplete relevance data graded relevance version metric called rpref proposed application measure normalised discounted cumulative gain ndcg average precision avep condensed lists obtained ltering unjudged documents original ranked lists actually solution incompleteness bpref furthermore graded relevance boosts robustness ir evaluation incompleteness measure ndcg based condensed lists choices graded relevance test collections ntcir compare ten ir metrics terms system ranking stability pairwise discriminative power

standard machine learning techniques typically require ample training data form labeled instances situations tedious costly obtain sufficient labeled data adequate classifier performance text classification humans easily guess relevance features words indicative topic thereby enabling classifier focus feature weights appropriately absence sufficient labeled data describe algorithm tandem learning begins couple labeled instances iteration recommends features instances human label tandem learning using oracle results performance learning features instances humans emulate oracle extent results performance accuracy comparable oracle unique experimental design helps factor system error human error leading understanding interactive feature selection

effective organization search results critical improving utility search engine clustering search results effective organize search results allows user navigate relevant documents quickly deficiencies approach 1 clusters discovered necessarily correspond aspects topic user's perspective 2 cluster labels generated informative allow user identify cluster paper propose address deficiencies 1 learning aspects topic web search logs organizing search results accordingly 2 generating meaningful cluster labels using past query words entered users evaluate proposed method commercial search engine log data compared traditional methods clustering search results method result organization meaningful labels

recent document clustering receiving attentions fundamental technique unsupervised document organization automatictopic extraction fast information retrieval filtering paper propose novel method clustering documents using regularization unlike traditional globally regularized clustering methods method construct local regularized linear label predictor document vector combine local regularizers global smoothness regularizer call algorithm clustering local global regularization clgr cluster memberships documents achieved eigenvalue decomposition sparse symmetric matrix efficiently solved iterative methods finally experimental evaluations datasets superiorities clgr traditional document clustering methods

describe approach extracting semantics tags unstructured text labels assigned resources web based tag's usage patterns particular focus extracting event semantics tags assigned photos flickr popular photo sharing website supports time location latitude longitude metadata analyze methods inspired burst analysis techniques novel method scale structure identification evaluate methods subset flickr data scale structure identification method outperforms existing techniques approach methods described domains geo annotated web pages text terms extracted associated usage patterns

paper hierarchical classification framework proposed bridging semantic gap effectively achieving multi level image annotation automatically semantic gap low level computable visual features users real information partitioned gaps multiple approachesallare proposed bridge gaps effectively learn reliable contextual relationships atomic image concepts co appearances salient objects multi modal boosting algorithm proposed enable hierarchical image classification avoid inter level error transmission hierarchical boosting algorithm proposed incorporating concept ontology multi task learning achieve hierarchical image classifier training automatic error recovery bridge gap computable image concepts users real information novel hyperbolic visualization framework seamlessly incorporated enable intuitive query specification evaluation acquainting users global view scale image collections experiments scale image databases obtained positive results

relevance feedback powerful technique enhance content based image retrieval cbir performance solicits user's relevance judgments retrieved images returned cbir systems user's labeling learn classifier distinguish relevant irrelevant images top returnedimages informative ones challenge determine unlabeled images informative i.e improve classifier labeled training samples paper propose novel active learning algorithm called laplacian optimal design lod relevance feedback image retrieval algorithm based aregression model minimizes square error measured labeled images simultaneously preserves local geometrical structure image space specifically assume images sufficiently close measurements labels close constructing nearest neighbor graph geometrical structure image space described graph laplacian discuss results field optimal experimental design guide selection subset images amount information experimental results corel database suggest theproposed approach achieves precision relevance feedback image retrieval

presentation query biased document snippets results pages search engines expectation search engine users paper explore algorithms data structures required search engine allow efficient generation query biased snippets begin proposing analysing document compression method reduces snippet generation time 58 baseline using zlib compression library experiments reveal finding documents secondary storage dominates total cost generating snippets caching documents ram essential fast snippet generation process using simulation examine snippet generation performance size ram caches finally propose analyse document reordering compaction revealing scheme increases document cache hits marginal affect snippet quality scheme effectively doubles documents fit fixed size cache

web search engines lists captions comprising title snippet url help users decide search results visit understanding influence features captions web search behavior help validate algorithms guidelines improved generation paper develop methodology clickthrough logs commercial search engine study user behavior interacting search result captions findings study suggest relatively simple caption features presence terms query terms readability snippet length url shown caption significantly influence users web search behavior

existing methods conduct summarization tasks single documents separately interactions document assumption documents considered independent paper proposes novel framework called collabsum collaborative single document summarizations mutual influences multiple documents cluster context study collabsum implemented employing clustering algorithm obtain appropriate document clusters exploiting graph ranking based algorithm collaborative document summarizations cluster document cross document relationships sentences incorporated algorithm experiments duc2001 duc2002 datasets demonstrate encouraging performance proposed approach clustering algorithms investigated summarization performance relies positively quality document cluster

people repeat web searches information topics previously explored re information seen past query associated repeat search initial query nonetheless lead clicks results paper explores repeat search behavior analysis web query log 114 anonymous users separate controlled survey additional 119 volunteers study demonstrates 40 queries re finding queries re finding appears behavior search engines explicitly support explore demonstrate changes search engine results hinder re finding provide automatically detect repeat searches predict repeat clicks

novel web search interaction feature query provides links websites frequently visited users similar information popular destinations complement traditional search results allowing direct navigation authoritative resources query topic destinations identified using history search browsing behavior users extended time period collective behavior provides basis computing source authority describe user study compared suggestion destinations previously proposed suggestion related queries traditional unaided web search results search enhanced destination suggestions outperforms systems exploratory tasks performance obtained mining past user behavior query level granularity

geographic information retrieval gir systems allow users specify geographic context addition traditional query enabling system pinpoint search results relevancy location dependent particular local search services widely mechanism businesses hotels restaurants shops satisfy geographical restriction unfortunately useful types geographic restrictions currently supported systems including restrictions specify neighborhood business located boundaries city neighborhoods readily available automated techniques construct representations spatial extent neighborhoods required support restrictions paper propose technique using fuzzy footprints cope inherent vagueness neighborhood boundaries provide experimental results demonstrate potential technique local search setting

disk access performance major bottleneck traditional information retrieval systems compared system memory disk bandwidth poor seek times worse circumvent considering query evaluation strategies main memory accumulator trimming techniques combined inverted list skipping produce extremely performance retrieval systems resorting methods harm effectiveness evaluate techniques using galago retrieval system designed efficient query processing system achieves 69 improvement query throughput previous methods

paper study trade offs designing efficient caching systems web search engines explore impact approaches static vs dynamic caching caching query results vs.caching posting lists using query log spanning explore limitations caching demonstrate caching posting lists achieve hit rates caching query answers propose algorithm static caching posting lists outperforms previous methods study finding optimal split static cache answers posting lists finally measure changes query log affect effectiveness static caching observation distribution queries changes slowly time results observations applicable levels data access hierarchy instance memory disk layer broker remote server layer

web search engines maintain scale inverted indexes queried thousands times users eager information cope vast amounts query loads search engines prune index documents returned top results pruned index compute batches results approach improve performance reducing size index compute top results pruned index notice significant degradation result quality document top results included pruned index placed results computed pruned index fierce competition online search market phenomenon undesirable paper study avoid degradation result quality due pruning based performance optimization realizing benefit contribution modifications pruning techniques creating pruned index result computation algorithm guarantees top matching pages placed top search results computing batch pruned index time determine optimal size pruned index experimentally evaluate algorithms collection 130 million web pages

topic detection tracking topic segmentation play role capturing local sequential information documents previous usually focuses single documents similar multiple documents available domains paper introduce novel unsupervised method shared topic detection topic segmentation multiple similar documents based mutual information mi weighted mutual information wmi combination mi term weights basic idea optimal segmentation maximizes mi wmi approach detect shared topics documents optimal boundaries document align segments documents time handle single document segmentation special multi document segmentation alignment methods identify strengthen cue terms segmentation partially remove stop words using term weights based entropy learned multiple documents experimental results algorithm tasks single document segmentation shared topic detection multi document segmentation utilizing information multiple documents tremendously improve performance topic segmentation using wmi using mi multi document segmentation

consider analyzing word trajectories time frequency domains specific goal identifying reported periodic aperiodic words set words identical trends reconstruct event completely un supervised manner document frequency word time treated time series element document frequency inverse document frequency dfidf score time paper 1 applied spectral analysis categorize features event characteristics reported periodic aperiodic 2 modeled aperiodic features gaussian density periodic features gaussian mixture densities subsequently detected feature's burst truncated gaussian approach 3 proposed unsupervised greedy event detection algorithm detect aperiodic periodic events methods applied time series data extensively evaluated methods 1 reuters news corpus 3 able uncover meaningful aperiodic periodic events

event detection ned aims detecting multiple streams news stories reported event i.e reported previously overwhelming volume news available increasing ned system able detect events efficiently accurately paper propose ned model speed ned task using news indexing tree dynamically moreover based observation terms types effects ned task term reweighting approaches proposed improve ned accuracy approach propose adjust term weights dynamically based previous story clusters approach propose employ statistics training data learn named entity reweighting model class stories experimental results linguistic data consortium ldc datasets tdt2 tdt3 proposed model improve efficiency accuracy ned task significantly compared baseline system existing systems

consider duplicate document detection search evaluation query web results query detect duplicate web documents precision 0.91 recall 77 contrast charikar's algorithm designed duplicate detection indexing pipeline achieves precision 0.91 recall 0.58 improvement recall maintaining precision comes combining ideas concerned duplicate detection results query pairwise comparisons afford compute multiple pairwise signals pair documents model learned standard machine learning techniques improves recall 0.68 precision 0.90 duplicate detection focused text analysis html contents document web pages html indicator final contents page extended fetching techniques fill frames execute java script including signals based richer fetches improves recall 0.75 precision 0.91 finally explore using signals based query comparing contextual snippets based richer fetches improves recall 0.77 overall accuracy final model approaches human judges

propose methodology building practical robust query classification system identify thousands query classes reasonable accuracy dealing real time query volume commercial web search engine blind feedback technique query determine topic classifying web search results retrieved query motivated search advertising primarily focus rare queries hardest view machine learning aggregation account considerable fraction search engine traffic empirical evaluation confirms methodology yields considerably classification accuracy previously reported believe proposed methodology lead matching online ads rare queries overall user experience

search engines record documents clicked query query document pairs soft relevance judgments compared true judgments click logs noisy sparse relevance information apply markov random walk model click log producing probabilistic ranking documents query key advantage model ability retrieve relevant documents clicked query rank effectively conduct experiments click logs image search comparing backward random walk model forward random walk varying parameters walk length self transition probability effective combination backward walk self transition probability

paper reports experiment comparing retrieval effectiveness interactive information retrieval iir system adapts support information seeking strategies standard baseline iir system experiment 32 subjects searching 8 topics indicates using integrated iir system resulted significantly performance including user satisfaction search results significantly effective interaction significantly usability using baseline system

help users commercial web search engines successful searches understand users search expertise doing knowledge benefit paper study interaction logs advanced search engine users advanced understand user search results marked differences queries result clicks post query browsing search success users classify advanced based query operators relative classified advanced findings implications advanced users supported searches interactions help searchers experience levels relevant information learn improved searching strategies

paper study term based feedback information retrieval language modeling approach term feedback user directly judges relevance individual terms interaction feedback documents taking control query expansion process propose cluster based method selecting terms user judgment effective algorithms constructing refined query language models user term feedback algorithms shown bring significant improvement retrieval accuracy feedback baseline achieve comparable performance relevance feedback helpful relevant documents top

machine learning commonly improve ranked retrieval systems due computational difficulties learning techniques developed directly optimize mean average precision map despite widespread evaluating systems existing approaches optimizing map globally optimal solution computationally expensive contrast svm learning algorithm efficiently globally optimal solution straightforward relaxation map evaluate approach using trec 9 trec 10 web track corpora wt10g comparing svms optimized accuracy rocarea method produce statistically significant improvements map scores

central applications information retrieval ranking learning rank considered promising approach addressing issue ranking svm example art method learning rank empirically demonstrated effective paper study issue learning rank particularly approach using svm techniques perform task ranking svm advantageous shortcomings ranking svm employs single hyperplane feature space model ranking simple tackle complex ranking furthermore training ranking svm computationally costly paper look alternative approach ranking svm call multiple hyperplane ranker mhr comparisons approaches mhr takes divide conquer strategy employs multiple hyperplanes rank instances finally aggregates ranking results hyperplanes mhr contains ranking svm special mhr overcome shortcomings ranking svm suffers experimental results information retrieval datasets mhr outperform ranking svm ranking

effective ranking functions essential commercial search engines focus developing regression framework learning ranking functions improving relevance search engines serving diverse streams user queries explore supervised learning methodology machine learning distinguish types relevance judgments training data 1 absolute relevance judgments arising explicit labeling search results 2 relative relevance judgments extracted user click throughs search results converted absolute relevance judgments propose novel optimization framework emphasizing relative relevance judgments main contribution development algorithm based regression applied objective functions involving preference data i.e data indicating document relevant respect query experimental results carried using data sets obtained commercial search engine results significant improvements proposed methods existing methods

existing retrieval models documents scored primarily based various kinds term statistics document frequencies inverse document frequencies document lengths intuitively proximity matched query terms document exploited promote scores documents matched query terms close proximity heuristic explored literature unclear model proximity incorporate proximity measure existing retrieval model paper systematically explore query term proximity heuristic specifically propose study effectiveness five proximity measures modeling proximity perspective design heuristic constraints guide incorporating proposed proximity measures existing retrieval model experiments five standard trec test collections proposed proximity measures indeed highly correlated document relevance incorporating kl divergence language model okapi bm25 model significantly improve retrieval performance

existing pseudo relevance feedback methods typically perform averaging top retrieved documents ignore statistical dimension risk variance associated individual document models combination treating baseline feedback method black box output feedback model random variable estimate posterior distribution feed model resampling query's top retrieved documents using posterior mean mode enhanced feedback model perform model combination enhanced models based slightly modified query sampled original query resampling documents helps increase individual feedback model precision removing noise terms sampling query improves robustness worst performance emphasizing terms related multiple query aspects result meta feedback algorithm robust precise original strong baseline method

query expansion form pseudo relevance feedback relevance feedback common technique improve retrieval effectiveness previous approaches ignored issues role features importance modeling term dependencies paper propose robust query expansion technique based onthe markov random field model information retrieval technique called latent concept expansion provides mechanism modeling term dependencies expansion furthermore arbitrary features model provides powerful framework beyond simple term occurrence features implicitly expansion techniques evaluate technique relevance models art language modeling query expansion technique model demonstrates consistent significant improvements retrieval effectiveness trec data sets describe technique generate meaningful multi term concepts tasks query suggestion reformulation

variants language models proposed information retrieval existing models based multinomial distribution score documents based query likelihood computed based query generation probabilistic model paper propose study family query generation models based poisson distribution simplest forms family models existing multinomial models equivalent based smoothing methods families models behave poisson model advantages including naturally accommodating term smoothing modeling accurate background efficiently variants model corresponding smoothing methods evaluate representative trec test collections results basic models perform comparably poisson model perform multinomial model term smoothing performance improved stage smoothing

methodology based information nuggets recently emerged de facto standard answers complex questions evaluated implementations trec question answering tracks community gained understanding characteristics paper focuses particular aspect evaluation human assignment nuggets answer strings serves basis score computation byproduct trec 2006 ciqa task identical answer strings independently evaluated twice allowed assess consistency human judgments based results explored simulations assessor behavior provide method quantify scoring variations understanding variations researchers confident comparisons systems

current approaches identifying definitional sentences context question answering mainly involve linguistic syntactic patterns identify informative nuggets insufficient address novelty factor definitional nugget possess paper proposes address deficiency building human model external knowledge hoped model allow computation human sentence respect topic compare contrast model current definitional question answering models interestingness plays factor definitional question answering

graphical models applied various information retrieval natural language processing tasks recent literature paper apply probabilistic graphical model answer ranking question answering model estimates joint probability correctness answer candidates probability correctness individual candidate inferred joint prediction model estimate correctness individual answers correlations enables list accurate comprehensive answers model compared logistic regression model directly estimates probability correctness individual answer candidate extensive set empirical results based trec questions demonstrates effectiveness joint model answer ranking furthermore combine joint model logistic regression model improve efficiency accuracy answer ranking

bag words retrieval popular question answering qa system developers support constraint checking ranking linguistic semantic information qa system anapproach retrieval qa applying structured retrieval techniques types text annotations qa systems demonstrate structured approach retrieve relevant results highly ranked compared bag words sentence retrieval task characterize extent structured retrieval effectiveness depends quality annotations

investigate robustness widely ir relevance measures data collections incomplete judgments relevance measures consider bpref measure introduced buckley voorhees 7 inferred average precision infap introduced aslam yilmaz 4 normalized discounted cumulative gain ndcg measure introduced 228 rvelin kek 228 228 inen 8 main results ndcg consistently performs bpref infap experiments performed standard trec datasets levels incompleteness judgments using evaluation methods namely kendall correlation measures system rankings pairwise statistical significance testing latter independent

ir test collection series papers recent addressed question empirically enumerating consistency performance comparisons using alternate subsets collection paper propose using test theory based analysis variance specifically designed assess test collections using method measure test reliability estimate test collection's reliability built determine optimal allocation resources e.g invest judges queries method widespread field educational testing complements data driven approaches assessing test collections whereas data driven method focuses test results test theory focuses test designs offers unique practical results insights variety implications alternative test designs

relevance judgments compare text retrieval systems collection documents queries set systems compared standard approach forming judgments manually examine documents highly ranked systems relevance judgments provide benefit final result particularly aim identify systems paper propose experimental methodologies significantly reduce volume judgments required system comparisons using rank biased precision recently proposed effectiveness measure judging 200 documents 50 queries trec scale system evaluation containing 100 runs sufficient identify systems

ranking becoming fields especially information retrieval ir machine learning techniques proposed ranking ranksvm rankboost ranknet ranknet based probabilistic ranking framework leading promising results applied commercial web search engine paper conduct study probabilistic ranking framework provide novel loss function named fidelity loss measuring loss ranking fidelity loss notonly inherits effective properties probabilistic ranking framework ranknet possesses properties helpful ranking includes fidelity loss obtaining zero document pair finite upper bound conducting query level normalization propose algorithm named frank based generalized additive model sake minimizing fedelity loss learning effective ranking function evaluated proposed algorithm datasets trec dataset real web search dataset experimental results proposed frank algorithm outperforms learning based ranking methods conventional ir web search

paper address issue learning rank document retrieval task model automatically created training data utilized ranking documents goodness model usually evaluated performance measures map mean average precision ndcg normalized discounted cumulative gain ideally learning algorithm train ranking model directly optimize performance measures respect training data existing methods able train ranking models minimizing loss functions loosely related performance measures example ranking svm rankboost train ranking models minimizing classification errors instance pairs deal propose novel learning algorithm framework boosting minimize loss function directly defined performance measures algorithm referred adarank repeatedly constructs weak rankers basis reweighted training data finally linearly combines weak rankers ranking predictions prove training process adarank exactly enhancing performance measure experimental results benchmark datasets adarank significantly outperforms baseline methods bm25 ranking svm rankboost

paper propose method discover collection adapted ranking functions based genetic programming gp combined component approach cca based combination term weighting components i.e term frequency collection frequency normalization extracted ranking functions contrast related gp terminals cca based simple statistical information document collection meaningful effective proven components experimental results approach able outper form standard tf idf bm25 gp based approach collections cca obtained improvements mean average precision 40.87 trec 8 collection 24.85 wbr99 collection brazilian web collection baseline functions cca evolution process able reduce overtraining commonly found machine learning methods especially genetic programming converge faster gp based approach comparison

ranking topic information retrieval algorithms learning ranking models intensively studied feature selection despite importance reality feature selection methods classification directly applied ranking argue striking differences ranking classification develop feature selection methods ranking propose feature selection method paper specifically feature value rank training instances define ranking accuracy terms performance measure loss function importance feature define correlation ranking results features similarity based definitions formulate feature selection issue optimization features maximum total importance scores minimum total similarity scores demonstrate solve optimization efficient tested effectiveness feature selection method information retrieval datasets ranking models experimental results method outperform traditional feature selection methods ranking task

spam key electronic communication including scale email systems growing blogs content based filtering reliable method combating threat various forms academic researchers industrial practitioners disagree filter spam former advocated support vector machines svms content based filtering machine learning methodology art performance text classification similar performance gains demonstrated online spam filtering additionally practitioners cite cost svms reason prefer faster statistically robust bayesian methods paper offer resolution controversy online svms indeed art classification performance online spam filtering benchmark data sets nearly equivalent performance achieved relaxed online svm rosvm greatly reduced computational cost results experimentally verified email spam blog spam splog detection tasks

web spam significantly deteriorate quality search engine results incentive commercial search engines detect spam pages efficiently accurately paper spam detection system combines link based content based features topology web graph exploiting link dependencies web pages linked hosts tend belong class spam spam demonstrate methods incorporating web graph topology predictions obtained base classifier clustering host graph assigning label hosts cluster majority vote ii propagating predicted labels neighboring hosts iii using predicted labels neighboring hosts features retraining classifier result accurate system detecting web spam tested public dataset using algorithms applied practice scale web data

pagerank algorithm proven effective ranking web pages rank scores web pages manipulated handle manipulation cast insight web structure propose ranking algorithm called diffusionrank diffusionrank motivated heat diffusion phenomena connected web ranking activities flow web imagined heat flow link page treated pipe air conditioner heat flow embody structure underlying web graph theoretically diffusionrank serve generalization pagerank heat diffusion co efficient 947 tends infinity 1 947 0 diffusionrank pagerank low ability anti manipulation 947 0 diffusionrank obtains ability anti manipulation web structure completely ignored consequently 947 factor control balance ability preserving original web ability reducing effect manipulation found empirically 947 1 diffusionrank penicillin effect link manipulation moreover diffusionrank employed relations web divide web graph link communities experimental results diffusionrank algorithm achieves mentioned advantages expected

query semantic description qbsd natural paradigm retrieving content databases music major impediment development qbsd systems music information retrieval lack cleanly labeled publicly available heterogeneous data set songs associated annotations collected computer audition lab 500 song cal500 data set humans listen annotate songs using survey designed capture semantic associations music words adapt supervised multi class labeling sml model shown performance task image retrieval cal500 data learn model music retrieval model parameters estimated using weighted mixture hierarchies expectation maximization algorithm specifically designed handle real valued semantic association words songs binary class labels output sml model vector class conditional probabilities interpreted semantic multinomial distribution vocabulary representing semantic query query multinomial distribution quickly rank songs database based kullback leibler divergence query multinomial song's semantic multinomial qualitative quantitative results demonstrate sml model annotate novel song meaningful words retrieve relevant songs multi word text based query

approach automatically build search engine scale music collections queried natural language existing approaches depend explicit manual annotations meta data assigned individual audio pieces automatically derive descriptions methods web retrieval music information retrieval based id3 tags collection mp3 files retrieve relevant web pages via google queries contents pages characterize music pieces represent term vectors incorporating complementary information acous tic similarity able reduce dimensionality vector space improve performance retrieval i.e quality results furthermore usage audio similarity allows characterize audio pieces associated information found web

increased simulated queries evaluation estimation purposes information retrieval unaddressed issues regarding usage impact evaluation quality terms retrieval performance unlike real queries paper wefocus methods building simulated item topics explore quality real item topics using existing generation models starting explore factors influence generation item topic informed detailed analysis six european languages propose model improved document term selection properties simulated item topics generated comparable real item topics significant step towards validating potential usefulness simulated queries evaluation purposes becausebuilding models querying behavior provides deeper insight querying process retrieval mechanisms developed support user

query suggestion aims suggest relevant queries query help users specify information previously suggested terms language input query paper extend cross lingual query suggestion clqs query language suggest similar relevant queries languages scenarios cross language information retrieval clir cross lingual keyword bidding search engine advertisement instead relying existing query translation technologies clqs effective means map input query language queries language query log monolingual cross lingual information word translation relations word co occurrence statistics estimate cross lingual query similarity discriminative model benchmarks resulting clqs system significantly performs baseline system based dictionary based query translation besides resulting clqs tested french english clir tasks trec collections results demonstrate effectiveness traditional query translation methods

noabstract

propose novel method analysing data gathered fromtrec similar information retrieval evaluation experiments define normalized versions average precision construct weighted bipartite graph trec systems topics analyze meaning somewhat generalized indicators fromsocial network analysis systems topics graph apply method analysis trec 8 data amongthe results authority measures systems performance hubness topics reveals topics distinguishing effective systems current measures system effective trec effective easy topics using effectiveness measures

world wide web contains rich textual contents areinterconnected via complex hyperlinks huge database violates assumption held conventional statistical methods web page considered independent identical sample difficult apply traditional mining learning methods solving web mining e.g web page classification exploiting content link structure research direction recently received considerable attention stage methods exploit link structure content information combine authority information content information decompose link structure hub authority features apply additional document features practically attractive simplicity paper aims design algorithm exploits content linkage information carrying joint factorization linkage adjacency matrix document term matrix derives representation web pages low dimensional factor space explicitly separating content hub authority factors analysis performed based compact representation web pages experiments proposed method compared art methods demonstrates excellent accuracy hypertext classification webkb cora benchmarks

federated text retrieval systems query sent multiple collections time results returned collections gathered ranked central broker user usually assumed collections little overlap practice collections share common documents exact near duplicates potentially leading duplicates final results considering natural band width restrictions efficiency issues federated search sendingqueries redundant collections leads unnecessary costs propose novel method estimating rate lap collections based sampling using theestimated overlap statistics propose collection selection methods aim maximize unique relevant documents final results experimentally estimates overlap exact suggested techniques significantly improve search effectiveness collections overlap

server selection methods suitable distributed information retrieval applications rely absence cooperation availability unbiased samples documents constituent collections describe sampling methods depend normal query response mechanism applicable search facilities evaluate methods collections typical personal metasearch application results demonstrate biases exist methods particularly documents biases reduced eliminated choice parameters.we introduce sampling technique multiple queries produces samples similar quality current techniques significantly reduced cost

facilitate search relevant information setof online distributed collections federated information retrieval system typically represents collection centrally set vocabularies sampled documents accurate retrieval related precise representation reflects underlying content stored collection collections evolve time collection representations updated reflect change current solution proposed study examine implications date representation sets retrieval accuracy proposing policies managing updates policyis evaluated testbed forty dynamic collections eight week period findings date representations significantly degrade performance overtime adopting suitable update policy minimise

text search temporally versioned document collections web archives received little attention research consequence scalable principled solution search collection specified time address shortcoming propose efficient solution time travel text search extending inverted file index ready temporal search introduce approximate temporal coalescing tunable method reduce index size significantly affecting quality results improve performance time travel queries introduce principled techniques trade index size performance techniques formulated optimization solved near optimality finally approach evaluated comprehensive series experiments scale real world datasets results unequivocally methods build efficient time machine scalable versioned text collections

hash based similarity search reduces continuous similarity relation binary concept similar similar feature vectors considered similar mapped hash key runtime performance principle unequaled unaffected dimensionality concerns time similarity hashing applied success near similarity search document collections considered key technology near duplicate detection plagiarism analysis papers reveals design principles hash based search methods unified introduce stress statistics suited analyze performance hash based search methods explain rationale effectiveness based insights optimum hash functions similarity search derived results comparative study hash based search methods

recently manning et al 2007 resorted permuterm indexof garfield 1976 time efficient elegant solution string dictionary pattern queries possibly include wild card symbol called tolerant retrieval unfortunately permuterm index space inefficient quadruples dictionary size paper propose compressed permuterm index solves tolerant retrieval optimal query time i.e time proportional length searched pattern space close th empirical entropy indexed dictionary index solve sophisticated queries involve wild card symbols require prefix match multiple fields database records.the result based elegant variant burrows wheeler transform defined dictionary strings variable length allows easily adapt compressed indexes makinen navarro 2007 solve tolerant retrieval experiments index supports fast queries space occupancy close achievable compressing string dictionary via gzip bzip ppmdi improves approaches based front coding 50 absolute space occupancy guaranteeing comparable query time

current prediction techniques designed content based queries typically evaluated relatively homogenous test collections sizes serious challenges web search environments collections significantly heterogeneous types retrieval tasks exist paper techniques address challenges focus performance prediction types queries web search environments content based named page finding evaluation mainly performed gov2 collection addition evaluating models types queries separately consider challenging realistic situation types queries mixed prior information query types assist prediction mixed query situation novel query classifier adopted results prediction web query performance substantially accurate current art prediction techniques consequently paper provides practical approach performance prediction real world web settings

expertise retrieval unexplored data w3c collection time intranets universities knowledge intensive organisations offer examples relatively clean multilingual expertise data covering broad ranges expertise main expertise retrieval tasks set baseline approaches based generative language modeling aimed finding expertise relations topics people experimental evaluation introduce release test set based crawl university site using test set conduct series experiments aimed determining effectiveness baseline expertise retrieval methods applied test set aimed assessing refined models exploit characteristic features test set organizational structure university hierarchical structure topics test set expertise retrieval models shown robust respect environments w3c collection current techniques appear generalizable settings

contextual advertising context match cm refers placement commercial textual advertisements content generic web page sponsored search ss advertising consists placing ads result pages web search engine ads driven originating query cm usually intermediary commercial ad network entity charge optimizing ad selection twin goal increasing revenue shared publisher ad network improving user experience goals mind preferable ads relevant page content generic ads ss market developed quicker cm market textual ads characterized bid phrases representing queries advertisers ad displayed hence technologies cm relied previous solutions ss simply extracting phrases page content displaying ads corresponding searches phrases purely syntactic approach due vagaries phrase extraction lack context approach leads irrelevant ads overcome propose system contextual ad matching based combination semantic syntactic features

query relevance measures provide standardized repeatable measurements search result quality ignore users actually experience search session paper examines approximate user's ultimate session level satisfaction using simple relevance metric thisrelationship surprisingly strong incorporating additional properties query construct model predicts user satisfaction accurately relevance

effectiveness information retrieval ir systems influenced degree term overlap user queries relevant documents query document term mismatch partial total dealt ir systems query expansion qe method dealing term mismatch ir systems implementing query expansion typically evaluated executing query twice query expansion comparing result sets measures overall change performance directly measure effectiveness ir systems overcoming inherent issue term mismatch query relevant documents nor provide insight systems behave presence query document term mismatch paper propose approach evaluating query expansion techniques proposed approach attractive provides estimate system performance varying degrees query document term mismatch makes readily available test collections require additional relevance judgments form manual processing

evaluation information retrieval systems core tasks information retrieval include inability exhaustively label documents topic generalizability topics incorporating variability retrieval systems previous addresses evaluation systems ranking queries difficulty ranking individual retrievals performance approaches exist relevance judgments focus zero judgment performance prediction individual retrievals common shortcoming previous techniques assumption uncorrelated document scores judgments documents embedded dimensional space apply techniques spatial data analysis detect correlations document scores low correlation scores topically close documents implies poor retrieval performance compared art baseline demonstrate spatial analysis retrieval scores provides significantly prediction performance predictors incorporated classic predictors improve performance describe scale experiment evaluate zero judgment performance prediction massive retrieval systems variety collections languages

research information retrieval usually performanceimprovement sources evidence combined produce ranking documents e.g texts pictures sounds paper focus rank aggregation called data fusion rankings documents searched collection provided multiple methods combined produce ranking context propose rank aggregation method multiple criteria framework using aggregation mechanisms based decision rules identifying positive negative reasons judging document rank proposed method deals information retrieval distinctive features experimental results reported suggested method performs combsum combmnz operators

introduce relevance scoring technique enhances existing relevance scoring schemes term position information technique chronological term rank ctr captures positions terms occur sequence words document ctr conceptually computationally simple compared approaches document structure information term proximity term document features ctr paired okapi bm25 evaluate performance various combinations ctr okapi bm25 identify effective formula compare performance selected approach performance existing methods okapi bm25 pivoted length normalization language models significant improvements seen consistently variety trec data topic sets measured major retrieval performance metrics statistic relevance scoring retrieval improvements using chronological term rank enhanced methods future

due popularity weblogs blogs short wealth information helpful assessing public's sentiments opinions paper study mining sentiment information blogs investigate information predicting product sales performance based analysis complex nature sentiments propose sentiment plsa plsa blog entry viewed document generated hidden sentiment factors training plsa model blog data enables obtain succinct summary sentiment information embedded blogs arsa autoregressive sentiment aware model utilize sentiment information captured plsa predicting product sales performance extensive experiments conducted movie data set compare arsa alternative models account sentiment information model feature selection method experiments confirm effectiveness superiority proposed approach

retrieving information speech data broadcast news telephone conversations roundtable meetings systems vocabulary continuous speech recognition tools produce word transcripts transcripts indexed query terms retrieved index query terms recognizer's vocabulary retrieved recall search affected addition output word transcript advanced systems provide phonetic transcripts query terms matched phonetically phonetic transcripts suffer lower accuracy alternative word transcripts.we vocabulary independent system handle arbitrary queries exploiting information provided word transcripts phonetic transcripts speech recognizer generates word confusion networks phonetic lattices transcripts indexed query processing ranking purpose.the value proposed method demonstrated relative performance ofour system received overall ranking english speech data recent nist spoken term detection evaluation

paper describes techniques increasing accuracy oftopic label assignment conversational speech oral history interviews using supervised machine learning conjunction automatic speech recognition time shifted classification leverages local sequence information story told temporal label weighting takes complementary perspective using position interview bias label assignment probabilities methods combination yield 6 15 relative improvements classification accuracy using clipped precision measure models utility label sets segment summaries interactive speech retrieval applications

paper investigate methods improving performance morph based spoken document retrieval finnish extracting relevant index terms confusion networks approach morpheme subword units morphs recognition indexing alleviates vocabulary words especially inflectional languages finnish confusion networks offer convenient representation alternative recognition candidates aligning mutually exclusive terms giving posterior probability term rank competing terms posterior probability estimate term frequency indexing comparing 1 recognizer transcripts retrieval effectiveness significantly improved finally effect pruning recognition analyzed recognition speed increased reduction retrieval performance due increase 1 error rate compensated using confusion networks

traditionally stemming applied information retrieval tasks transforming words documents root form indexing applying similar transformation query terms increases recall naive strategy web search lowers precision requires significant amount additional computation paper propose context sensitive stemming method addresses issues unique properties approach feasible web search based statistical language modeling perform context sensitive analysis query accurately predict morphological variants useful expand query term submitting query search engine dramatically reduces bad expansions reduces cost additional computation improves precision time approach performs context sensitive document matching expanded variants conservative strategy serves safeguard spurious stemming improving precision using word pluralization handling example stemming approach experiments major web search engine stemming 29 query traffic improve relevance measured average discounted cumulative gain dcg5 6.1 queriesand 1.8 query traffic

paper motivated practical content extraction available data source evaluation benchmark ace program chinese entity detection recognition edr task particular task language independent language dependent challenges e.g rising complication extraction targets word segmentation paper propose novel solution alleviate special task mention detection takes advantages machine learning approaches character based models manipulates types entities mentioned constitution units i.e extents heads separately mentions referring entity linked integrating specific closest rule based pairwise clustering algorithms types mentions entities determined head driven classification approaches implemented system achieves ace value 66.1 evaluated edr 2005 chinese corpus top tier results alternative approaches mention detection clustering discussed analyzed

paper study incorporating domain specific knowledge i.e information concepts relationships concepts domain information retrieval ir system improve effectiveness retrieving biomedical literature effects types domain specific knowledge performance contribution examined based trec platform appropriate domain specific knowledge proposed conceptual retrieval model yields 23 improvement reported result passage retrieval genomics track trec 2006

intersecting inverted indexes fundamental operation applications information retrieval databases efficient indexing operation hard arbitrary data distributions text corpora information retrieval applications convenient power law constraints zipf's law tails allow materialize carefully chosen combinations multi keyword indexes significantly improve worst performance requiring excessive storage multi keyword indexes limit postings accessed computing arbitrary index intersections evaluation commerce collection 20 million products indexes arbitrary keywords intersected accessing 20 postings largest single keyword index

ester modular highly efficient system combined text ontology search ester builds query engine supports basic operations prefix search join implemented efficiently compact index combination provide powerful querying capabilities ester answer basic sparql graph pattern queries ontology reducing basic operations ester supports natural blend semantic queries ordinary text queries moreover prefix search operation allows interactive proactive user interface keystroke suggests user semantic interpretations query speculatively executes interpretations proof concept applied ester english wikipedia contains 3 million documents combined recent yago ontology contains 2.5 million variety complex queries ester achieves worst query processing times fraction single machine index size 4 gb

paper query driven indexing retrieval strategy efficient text retrieval document collections distributed structured p2p network indexing strategy based properties 1 generated distributed index stores posting lists carefully chosen indexing term combinations 2 posting lists containing document references truncated bounded top ranked elements properties guarantee acceptable storage bandwidth requirements essentially indexing term combinations remains scalable transmitted posting lists exceed constant size generated term combinations term statistics extracted available query logs index combinations frequently user queries avoiding generation superfluous indexing term combinations achieve additional substantial reduction bandwidth storage consumption result generated distributed index corresponds constantly evolving query driven indexing structure efficiently follows current information users precisely theoretical analysis experimental results indicate price marginal loss retrieval quality rare queries generated index size network traffic remain manageable web size document collections furthermore experiments time achieved retrieval quality comparable obtained art centralized query engine

noabstract

paper introduces locality discriminating indexing ldi algorithm document classification based hypothesis samples classes reside class specific manifold structures ldi seeks projection preserves class local structures suppresses class overlap comparative experiments proposed method isable derives compact discriminating document representations classification

paper management method morphological variation keywords method called fcg frequent generation based skewed distributions word forms natural languages suitable languages fair amount morphological variation morphologically rich proposed method evaluated languages finnish swedish german russian varying degrees morphological complexity

existing measures evaluating clustering results e.g measure limitation overestimating cluster quality usually adopt greedy matching classes reference clusters clusters system clusters allow multiple classes correspond cluster locally optimal solution paper proposes evaluation strategy overcome limitation existing evaluation measures using optimal matching graph theory weighted bipartite graph built classes clusters disjoint sets vertices edge weight class cluster computed using basic metric total weight optimal matching graph acquired evaluate quality clusters optimal matching allows matching classes clusters globally optimal solution achieved preliminary study performed demonstrate effectiveness proposed evaluation strategy

paper revisit dependence language modelfor information retrieval proposed 1 thismodel deficient theoretical view thenpropose model founded theoretically integratingdependencies terms language model.this model simpler oneproposed 1 yields similar results experiments syntactic semantic dependencies

query ambiguity prevents existing retrieval systems returning reasonable results query lots resolving ambiguity vague queries handled using corresponding approaches separately identified advance quantification degree lack ambiguity laysthe groundwork identification poster propose measure using query topics based topic structure selected directory project odp taxonomy introduce clarity score quantify lack ambiguity respect data sets constructed trec collections rank correlation test results demonstrate strong positive association clarity scores retrieval precisions queries

explore error correcting output codes ecoc enhance performance centroid text classifier framework decompose multi class multiple binary learn individual binary classification centroid classifier decomposition incurs considerable bias centroid classifier results noticeable degradation performance address issue model refinement adjust called bias

paper describes user oriented performance evaluation measure text segmentation experiments proposed measure differentiates error distributions varying user impact

paper describes maximum entropy based story segmentation system arabic chinese english experiments broadcast news data tdt 3 tdt 4 corpora collected darpa gale project obtain substantial performance gain using multiple overlapping windows text based features

approach academic literature search considering unpublished manuscript query search system text previous literature citation graph connects relevant related material evaluate technique manual automatic evaluation methods magnitude improvement mean average precision compared text similarity baseline

federated search task retrieving relevant documents information resources main research federated search combine results sources single ranked list recent proposed regression based method download documents ranked list sources calculated comparable scores documents estimated mapping functions transform source specific scores comparable scores experiments shown downloading documents improves accuracy results merging downloading documents increases computation communication costs paper proposes utility based optimization method enables system automatically decide desired training documents download according user's effectiveness efficiency

searching people's information flowthrough expressing information retrieval request posed asearch engine hypothesise degree specificity anir request correspond length search query ourresults strong correlation decreasing query lengthand increasing broadness generality ir request foundan average cross specificity broad narrow 3words query results implications searchengines responding queries differing lengths

page web snippet document excerpts allowing user understand document indeed relevant accessing paper proposes effective snippet generation method pseudo relevance feedback technique text summarization techniques applied salient sentences extraction generating quality snippets experimental results proposed method performance methods including google naver

paper perspective probability ranking principle prp defining retrieval effectiveness terms novel expected rank measure set documents particular query perspective based preserving decision preferences imposes weaker conditions prp utility theoretic perspective prp

question answering main kinds matching methods finding answer sentences question term based approaches simple efficient effective yield recall event based approaches syntactic semantic information account latter sacrifice recall increased precision actually capture meaning events denoted textual units passage sentence propose robust data driven method learns mapping questions answers using logistic regression combining term based event based approaches significantly outperforms individual methods

confluence enhancement desktop file search tool called confluence extracts conceptual relationships files temporal access patterns file system limitation purely file based approach file operations increasingly abstracted applications correlation user's activity weakens thereby reduces applicability temporal patterns deal augment file event stream stream window focus events ui layer 3 algorithms analyze stream extracting user's task information informs existing confluence algorithms results conclusions preliminary user study confluence

common motivation personalised search systems ability disambiguate queries based knowledge user's analysis log files search providers covering range scenarios suggests sort disambiguation marginal specialised providers web search

noabstract

structured information retrieval aim exploit document structure retrieve relevant components allowing user straight relevant material paper looks called entry beps intended user starting access relevant information document examine relationship beps relevant components inex 2006 ad hoc assessments main findings following documents short assessors choose entry distance start document entry coincide relevant character relevant documents strong relation bep relevant text third browsing beps articles single relevant passages container beps context beps articles relevant passages

noabstract

paper investigate consistency answer assessment complex question answering task examining features assessor consistency types answers question type

propose language model based approach addressing performance robustness respect free parameters values pseudo feedback based query expansion methods query create set language models representing forms expansion varying parameters values expansion method select single model using criteria originally proposed evaluating performance using original query deciding employ expansion experimental results criteria highly effective selecting relevance language models significantly effective poor performing ones yield performance indistinguishable manually optimized relevance models

describe technique automatically classify web page existing bookmark category help user bookmark page hyperbk compares bag words representation page descriptions categories user's bookmark file unlike default web browser dialog boxes user category saved bookmarked file hyperbk offers category similar page bookmarked user opt create category save page elsewhere evaluation user's preferred category offered average 61 time

study classification news articles emotions invoke readers differs previous studies focused classification documents authors emotions instead readers various combinations feature sets combination identifying emotional influences news articles readers

noabstract

paper approaches lexical chains topic segmentation using weighted lexical chains wlc weighted lexical links wll repeated occurrences lemmas text main advantage using approaches suppression empirical parameter called hiatus lexical chain processing evaluation according windowdiff measure automatically built corpus slight improvements wll compared art methods based lexical chains

modeling web query reformulation processes unsolved paper argue lexical analysis highly beneficial purpose propose variation query clarity speech pattern transitions indicators user's search actions experiments log 2.4 million queries techniques flexible current approaches providing insights user's web behavioral patterns

digital library information retrieval technologies provide solutions bridging digital divide developing countries understand information access practices remote poor communities countries understand information communities means provide access relevant information investigated current information access practices indian village

consensus clustering task deriving single labeling applying consensus function cluster ensemble introduces bordaconsensus consensus function soft cluster ensembles based borda voting scheme contrast classic hard consensus functions operate labelings proposal considers cluster membership information able tackle multiclass clustering initial scale experiments reveal compared art consensus functions bordaconsensus constitutes performance vs complexity trade

poster overviews main characteristics flexible retrieval systems shapes binary images discusses evaluation results system applies multiple indexing criteria shapes synthesizing distinct characteristics global features objects contour fourier coefficients boundary irregularities multifractal spectrum presence concavities convexities boundary contour scale space distribution system flexible allows customizing retrieval function fit application query binary image containing desired shape set parameters specifying distinct importance shape characteristics account evaluate relevance retrieved shapes retrieval function defined flexible multicriteria fusion function producing ranked results evaluation experiments system suited retrieval purposes combination distinct shape indexing criteria increases recall precision respect application single indexing criterion

traditional text classification studied ir literature mainly based topics class category represents particular topic e.g sports politics sciences real world text classification require refined classification based semantic aspects example set documents particular disease documents report outbreak disease describe cure disease discuss prevent disease include information classify text semantic level traditional bag words model sufficient paper report text classification study semantic level sentence semantic structure features useful classification experimental results based disease outbreak dataset demonstrated effectiveness proposed approach

relevant context retrieval task document article retrieval twist relevant articles retrieved relevant information article captured set xml elements correctly identified main research question evaluate relevant context task propose generalized average precision measure meets main requirements score reflects ranked list articles inherent result list time ii score reflects retrieved information article i.e set elements corresponds relevant information resulting measure inex 2006

prior attempts theoretically justify effectiveness inverse document frequency idf starting robertson sparck jones's probabilistic model based strong complex assumptions intuitively plausible assumption suffices moreover assumption conceptually simple provides solution estimation deemed intractable robertson walker 1997

examine validity power test wilcoxon test sign test determining difference performance ir systems significant empirical tests conducted subsets trec2004 robust retrieval collection indicate values computed tests difference mean average precision map systems accurate fora wide range sample sizes significance estimates similarly tests power test proving superior overall test valid comparing geometric mean average precision gmap exhibiting slightly superior accuracy slightly inferior power mapcomparison

poster introduces novel approach information retrieval statistical model averaging improve latent semantic indexing lsi instead choosing single dimensionality lsi propose using models differing dimensionality inform retrieval manage ensemble weight model's contribution extent inversely proportional aic akaike information criterion model contributes proportionally expected kullback leibler divergence distribution generated data results standard ir test collections demonstrating significant improvement traditional vector space model single model lsi

investigate diverse goals people issue query search engine ability current search engines address diversity quantify potential value personalizing search results based analysis variance found results individuals rated relevant query information goal expressed analysis suggests search engines job ranking results maximize global happiness job specific individuals

enterprise search exists relationship task document type refine search results poster adapt popular okapi bm25 scoring function weight term frequency based relevance document type task click frequency task type pair estimate realistic weight using w3c collection trec enterprise track evaluations approach leads significant improvements search precision

information sources contain information accessed search specific search engines federated search provides search solutions type hidden information searched conventional search engines scenarios federated search search health care providers intelligence agencies individual information source disclose source search results users sources paper proposes step federated search protocol protects privacy information sources attempt address research protecting source privacy federated text search

propose approach qrrs query relaxative ranking svm divides ranking function relaxation steps cheap features ranking svm steps query efficiency search quality approach improved compared conventional ranking svm

collection selection ranking collections according user query crucial distributed search features rank collections current collection selection methods hundreds features exploited rank web pages web search lack features affects efficiency collection selection distributed search paper exploit features learn rank collections svm rankingsvm respectively experimental results features beneficial collection selection learned ranking functions outperform classical cori algorithm

paper novel online video recommendation system called videoreach alleviates users efforts finding relevant videos according current viewings sufficient collection user profiles required traditional recommenders system video recommendation formulated finding list relevant videos terms multimodal relevance i.e textual visual aural relevance user click videos intra weights relevance individual modality inter weights modalities adopt relevance feedback automatically optimal weights user click attention fusion function fuse multimodal relevance 20 clips representative test videos searched top 10 queries 13k online videos report superior performance compared existing video site

modern information retrieval ir test collections violate completeness assumption cranfield paradigm maximise available resources sample documents i.e pool judged relevance human assessor subsequent evaluation protocol distinctions assessed unassesseddocuments documents pool assumedto relevant topic beneficial practical view relative performance compared confidence experimental conditions fair systems incompleteness relevance assessments forms uncertainty emerge evaluation aleatory uncertainty refers variation system performance topic set addressed statistical significance tests form uncertainty epistemic refers amount knowledge ignorance estimate system's performance epistemic uncertainty consequence incompleteness addressed current evaluation protocol study attempt modelling aleatory epistemic uncertainty associatedwith ir evaluation aim account variability associated system performance amount knowledge performance estimate

paper discusses importance preservation shape matching descriptor introduced supports preserving abandoning evaluation retrieval results improved 38 original preserved

paper experimental study users assessing quality google web search results particular look users satisfaction correlates effectiveness google quantified ir measures precision suite cumulative gain measures cg dcg ndcg results indicate strong correlation users satisfaction cg precision moderate correlation dcg surprisingly negligible correlation ndcg reasons low correlation ndcg examined

paper novel filtering system based model reshapes aims content based filtering filtering system developed ec project peng aimed providing news professionals journalists system supporting filtering retrieval capabilities particular suggest tackling information overload filtering systems account multiple aspects incoming documents estimate relevance user's profile help users understand documents distinct solely attempting select relevant material stream block inappropriate material aiming filtering model based multiple criteria defined based ideas gleamed project requirements stage filtering model briefly described paper

paper revisits static term based pruning technique carmel et al sigir 2001 ad hoc retrieval addressing issues concerning algorithmic design account original technique able retain precision considerable inverted file removed improve precision scenarios key design features properly selected

argued information extraction ie machine learning ml approaches save development time ml methods e.g active learning require training data saving development cost development cost claims normally controlled studies development cost savings actually occur situation language engineering contrasted software engineering lot studies investigating system development cost carried argue controlled studies measure actual system development time language engineering carry experiment resource monitoring ie task named entity taggers surprise domain developed parallel using competing methods human development time accounted forusing logging facility.we report development cost results parallel implementations named entity tagger breakdown development time alternative methods aware detailed previous parallel studies detail system development time spent creating named entity tagger

noabstract

topical classification web queries drawn recent promise offers improving retrieval effectiveness efficiency promise depends classification performed query retrieve documents examine previously unaddressed issues query classification pre versus post retrieval classification effectiveness effect training explicitly classified queries versus bridging classifier trained using document taxonomy bridging classifiers map categories document taxonomy onto query classification provide sufficient training data training classifiers explicitly manually classified queries outperforms bridged classifier 48 f1 score pre retrieval classifier using query terms performs merely 11 worse bridged classifier requires snippets retrieved documents

test collections useful reusable reliably rank systems contribute pools pooled relevance judgments collections reusable easons sparse sufficiently complete biased sense theywill unfairly rank class systems trec 2006 terabyte track judged pool deep random sample measure effects sparseness bias

subscribers popular news blog feeds rss atom information overload feed sources usually deliver items periodically solution clustering similar items feed reader information manageable user clustering items feed reader challenging task usually actual article received feed paper propose method improving accuracy clustering short texts enriching representation additional features wikipedia empirical results indicate enriched representation text items substantially improve clustering accuracy compared conventional bag words representation

collection size feature represent content summaries collection plays vital role collection selection distributed search uncooperative environments collection size estimation algorithms adopted estimate sizes collections search interfaces paper proposes heterogeneous capture hc algorithm capture probabilities documents modeled logistic regression heterogeneous capture probabilities hc algorithm estimates collection size conditional maximum likelihood experimental results real web data hc algorithm outperforms multiple capture recapture capture history algorithms

domain video content retrieval approach selecting words phrases highly imperfect automatically generated transcripts extracted terms ranked according descriptiveness user multimedia browser interface sense querying wordnet lexical database method text selection ranking evaluation 679 video summarization tasks 442 users method ranking emphasizing terms according descriptiveness results accuracy responses time compared baseline ranking

paper proposes method combine text based citation based retrieval methods invalidity patent search using ntcir 6 test collection including eight uspto patents effectiveness method experimentally

poster focuses study term context dependence application sentence retrieval based markov random field mrf forms dependence query terms considered assumptions term dependence relationship feature functions defined purpose utilize association features query terms sentence evaluate relevance sentence experimental results proven efficiency proposed retrieval models improving performance sentence retrieval

ad hoc retrieval task query usually short user expects relevant documents result pages explored possibilities using wikipedia's articles external corpus expand ad hoc queries results promising improvements measures emphasize weak queries

noabstract

noabstract

tokenization fundamental preprocessing step information retrieval systems text index terms paper quantifies compares influence various simple tokenization techniques document retrieval effectiveness domains biomedicine news expected biomedical retrieval sensitive changes tokenization method tokenization strategy difference mediocre performing ir system especially biomedical domain

paper addresses learning classify textsby exploiting information derived clustering training testing sets incorporation knowledge resulting clustering feature space representation texts expected boost performance classifier experiments conducted widely datasets demonstrate effectiveness proposed algorithm especially training sets

online news blog aggregators google yahoo msn allow users browse search hundreds news sources results dozens hundreds stories event news aggregators cluster stories allowing user efficiently scan major news items time currently allow alternative browsing mechanisms clusters furthermore intra cluster ranking mechanisms based notion authority popularity source leads classic power law phenomenon popular stories sources ones popular authoritative reinforcing dominant viewpoint ideally aggregators exploit availability tremendous sources identify various dominant threads viewpoints story highlight threads users paper initial limited approach interface classifies articles categories opinion combination classifier trained 140k training set editorials reports ii interactive user interface ameliorates classification errors re presentation effective highlighting underlying viewpoints story cluster briefly discuss classifier training set ui report initial anecdotal user feedback evaluation

investigate difficult matching semi structured resumes jobs scale real world collection compare standard approaches structured relevance models srm extensionof relevance based language model modeling retrieving semi structured documents preliminary experiments srm approach achieved promising performance performed typical unstructured relevance models

online product reviews opinion sources web paper studies determining semantic orientations positive negative opinions expressed product features reviews existing approaches set opinion words purpose semantic orientations words context dependent paper propose linguistic rules deal opinion aggregation function extensive experiments rules function highly effective system called opinion observer built

identifying redundant information sentences useful applications summarization document provenance detecting text reuse novelty detection task identifying redundant information sentences defined follows query sentence task retrieve sentences collection express subset information query sentence sentence retrieval techniques rank sentences based measure similarity query effectiveness techniques depends similarity measure rank sentences effective retrieval model able handle low word overlap query candidate sentences beyond word overlap simple language modeling techniques query likelihood retrieval outperformed tf idf word overlap based methods ranking sentences paper compare performance sentence retrieval using language modeling techniques identifying redundant information

paper formulate image retrieval text query vector space classification achieved creating dimensional visual vocabulary represents image documents detail representation image documents enables application text retrieval techniques rocchio tf idf na 237 ve bayes semantic image retrieval tested methods corel images subset achieve art retrieval performance using proposed methods

paper experiments using algorithm web page topic segmentation significant precision improvement retrieval documents issued web track corpus trec 2001 instead processing document web page segmented semantic blocks according visual criteria horizontal lines colors structural tags headings lt h1 gt lt h6 gt paragraph lt gt conclude combining visual content layout criteria results increasing precision ranking page calculated relevant segments pages resulting segmentation algorithm

clustering algorithms widely information retrieval applications difficult define objective result article analyzes document clustering algorithms illustrates equivalent optimization global functions experiments performance counter examples fail return optimal solution argue monte carlo algorithms global optimization framework potential solutions traditional clustering able handle complex structures

task finding people experts topic recently received increased attention introduce expert finding task example experts instead natural language query system's task return similar experts define compare evaluate representing experts investigate size theinitial example set affects performance morefine grained representations candidates result performance larger sample sets input lead improved precision

class imbalance hinder learning performance classification algorithms various real world classification tasks text categorization suffer phenomenon demonstrate active learning capable solving

identification plagiarized passages document collections retrieval strategies rely stochastic sampling chunk indexes using entire wikipedia corpus compile gram indexes compare fingerprint index plagiarism analysis index provides analysis speed factor 1.5 magnitude equivalent terms precision recall

paper address task automatically finding expert organization expert search theoretically based probabilistic algorithm models retrieved documents mixtures expert candidate language models experiments approach outperforms existing theoretically sound solutions

estimating term weights information retrieval ir using term co occurrence measure dependency terms.we random walk graph based ranking algorithm graph encodes terms co occurrence dependencies text derive term weights represent quantification term contributes context evaluation trec collections 350 topics random walk based term weights perform comparably traditional tf idf term weighting outperform distance co occurring terms 6 30 terms

query logs pseudo relevance feedback prf offer terms refine web searchers queries selected offered searchers improve search effectiveness poster study techniques aims characterize degree similarity set test queries set broken query type results suggest similarity increases amount evidence provided prf algorithm ii similarity higherwhen titles snippets prf text iii similarity navigational informational queries findings implications combined usage query logs prf generating query refinement alternatives

noabstract

noabstract

define method estimate random systematic errors resulting incomplete relevance assessments.mean average precision map computed topics shallow assessment pool substantially outperforms adjudication effort map computed fewer topics deeper pools computed pools depth move front pooling previously reported yield substantially rank correlation yields similar power lower bias compared tofixed depth pooling

poster describes potential relatively measure information retrieval research kendall's tau rank correlation coefficient coefficient determining similarity test collections ranking sets retrieval runs threshold values coefficient defined published studies information retrieval poster results basing decisions thresholds reliableas assumed

opinion holder extraction research discriminating opinions viewed perspectives paper describe experience participation ntcir 6 opinion analysis pilot task focusing opinion holder extraction results japanese english approach opinion holder extraction based discrimination author authority viewpoints opinionated sentences evaluation results fair respect japanese documents

term dependency co occurrence studied language modelling instance metzler croft retrieval performance significantlyenhanced using term dependency information weshow term dependency modelled divergence randomness dfr framework evaluate term dependency model adhoc retrieval tasks using trec gov2 terabyte collection furthermore examine effect varying term dependency window size retrieval performance proposed model experiments term dependency indeed besuccessfully incorporated dfr framework

question answer portals naver yahoo answers growing popularity despite increased popularity quality answers uneven users usually provide answers provide bad answers hence estimating authority expected quality users crucial task emerging domain potential applications answer ranking incentive mechanism design adapt powerful link analysis methodology web domain step towards estimating authority question answer portals experimental results 3 million answers yahoo answers promising warrant exploration lines outlined poster

fraction queries submitted web search enginesoccur infrequently describe search log studiesaimed elucidating behaviors associated rare andcommon queries analyses discussresearch directions

dimensionality reduction plays role efficient similarity search based nearest neighbor nn queries dimensional feature space paper introduce novel type nn query namely conditional nn ck nn considers dimension specific constraint addition inter distances existing dimensionality reduction methods applicable type queries propose novel mean std standard deviation guided dimensionality reduction msdr support pruning based efficient ck nn query processing strategy preliminary experimental results 3d protein structure data demonstrate msdr method promising

focused structured document retrieval employs concept entry bep intended provide optimal starting users browse relevant document components 4 paper describe evaluate method finding beps xml documents experiments conducted framework inex 2006 evaluation campaign wikipedia xml collection 2 shown effectiveness proposed approach

sponsored search major revenue source search companies web searchers issue queries advertisement keywords limited query rewriting technique effectively matches user queries relevant advertisement keywords increases amount web advertisements available match relevance critical clicks study aim improve query rewriting relevance purpose active learning algorithm called transductive experimental design select informative samples train query rewriting relevance model experiments approach significantly improves model accuracy rewriting relevance

studies focus web focus peer peer file sharing system queries despite massive scale terms internet traffic analyzed million queries collected gnutella network differentiated findings web queries

sponsored links primary business model web search engines providing web consumers relevant results research addresses issue investigating relevance sponsored sponsored links ecommerce queries major search engines results average relevance ratings sponsored sponsored links virtually relevance ratings sponsored links statistically 108 ecommerce queries 8,256 retrieved links queries major web search engines google msn yahoo implications web search engines sponsored search term business model mechanism finding relevant information searchers

research investigate model online searching learning paradigm examined searching characteristics 41 participants engaged 246 searching tasks classified searching tasks according anderson krathwohl's taxonomy updated version bloom's taxonomy anderson krathwohl six level categorization cognitive learning research results applying takes searching effort measured queries session specific topics searched sessions categories remembering understanding lower learning levels exhibit searching characteristics similar categories evaluating creating searchers rely primarily internal knowledge searching primarily checking verification engaged evaluating creating implications commonly held notions web searchers simple information goals correct discuss implications web searching including designing interfaces support exploration alternate views

noabstract

successful navigation relevant web page relevant pages depends page linking relevant pages measured distance travel relevant page relevant page found bimodal distribution distances peaking 4 15 hops attempt easier navigate relevant pages added content similarity links pages additional links significantly relevant documents close browser plug tool provides links pages similar page increase ability web users relevant pages via navigation

common language modeling approach assumes data generated mixture language models em algorithm usually maximum likelihood estimation unknown mixture component mixture weights language models paper provide efficient algorithm complexity exact solution words occurred merit probabilities words exactly zeros means mixture language model serves feature selection technique

graph ranking based algorithms e.g textrank proposed multi document summarization recent algorithms miss dimension temporal dimension summarizing evolving topics evolving topic recent documents usually earlier documents recent documents contain novel information earlier documents novelty oriented summary appropriate reflect changing topic propose timedtextrank algorithm temporal information documents based graph ranking based algorithm preliminary study performed demonstrate effectiveness proposed timedtextrank algorithm dynamic multi document summarization

web includes pages intended deceive search engines attain unwarranted result ranking links web pages calculate authority ranking systems benefit knowing pages contain content trusted propose compare various trust propagation methods estimate trustworthiness page trust preserving propagation method able achieve close fifty percent improvement trustrank separating spam spam pages

mobile spam increasing threat addressed using filtering systems employed email spam believe email filtering techniques require adaptation reach levels performance sms spam especially regarding message representation test assumption performed experiments sms filtering using top performing email spam filters mobile spam messages using suitable feature representation results supporting hypothesis

web page relevant multiple topics nominally single topic page attract attention links multiple communities instead indiscriminately summing authority provided pages decompose web page separate subnodes respect community considering relevance communities able model query specific reputation potential result apply total 125 queries trec gov dataset demonstrate community relevance improve ranking performance

poster investigates novel query suggestion technique selects query refinements combination users post query navigation patterns query logs search engine compare technique queries retrieve top ranked search results searchers post query browsing i.e landing pages approach based query refinements user search sessions extracted query logs findings demonstrate effectiveness using landing pages direct generation query suggestions complementary nature suggestions generates regard traditional query log based refinement methodologies

using saracevic's relevance types explore approaches combining algorithm cognitive relevance term relevance feedback scenario data collected 21 users provided relevance feedback terms suggested system 50 trec hard topics former type feedback considered cognitive relevance latter type considered algorithm relevance construct retrieval runs using types relevance feedback experiment combining simple boolean operators results minimal differences performance respect techniques

describe preliminary analysis queries created 81 users 4 topics trec robust track goal explore potential benefits using queries created multiple users retrieval performance difficult topics examine overlap users queries overlap results respect queries topic explore potential benefits combining users queries various results provide evidence access multiple users queries improve retrieval individual searchers difficult topics

mailing list archives enterprise valuable source employees dig past proceedings organization relevant task proceedings discussions topics cumbersome regular search techniques context due genre documents belong paper propose methods based theory subjectivity retrieve email messages contain argumentative discussions topic user

finding significant contextual features challenging task development interactive information retrieval ir systems paper investigated simple method facilitate task looking aggregated relevance judgements retrieved documents study suggested agreement relevance judgements indicate effectiveness retrieved documents source significant features effect highly agreed documents practical implication design adaptive search models interactive ir systems

conceal content communications replacing words trigger attention example instead writing bomb position terrorist chose write flower position substituted sentence sound bit odd human reader shown prior research oddity detectable text mining approaches importance component suggested oddity detection approach thoroughly investigated approach compared obvious candidate task hidden markov models hmm explore oddity detection algorithms reported earlier specifically based pointwise mutual information pmi hidden markov models hmm

real world test collection methods essential query test set representative load expected actual application using random sample queries media company's query log gold standard test set demonstrate biases sitemap derived top query sets lead significant perturbations engine rankings differences estimated performance levels

paper contribution image indexing applied document creation task method ranks set photographs based aesthetically predefined document color harmony document visual balance image quality consideration user study conducted people range expertise document creation helped gather visual features consider algorithm benefits traditional document creation task changing web page banner colors layout

cluster based retrieval cbr experiments largest available turkish document collection experiments evaluate retrieval effectiveness efficiency automatically generated clustering structure manual classification documents particular compare cbr effectiveness text search fs evaluate implementation alternatives cbr findings reveal cbr yields comparable effectiveness figures fs furthermore using specifically tailored cluster skipping inverted index significantly improve memory query processing efficiency cbr comparison traditional cbr techniques fs

active learning efficiently hones decision boundary relevant irrelevant documents process miss entire clusters relevant documents yielding classifiers low recall paper propose method increase active learning recall constraining sampling document subset rich relevant examples

developing support creativity learning information discovery exploratory search users engage creative tasks inventing products services system supports evolving information gathers relevant information visually using images text users able search browse explore results multiple queries interact information elements manipulating design expressing field study conducted evaluate system undergraduate class results demonstrated efficacy system developing creative ideas exposure diverse information visual interactive forms shown support students engaged invention tasks

noabstract

iskodor integrates personal collections peer search centralized search services user modeling iskodor fills roles discovery sites suitable information stores context based query interpretation automatic profile based filtering information explanation control achieved graphical depiction sources explicit feedback regard ingutility explicit control peer association behavior information sharing

noabstract

professionals workplace precision search tools capable retrieving information useful appropriate task hand approach identifying content relevant useful task context search site enterprise search engine software engineering domain exploits relationships user's tasks document genres collection improve retrieval precision

suppose mobile device keyboard e.g cell phone perform near search nearest pizza enter queries quickly t9 wild encourages users enter patterns implicit explicit wild cards regular expressions search engine microsoft local live logs queries particular location example 7 6 short hand regular expression pqrs mno matches post office space needle seattle queries local pizza whereas boeing company seattle chicago moderately nearby somewhat elsewhere smoothing query observed

noabstract

radio oranje demonstrator attractive multimedia user experience cultural heritage domain based collection mono media audio documents supports online search browsing collection using indexing techniques specialized content visualizations related photo database

project develops tools manage personal memories include multimedia retrieval system user interfaces devices paper demonstration mobile interface allows browsing retrieving taking pictures automatically annotated gps data audio information multimedia retrieval system multimodal information visual content gps metadata audio information interface evaluated cultural heritage site

noabstract

noabstract

describe novel system evaluating performing stream based text categorization stream based text categorization considers text categorized stream symbols differs traditional feature based approach relies extracting features text system implements character based languages models specifically models based ppm text compression scheme count based measures measure measure system demonstrates techniques outperform svm feature based classifier stream related classification tasks authorship ascription

noabstract

develop implement indexing technology allows complete possibly documents queries retrieval performance comparable standard term query approach aims retrieval tasks near duplicate detection similarity search demonstrate performance technology compiled search index wikipedia pocket contains 2 million english german wikipedia articles sup 1 sup index search interface fits conventional cd 0.7 gigabyte ingredients indexing technology similarity hashing minimal perfect hashing

noabstract

traditional ranking schemes relevance web page user query search engine appropriate search term contains geographic information geographic entities addresses city names location names appear twice web page typically heading larger font consequently alternative ranking approach traditional weighted tf idf relevance ranking web site contains geographic entity neighbours refer entity refer geographic entities local search engine applies novel ranking algorithm suitable ranking web pages geographic content describe major components geographic ranking focused crawling geographic extractor related web sites feature

debut pagerank hits hyperlink induced web document ranking web increasingly vast topically diverse vastness led topic sensitive ranking variants address dimensionality web providing tools focused search focused search engine seeks coverage subset topics web users relevant search results domain demonstration introduce readers genieknows.com vertical search engine

noabstract

noabstract

research explores relationship information retrieval ir systems effectiveness users performance accuracy speed satisfaction retrieved results precision results completeness results overall system success previous studies concluded improvements ir systems based increase ir effectiveness measures reflect improvement users performance aims exploiting factors possibly considered confounding variables interactive information retrieval iir evaluation research look substantive approaches evaluate iir systems aim build interactive evaluation framework brings aspects systems effectiveness users performance satisfaction research involves developing methods capturing users satisfaction retrieved results ir systems examination users assess own performance task completion furthermore identifying evaluation measures batch mode interactive experiment correlate interactive ir system research hope develop valid reliable metrics iir evaluation study set explore relationship system effectiveness quantified traditional measures precision recall users effectiveness satisfaction results study limited users tasks involve finding images recall based tasks concluded direct relationship system effectiveness users performance people learn adapt system regardless effectiveness study recommends combination measures e.g system effectiveness user performance satisfaction evaluate iir systems based observation study found users familiarity search topic increased performance set experiment investigate users satisfaction correlate ir effectiveness measures precision suite cumulative gain measures cg dcg ndcg simple web searching tasks results study shown cg precision ndcg reflecting users satisfaction results ir system concluded users web search engines context simple search task concerned precision completeness search stemmed stronger correlation users satisfaction success overall search satisfaction accuracy search results satisfaction completeness search scholars 1 2 3 4 recommended considering perceptions users ir effectiveness measures interpreted measures effectiveness issue iir evaluation focusing maximizing retrieval performance refining ir techniques understanding users satisfaction behaviors information raises investigation measures translate users performance satisfaction criterion system future plans incorporate variables domain knowledge motivation task complexity search behaviours user performance users evaluation ir system performance evaluating interactive ir systems attempt explore suitability measures iir evaluation proposed approach adopts systematic multidimensional approach evaluation including classical traditional evaluation measures precision recall interactive traditional measures users characteristics satisfaction

noabstract

noabstract

proposed phd thesis examined attention data user especially generated eye tracker exploited enhance personalize information retrieval methods

logical approach information retrieval tries model relevance document query logical implication documents queries van rijsbergen retrieval status value document query proportional degree implication document query based probabilistic logics information retrieval conceived add additional layer abstraction information retrieval task probabilistic models retrieval documents expressed logics implemented directly aim research develop logic ir task document summarisation summarisation logic adds abstraction layer summarisation task similarly logic document retrieval adds layer retrieval task probabilistic models document summarisation expressed logical formulae actual implementation hidden logical expressions extract worthyness textual components logic measured degree implication textual components surrounding contexts providing measure components context situated

human activity defines information context awaken start hold meetings roughly time day retrieve information items day planners itineraries schedules agendas reports menus web pages activities information retrieval systems lack sensitivity recurrent context requiring users remember re enter search cues objects regardless regularly consistently objects develop ad hoc storage strategies propose addition semantic cues information objects indexed temporal sensory cues clock time location objects retrieved external environmental context addition internal semantic content cue event object ceo model network representation associate information objects times conditions location weather typically users query system review activities revealing particular times information objects tend system pre fetch items proven useful past similar situations ceo model incremental real time dynamic maintaining accurate summary user's information behaviour changes time environmentally aware systems applications personal information management mobile devices smart homes memory prosthesis model support autonomous living cognitively impaired comprehensive research agenda based promising preliminary findings

temporal spatial information text documents expressed qualitative moreover frequently affected vagueness calling appropriate extensions traditional frameworks qualitative reasoning time space research aims defining extensions based fuzzy set theory applying resulting frameworks kinds intelligent information retrieval viz temporal question answering geographic information retrieval

noabstract

thesis paper tackles selected issues unstructured peer peer information retrieval p2pir systems using world knowledge solving p2pir called reference corpora estimating global term weights idf instead sampling distributed collection dedicated question query routing unstructured p2pir systems using peer resource descriptions world knowledge query expansion

content based multimedia information retrieval defined task matching multi modal information various components multimedia corpus retrieving relevant elements matching retrieval takes multiple features visual audio level low level seen independent retrieval expert task answering query formulated data fusion depending query expert perform retrieval coefficients weight expert increase overall performance previous approaches expert coefficient generation included query independent coefficients identification query classes machine learning methods name approach propose seeks dynamically create expert coefficients query dependent approach based earlier experiments initial correlation observed score distribution retrieval expert relative performance compared experts query created basic method leverages observations create query time coefficients achieve comparable performance oracle determined query independent weights experts collections aforementioned experiment previous research examinedscore distribution respect relevance whereas seeks compare expert scores query determine relative performance aim explore correlation eliminating potential bias data collections retrieval experts queries experiments obtain robust observations using extending previous investigations data fusion explore data fusion succeeds multimedia retrieval aim refine extend existing techniques automatic coefficient generation incorporate observations improve performance finally combine approach existing data fusion methods query class coefficients approach complimenting achieve performance improvements

google entered china market late comer late 2005 local employees inadequate product line market share talk discuss google china's efforts build team learn local user apply global innovation model won users past 2.5 talk cover results user studies key findings chinese users searching using internet discuss findings applied products products gained traction market discuss google's progress chinese search relevance search user experience key technology innovated talk discuss process internationalization google hired locally applied global 20 project approach encourage truly relevant local innovations discuss examples innovations product innovations weather map input method editor sms greetings search research innovations parallel svm svd google china's progress dispelled myth multinational internet companies succeed china key ingredients success story focus customer embrace corporate culture empower local flexibility course innovate innovate innovate

exploitation fundamental invariants elegant solutions computational wide variety domains powerful approaches exploit invariants principle guilt association particular principle guilt association foundation remote homolog detection protein function prediction disease subtype diagnosis treatment plan prognosis challenges computational biology principle suggests entities specific relationship exhibit invariant properties underlying relationship example protein predicted particular biological function exhibits underlying invariant properties functional viz guilty association functional shared invariant properties talk plan facets guilt association computational prediction protein function draw parallels facets information retrieval specifically plan touch following facets issue chance associations novel generalizable forms association fusion multiple heterogeneous sources evidence dichotomy knowing degree reliability entities relationship knowing relationship hope talk informational retrieval community window opportunities computational biology benefit depth variety solutions information retrieval offer

searching medical information web highly popular remains challenging task searchers uncertain exact medical situations unfamiliar medical terminology address challenge built intelligent medical web search engine called imed medical knowledge interactive questionnaire help searchers form queries paper focuses imed's iterative search advisor integrates medical linguistic knowledge help searchers improve search results iteratively iterative process common web search especially crucial medical web search searchers miss desired search results due limited medical knowledge task's inherent difficulty imed's iterative search advisor helps searcher relevant symptoms signs automatically suggested based searcher's description situation instead taking granted searcher's answers questions imed ranks recommends alternative answers according likelihoods correct answers third related mesh medical phrases suggested help searcher refine situation description demonstrate effectiveness imed's iterative search advisor evaluating using real medical records usmle medical exam questions

handling queries involve pruning query retain terms reduction expanding query include related concepts expansion automatic techniques exist roughly 25 performance improvements terms map realized past interactive variants selectively reducing expanding query leads average improvement 51 map baseline standard trec test collections demonstrate user interaction achieve improvement interaction techniques users fixed options queries achieve improvements interacting user i.e techniques identify optimal options users resulting interface average 70 fewer options consider previous algorithms supporting interactive reduction expansion exponential nature extend utility operational environments techniques complexity algorithms polynomial finally analysis queries continue exhibit poor performance spite techniques

context document retrieval biomedical domain paper explores complex relationship quality initial query results overall utility interactive retrieval system demonstrate content similarity browsing tool compensate poor retrieval results relationship retrieval performance overall utility linear arguments advanced user simulations characterize relevance documents user encounter browsing strategies broader implications ir provides study user simulations exploited formative tool automatic utility evaluation simulation based studies provide researchers additional evaluation tool complement interactive cranfield style experiments

searching people web common query types web search engines person name queried returned webpages contain documents related distinct namesakes queried name task disambiguating finding webpages related specific person left user web people search weps approaches developed recently attempt automate disambiguation process nevertheless disambiguation quality techniques leaves major improvement paper server weps approach based collecting co occurrence information theweb theweb external data source skyline based classification technique developed classifying collected co occurrence information clustering decisions clustering technique specifically designed handle dominance exists data adapt clustering quality measure properties allow framework major advantage terms result quality weps techniques aware including 18 methods covered recent weps competition 2

developed unsupervised framework simultaneously extracting normalizing attributes products multiple web pages originated sites framework designed based probabilistic graphical model model page independent content information page dependent layout information text fragments web pages characteristic framework previously unseen attributes discovered clue contained layout format text fragments framework tackles extraction normalization tasks jointly considering relationship content layout information dirichlet process prior employed leading advantage discovered product attributes unlimited unsupervised inference algorithm based variational method semantics normalized attributes visualized examining term weights model framework applied wide range web mining applications product matching retrieval conducted extensive experiments domains consisting 300 web pages 150 web sites demonstrating robustness effectiveness framework

web search engine provide quality results queries users utilize multiple search engines paper propose evaluate framework maximizes users search effective ness directing engine yields results current query contrast prior meta search advocate replacement multiple engines aggregate facilitate simultaneous individual engines describe machine learning approach supporting switching search engines demonstrate viability tolerable interruption levels findings implications fluid competition search engines

goal system evaluation information retrieval determine set systems superior collection tool determine system evaluation metric average precision computes relative collection specific scores argue broader goal achievable paper demonstrate standardization scores substantially independent particular collection allowing systems compared tested collections compared current methods techniques provide richer information system performance improved clarity outcome reporting simplicity reviewing results disparate sources

test collections extensively evaluation information retrieval systems crucial degree results predict user effectiveness past studies substantiate relationship system user effectiveness recently correlations begun emerge results paper strengthen extend findings introduce novel methodology investigating relationship success establishing significant correlation system user effectiveness shown users behave discern differences pairs systems absolute difference test collection effectiveness results strengthen test collections ir evaluation confirming users effectiveness predicted successfully

various measures binary preference bpref inferred average precision infap binary normalised discounted cumulative gain ndcg proposed alternatives mean average precision map sensitive relevance judgements completeness primary aim system building train system respond user queries robust stable manner paper investigate importance choice evaluation measure training levels evaluation incompleteness simulate evaluation incompleteness sampling relevance assessments scale experiments standard trec test collections examine retrieval sensitivity training i.e training process based discussed measures impact final retrieval performance experimental results training bpref infap ndcg provides significantly retrieval performance training map relevance judgements completeness extremely low relevance judgements completeness increases measures behave similarly

widespread deployment recommender systems lead user feedback varying quality users faithfully express true opinion provide noisy ratings detrimental quality generated recommendations presence noise violate modeling assumptions lead instabilities estimation prediction worse malicious users deliberately insert attack profiles attempt bias recommender system benefit previous research attempted study robustness various existing collaborative filtering cf approaches remains unsolved approaches neighbor selection algorithms association rules robust matrix factorization produced unsatisfactory results describes collaborative algorithm based svd accurate highly stable shilling algorithm exploits previously established svd based shilling detection algorithms combines svd based cf experimental results diminished effect kinds shilling attacks offers significant improvement previous robust collaborative filtering frameworks

recommender system able suggest items preferred user systems degree preference represented rating score database users past ratings set items traditional collaborative filtering algorithms based predicting potential ratings user assign unrated items ranked predicted ratings produce list recommended items paper propose collaborative filtering approach addresses item ranking directly modeling user preferences derived ratings measure similarity users based correlation rankings items rating values propose collaborative filtering algorithms ranking items based preferences similar users experimental results real world movie rating data sets proposed approach outperforms traditional collaborative filtering algorithms significantly ndcg measure evaluating ranked results

collaborative filtering cf requires user rated training examples statistical inference preferences users active learning strategies identify informative set training examples minimum interactions users current active learning approaches cf implicit unrealistic assumption user provide rating queried item paper introduces approach assumption personalize active learning user query items user provide rating propose extended form bayesian active learning aspect model cf illustrate examine idea comparative evaluation method established baseline method benchmark datasets statistically significant improvements method performance baseline method representative existing approaches personalization account

paper boosting based algorithm learning bipartite ranking function brf partially labeled data attempts build brf transductive setting test methods advance unlabeled data proposed approach semi supervised inductive ranking algorithm opposed transductive algorithms able infer examples training evaluate approach using trec 9 ohsumed reuters 21578 data collections comparing semi supervised classification algorithms rocarea auc uninterpolated average precision aup mean precision 50 tp precision recall pr curves unbalanced irrelevant examples relevant ones method produce statistically significant improvements respect ranking measures

central issues learning rank information retrieval develop algorithms construct ranking models directly optimizing evaluation measures information retrieval mean average precision map normalized discounted cumulative gain ndcg algorithms including svm sup map sup adarank proposed effectiveness verified relationships algorithms furthermore comparisons conducted paper conduct study approach directly optimizing evaluation measures learning rank information retrieval ir focus methods minimize loss functions upper bounding basic loss function defined ir measures provide framework study analyze existing algorithms svm sup map sup adarank framework framework based upper bound analysis types upper bounds discussed moreover derive algorithms basis analysis create example algorithm called permurank conducted comparisons svm sup map sup adarank permurank conventional methods ranking svm rankboost using benchmark datasets experimental results methods based direct optimization evaluation measures outperform conventional methods ranking svm rankboost significant difference exists performances direct optimization methods themselves

ranking models proposed information retrieval recently machine learning techniques applied ranking model construction existing methods consideration significant differences exist queries resort single function ranking documents paper argue employ ranking models queries onduct call query dependent ranking attempt propose nearest neighbor knn method query dependent ranking consider online method creates ranking model query using labeled neighbors query query feature space rank documents respect query using created model offline approximations method create ranking models advance enhance efficiency ranking prove theory indicates approximations accurate terms difference loss prediction learning algorithm stable respect minor changes training examples experimental results proposed online offline methods outperform baseline method using single ranking function

efficient similarity search dimensional spaces content based retrieval systems recent studies shown sketches effectively approximate l1 distance dimensional spaces filtering sketches speed similarity search magnitude challenge reduce size sketches compact compromising accuracy distance estimation paper efficient sketch algorithm similarity search l2 distances novel asymmetric distance estimation technique asymmetric estimator takes advantage original feature vector query boost distance estimation accuracy apply asymmetric method existing sketches cosine similarity l1 distance evaluations datasets extracted images telephone records l2 sketch outperforms existing methods asymmetric estimators consistently improve accuracy sketch methods achieve search quality asymmetric estimators reduce sketch size 10 40

results caching efficient technique reducing query processing load hence commonly real search engines technique bounds maximum hit rate due fraction singleton queries limitation paper propose resin architecture combination results caching index pruning overcome limitation argue results caching inexpensive efficient reduce query processing load cheaper implement compared pruned index time index pruning performance fundamentally affected changes query traffic results cache induces experiment real query logs document collection combination techniques enables efficient reduction query processing costs practical web search engines

recent research demonstrated beyond doubts benefits compressing natural language texts using word based statistical semistatic compression achieves extremely competitive compression rates direct search compressed text carried faster original text indexing based inverted lists benefits compression compression methods assign variable length codeword text word coding methods plain huffman restricted prefix byte codes mark codeword boundaries hence accessed random positions nor searched fastest text search algorithms coding methods tagged huffman tagged dense code dense code mark codeword boundaries achieving self synchronization property enables fast search random access exchange loss compression effectiveness paper performing simple reordering target symbols compressed text precisely reorganizing bytes wavelet treelike shape using little additional space searching capabilities greatly improved drastic impact compression decompression times approach codes achieve synchronism searched fast accessed arbitrary moreover reordered compressed text implicitly indexed representation text searched words time independent text length achieve fast sequential search time indexed search time extra space cost experiment word based compression techniques characteristics plain huffman tagged dense code restricted prefix byte codes searching capabilities achieved reordering compressed representation corpora reordered versions efficient classical counterparts efficient explicit inverted indexes built collection using amount space

recent studies found weak relationship performance retrieval system success achievable human searchers hypothesize searchers successful precisely alter behavior explore causal relation system performance search behavior control system performance hoping elicit adaptive search behaviors 36 subjects completed 12 searches using standard system degraded systems using linear model isolate main effect system performance measuring removing main effects due searcher variation topic difficulty position search time series searchers using degraded systems successful using standard system achieving success alter behavior measured real time suitably instrumented system findings suggest aspects behavioral dynamics provide unobtrusive indicators system performance

social service web 2.0 folksonomy provides users ability save organize bookmarks online social annotations tags social annotations quality descriptors web pages topics indicators web users propose personalized search framework utilize folksonomy personalized search specifically properties folksonomy namely categorization keyword structure property explored framework rank web page decided term matching query web page's content topic matching user's web page's topics evaluation propose automatic evaluation framework based folksonomy data able help lighten common cost personalized search evaluations series experiments conducted using heterogeneous data sets crawled del.icio.us dogear extensive experimental results personalized search approach significantly improve search quality

previous personalized search algorithms results queries personalized manner paper lot variation queries benefits achieved personalization queries issues query looking queries people results express examine variability user intent using explicit relevance judgments scale log analysis user behavior patterns variation user behavior correlated variation explicit relevance judgments query factors result entropy result quality task affect variation behavior characterize queries using variety features query results returned query people's interaction history query using features build predictive models identify queries benefit personalization

exploiting information induced query specific clustering top retrieved documents proposed means improving precision top ranks returned results novel language model approach ranking query specific clusters presumed percentage relevant documents contain previous cluster ranking approaches focus cluster model exploits information induced documents associated cluster model substantially outperforms previous approaches identifying clusters containing relevant document percentage furthermore using model produce document ranking yields precision top ranks performance consistently initial ranking clustering performed performance favorably compares art pseudo feedback retrieval method

traditional text clustering methods based bag words bow representation based frequency statistics set documents bow ignores information semantic relationships key terms overcome methods proposed enrich text representation external resource past wordnet approaches suffer limitations 1 wordnet limited coverage lack effective word sense disambiguation ability 2 text representation enrichment strategies append replace document terms hypernym synonym overly simple paper overcome deficiencies propose build concept thesaurus based semantic relations synonym hypernym associative relation extracted wikipedia develop unified framework leverage semantic relations enhance traditional content similarity measure text clustering experimental results reuters ohsumed datasets help wikipedia thesaurus clustering performance method improved compared previous methods addition optimized weights hypernym synonym associative concepts tuned help labeled data users provided clustering performance improved

ir clustering directly cluster documents document space using cosine similarity documents similarity measure real world applications usually knowledge word wish transform knowledge document concept paper provide mechanism knowledge transformation knowledge model type knowledge transformation model nonnegative matrix factorization model fsg sup sup word document semantic matrix posterior probability word belonging word cluster represents knowledge word space posterior probability document belonging document cluster represents knowledge document space scaled matrix factor provides condensed view knowledge words improve document clustering i.e knowledge word space transformed document space perform extensive experiments validate approach

paper proposes learning approach merging process multilingual information retrieval mlir conduct learning approach features influence mlir merging process features mainly extracted levels query document translation feature extraction frank ranking algorithm construct merge model knowledge practice attempt learning based ranking algorithm construct merge model mlir merging experiments test collections task crosslingual information retrieval clir ntcir3 4 5 employed assess performance proposed method moreover merging methods carried comparison including traditional merging methods 2 step merging strategy merging method based logistic regression experimental results method significantly improve merging quality types datasets addition effectiveness merge model generated frank method identify key factors influence merging process information provide insight understanding mlir merging

paper explores topic aspect i.e subtopic facet classification english chinese collections evaluation model assumes bilingual user found documents topic identified passages language aspects topic additional passages automatically labeled using nearest neighbor classifier local i.e result set latent semantic analysis experiments training examples available language classification using training examples languages achieve effectiveness using training examples language total training examples held constant classification effectiveness correlates positively fraction language training examples training set results suggest supervised classification benefit hand annotating language examples performing classification bilingual collections useful label examples language

address geocoding process finding map location structured postal address relatively studied paper consider crosslingual location search queries limited postal addresses language script search query underlying data stored knowledge system crosslingual location search system able geocode complex addresses statistical machine transliteration system convert location names script query stored data sufficient simply feed resulting transliterations monolingual geocoding system ambiguity inherent conversion drastically expands location search space significantly lowers quality results strength approach lies integrated nature abstraction fuzzy search text domain achieve maximum coverage despite transliteration ambiguities applying spatial constraints geographic domain focus viable interpretations query experiments structured unstructured queries set diverse languages scripts arabic english hindi japanese searching locations regions world crosslingual location search accuracy levels comparable commercial monolingual systems achieve levels performance using techniques applied crosslingual searches language script arbitrary spatial data

negative relevance feedback special relevance feedback positive example happens topic difficult search results poor principle standard relevance feedback technique applied negative relevance feedback perform due lack positive examples paper conduct systematic study methods negative relevance feedback compare set representative negative feedback methods covering vector space models language models special heuristics negative feedback evaluating negative feedback methods requires test set sufficient difficult topics naturally difficult topics existing test collections sampling strategies adapt test collection easy topics evaluate negative feedback experiment results trec collections language model based negative feedback methods effective based vector space models using multiple negative models effective heuristic negative feedback results feasible adapt test collections easy topics evaluating negative feedback methods sampling

relevance feedback traditionally terms relevant documents enrich user's initial query effective method improving retrieval performance traditional relevance feedback algorithms lead overfitting limited amount training data term space paper introduces online bayesian logistic regression algorithm incorporate relevance feedback information approach addresses overfitting projecting original feature space onto compact set retains information set features consist original retrieval score distance relevant documents distance relevant documents reduce human evaluation effort ascertaining relevance introduce active learning algorithm based variance reduction actively select documents user evaluation active learning algorithm aims select feedback documents reduce model variance variance reduction approach leads capturing relevance diversity uncertainty unlabeled documents principled manner critical factors active learning indicated previous literature experiments trec datasets demonstrate effectiveness proposed approach

typical pseudo relevance feedback methods assume top retrieved documents relevant pseudo relevant documents expand terms initial retrieval set contain deal noise paper cluster based resampling method select pseudo relevant documents based relevance model main idea document clusters dominant documents initial retrieval set repeatedly feed documents emphasize core topics query experimental results scale web trec collections significant improvements relevance model justification resampling approach examine relevance density feedback documents relevance density result retrieval accuracy ultimately approaching true relevance feedback resampling approach relevance density baseline relevance model collections resulting retrieval accuracy pseudo relevance feedback result indicates proposed method effective pseudo relevance feedback

pseudo relevance feedback assumes frequent terms pseudo feedback documents useful retrieval study re examine assumption hold reality expansion terms identified traditional approaches indeed unrelated query harmful retrieval expansion terms distinguished bad ones merely distributions feedback documents collection propose integrate term classification process predict usefulness expansion terms multiple additional features integrated process experiments trec collections retrieval effectiveness improved term classification addition demonstrate terms identified directly according impact retrieval effectiveness i.e using supervised learning instead unsupervised learning

ranking algorithms goal appropriately set objects documents component information retrieval systems previous ranking algorithms focused labeled data available training i.e supervised learning paper consider question unlabeled test data exploited improve ranking performance framework transductive learning ranking functions answer affirmative framework based generating features test data via kernelpca incorporating features via boosting learning ranking functions adapted individual test queries evaluate method letor trec ohsumed dataset demonstrate significant improvements

paper address issue learning rank document retrieval using thurstonian models based sparse gaussian processes thurstonian models represent document query probability distribution score space distributions scores naturally rise distributions document rankings observed rankings train model instead document training set judged particular relevance level example bad fair excellent performance model evaluated using information retrieval ir metrics normalised discounted cumulative gain ndcg recently taylor et al method called softrank allows direct gradient optimisation smoothed version ndcg using thurstonian model approach document scores represented outputs neural network score distributions created artificially adding random noise scores softrank mechanism applied ir metrics underlying models paper extend softrank framework score uncertainties naturally provided gaussian process gp probabilistic linear regression model develop model using sparse gaussian process techniques improved performance efficiency competitive results baseline methods tested publicly available letor ohsumed data set explore available uncertainty information prediction affects model performance

applications results form ranked lists information retrieval applications documents sorted according relevance query led information retrieval community methods automatically learn effective ranking functions paper propose novel method uncovers patterns rules training data associating features document relevance query discovered rules rank documents address typical inherent utilization association rules missing rules rule explosion proposed method generates rules demand driven basis query time result extremely fast effective ranking method conducted systematic evaluation proposed method using letor benchmark collections generating rules demand driven basis boost ranking performance providing gains ranging 12 123 outperforming art methods learn rank time consuming laborious pre processing highlight additional information query terms generated rules discriminative improving ranking performance

designing effective ranking functions core information retrieval web search ranking functions directly impact relevance search results focus research intersection web search machine learning learning ranking functions preference data particular recently attracted objective paper empirically examine objective functions learning ranking functions preference data specifically investigate roles ties learning process ties mean preference judgments documents equal degree relevance respect query type data ignored properly modeled past paper analyze properties ties develop novel learning frameworks combine ties preference data using statistical paired comparison models improve performance learned ranking functions resulting optimization explicitly incorporating ties preference data solved using gradient boosting methods experimental studies conducted using publicly available data sets demonstrate effectiveness proposed methods

sentence ranking issue concern document summarization researchers mutual reinforcement principle sentence term simultaneous key phrase salient sentence extraction generic single document summarization extend mutual reinforcement chain mrc text granularities i.e document sentence terms aim provide reinforcement framework formal mathematical modeling mrc step incorporate query influence mrc cope query oriented multi document summarization previous summarization approaches calculate similarity regardless query develop query sensitive similarity measure affinity pair texts evaluated duc 2005 dataset experimental results suggest proposed query sensitive mrc qs mrc promising approach summarization

comments left readers web documents contain valuable information utilized information retrieval tasks including document search visualization summarization paper study comments oriented document summarization aim summarize web document e.g blog post considering content comments left readers identify relations namely topic quotation mention comments linked model relations graphs importance comment scored graph based method graphs merged multi relation graph ii tensor based method graphs construct 3rd tensor generate comments oriented summary extract sentences web document using feature biased approach uniform document approach former scores sentences bias keywords derived comments latter scores sentences uniformly comments experiments using set blog posts manually labeled sentences proposed summarization methods utilizing comments significant improvement using comments methods using feature biased sentence extraction approach observed outperform using uniform document approach

markov random walk model recently exploited multi document summarization link relationships sentences document set assumption sentences indistinguishable document set usually covers topic themes theme represented cluster sentences topic themes usually equally sentences theme cluster deemed salient sentences trivial theme cluster paper proposes cluster based conditional markov random walk model clustercmrw cluster based hits model clusterhits leverage cluster level information experimental results duc2001 duc2002 datasets demonstrate effectiveness proposed summarization models results demonstrate clustercmrw model robust clusterhits model respect cluster

multi document summarization aims create compressed summary retaining main characteristics original set documents approaches statistics machine learning techniques extract sentences documents paper propose multi document summarization framework based sentence level semantic analysis symmetric negative matrix factorization calculate sentence sentence similarities using semantic analysis construct similarity matrix symmetric matrix factorization shown equivalent normalized spectral clustering sentences clusters finally informative sentences selected form summary experimental results duc2005 duc2006 data sets demonstrate improvement proposed framework implemented existing summarization systems study factors benefit performance conducted

describe approach information retrieval algorithmic mediation intentional synchronous collaborative exploratory search using system users common information search simultaneously collaborative system provides tools user interfaces importantly algorithmically mediated retrieval focus enhance augment team's search communication activities collaborative search outperformed post hoc merging similarly instrumented single user runs algorithmic mediation improved collaborative search allowing team searchers relevant information efficiently effectively exploratory search allowing searchers relevant information found individually

information filtering referred publish subscribe complements time searching users able subscribe information sources notified whenever documents published approximate information filtering selected information sources publish documents relevant user future monitored achieve functionality subscriber exploits statistical metadata identify promising publishers index continuous query publishers statistics maintained directory usually keyword basis disregarding correlations keywords using coarse information poor publisher selection lead poor filtering performance loss documents sup 1 sup based observation extends query routing techniques domain distributed information retrieval peer peer p2p networks provides algorithms exploiting correlation keywords filtering setting develop evaluate algorithms based single key multi key statistics utilize synopses hash sketches kmv synopses compactly represent publishers experimental evaluation using real life corpora web blog data demonstrates filtering effectiveness approaches highlights tradeoffs

search engine click logs provide invaluable source relevance information information biased ignore documents result list users actually seen clicked otherwise estimate document relevance simple counting paper propose set assumptions user browsing behavior allows estimation probability document seen thereby providing unbiased estimate document relevance train test compare model alternatives described literature gather set real data proceed extensive cross validation experiment solution outperforms significantly previous models effect gain insight browsing behavior users compare conclusions eye tracking experiments joachims et al 12 particular findings confirm user document directly clicked document explain documents situated relevant document clicked

click graphs improving query intent classifiers critical vertical search purpose search services offered unified user interface previous query classification primarily focused improving feature representation queries e.g augmenting queries search engine results investigate completely orthogonal approach instead enriching feature representation aim drastically increasing amounts training data semi supervised learning click graphs specifically infer class memberships unlabeled queries labeled ones according proximities click graph moreover regularize learning click graphs content based classification avoid propagating erroneous labels demonstrate effectiveness algorithms applications product intent job intent classification expand training data automatically labeled queries magnitude leading significant improvements classification performance additional finding amount training data obtained fashion classifiers using query words phrases features remarkably

blog feed search poses challenges traditional ad hoc document retrieval units retrieval blogs collections documents blog posts adapt art federated search model feed retrieval task significant improvement algorithms based performing submissions trec 2007 blog distillation task 12 typical query expansion techniques pseudo relevance feedback using blog corpus provide significant performance improvement dramatically hurt performance perform depth analysis behavior pseudo relevance feedback task develop novel query expansion technique using link structure wikipedia query expansion technique provides significant consistent performance improvements task yielding 22 14 improvement map unexpanded query baseline federated algorithms respectively

study paper bridging semantic gap low level image features level semantic concepts key hindrance content based image retrieval piloted rich textual information web images proposed framework tries learn distance measure visual space retrieve semantically relevant images unseen query image framework differentiates traditional distance metric learning methods following 1 ranking based distance metric learning method proposed image retrieval optimizing leave retrieval performance training data 2 scalable millions images rich textual information crawled web learn similarity measure learning framework particularly considers indexing ensure retrieval efficiency 3 alleviate noises unbalanced labels images utilize textual information latent dirichlet allocation based topic level text model introduced define pairwise semantic similarity images learnt distance measure directly applied applications content based image retrieval search based image annotation experimental results applications million web image database effectiveness efficiency proposed framework

recent efforts task spoken document retrieval sdr speech lattices speech lattices contain information alternative speech transcription hypotheses 1 transcripts information improve retrieval accuracy overcoming recognition errors 1 transcription paper look using lattices query example spoken document retrieval task retrieving documents speech corpus queries themselves form complete spoken documents query exemplars extend previously proposed method sdr short queries query example task specifically retrieval method based statistical modeling compute expected word counts document query lattices estimate statistical models counts compute relevance scores divergences models experimental results speech corpus conversational english statistics lattices documents query exemplars results retrieval accuracy using 1 transcripts documents queries addition investigate effect stop word removal improves retrieval accuracy knowledge lattice based approach query example spoken document retrieval

address specific enterprise document search scenario information expressed elaborate manner scenario information expressed using short query keywords examples key reference pages setup investigate examples utilized improve performance document retrieval task approach based language modeling framework query model modified resemble example pages compare methods sampling expansion terms example pages support query dependent query independent query expansion latter motivated wish increase aspect recall attempts uncover aspects information captured query evaluation purposes csiro data set created trec 2007 enterprise track performance achieved query models based query independent sampling expansion terms example documents

paper addresses issue query refinement involves reformulating ill formed search queries enhance relevance search results query refinement typically includes tasks spelling error correction word splitting word merging phrase segmentation word stemming acronym expansion previous research tasks addressed separately employing generative models paper proposes employing unified discriminative model query refinement specifically proposes conditional random field crf model suitable referred conditional random field query refinement crf qr sequence query words crf qr predicts sequence refined query words corresponding refinement operations sense crf qr differs greatly conventional crf models types crf qr models namely basic model extended model introduced merit employing crf qr refinement tasks performed simultaneously accuracy refinement enhanced furthermore advantages discriminative models generative models leveraged experimental results demonstrate crf qr significantly outperform baseline methods furthermore crf qr web search significant improvement relevance obtained

examine effect incorporating gaze based attention feedback user personalizing search process employing eye tracking data track document user read information subdocument level implicit feedback query expansion reranking evaluated variants incorporating gaze data subdocument level compared baseline based context document level results considering reading behavior feedback yields powerful improvements search result accuracy ca 32 extent improvements varies depending internal structure viewed documents type current information

user feedback considered critical element information seeking process especially relation relevance assessment current feedback techniques determine content relevance respect cognitive situational levels interaction occurs user retrieval system apart real life information objects users interact intentions motivations feelings seen critical aspects cognition decision study paper serves starting exploration role emotions information seeking process results latter interweave physiological psychological cognitive processes form distinctive patterns according specific task according specific user

primary business model web search based textual advertising contextually relevant ads displayed alongside search results address selecting ads relevant queries profitable search engine optimizing ad relevance revenue equivalent selecting ads satisfy constraints naturally incurs computational costs time constraints lead reduced relevance profitability propose novel stage approach conducts analysis ahead time offine preprocessing phase leverages additional knowledge impractical real time rewrites frequent queries subsequently facilitates fast accurate online matching empirical evaluation method optimized relevance matches art method improving expected revenue optimizing revenue substantial improvements expected revenue

opinion retrieval task growing social life academic research relevant opinionate documents according user's query key issues combine document's opinionate score ranking score extent subjective objective topic relevance score current solutions document ranking opinion retrieval ad hoc linear combination short theoretical foundation careful analysis paper focus lexicon based opinion retrieval novel generation model unifies topic relevance opinion generation quadratic combination proposed paper model relevance based ranking serves weighting factor lexicon based sentiment ranking function essentially popular heuristic linear combination approaches effect sentiment dictionaries discussed experimental results trec blog datasets significant effectiveness proposed unified model improvements 28.1 40.3 obtained terms map 10 respectively conclusion limited blog environment besides unified generation model contribution demonstrates opinion retrieval task bayesian approach combining multiple ranking functions superior using linear combination applicable result re ranking applications similar scenario

approach using passage level evidence document retrieval shown mixed results applied variety test beds characteristics main reason inconsistent performance exists unified framework model evidence individual passages document paper proposes probabilistic models formally model evidence set top ranked passages document probabilistic model follows retrieval criterion document relevant passage document relevant models passage independently probabilistic model goes step incorporates similarity correlations passages models trained discriminative manner furthermore combination approach combine ranked lists document retrieval passage based retrieval extensive set experiments conducted trec test beds effectiveness proposed discriminative probabilistic models passage based retrieval proposed algorithms compared art document retrieval algorithm language model approach passage based retrieval furthermore combined approach shown provide results document retrieval passage based retrieval approaches

classical probabilistic models attempt capture ad hoc information retrieval rigorous probabilistic framework recognized primary obstacle effective performance probabilistic models estimate relevance model dirichlet compound multinomial dcm distribution relies hierarchical bayesian modeling techniques polya urn scheme appropriate generative model traditional multinomial distribution text documents explore probabilistic model based dcm distribution enables efficient retrieval accurate ranking dcm distribution captures dependency repetitive word occurrences probabilistic model able model concavity score function effectively avoid empirical tuning retrieval parameters design parameter estimation algorithms automatically set model parameters additionally propose pseudo relevance feedback algorithm based latent mixture modeling dirichlet compound multinomial distribution improve retrieval accuracy finally experiments baseline probabilistic retrieval algorithm based dcm distribution corresponding pseudo relevance feedback algorithm outperform existing language modeling systems trec retrieval tasks

interpretations tf idf based binary independence retrieval poisson information theory language modelling paper contributes review existing interpretations tf idf systematically related probabilities approaches explored space independent space disjoint terms independent terms extreme query query term assumption uncovers tf idf analogy probabilistic odds mirrors relevance feedback disjoint terms relationship probability theory tf idf established integral 1 dx log study uncovers components divergence randomness pivoted document length inherent document query independence dqi measure interestingly integral dqi term occurrence probability leads tf idf

web pages people variety contexts contexts sufficiently distinct page's importance represented multiple domains authority indiscriminately mixes reputations determine domains authority examining contexts page cited determine separate domains authority model additionally determines local flow authority based relative similarity source target authority domains differentiate incoming outgoing hyperlinks topicality importance treating indiscriminately approach compares favorably topical ranking methods real world datasets produces approximately 10 improvement precision quality top ten results pagerank

paper proposes method computing page importance referred browserank conventional approach compute page importance exploit link graph web build model based graph instance pagerank algorithm employs discrete time markov process model unfortunately link graph incomplete inaccurate respect data determining page importance links easily added deleted web content creators paper propose computing page importance using user browsing graph created user behavior data graph vertices represent pages directed edges represent transitions pages users web browsing history furthermore lengths staying time spent pages users included user browsing graph reliable link graph inferring page importance paper proposes using continuous time markov process user browsing graph model computing stationary probability distribution process page importance efficient algorithm computation devised leverage hundreds millions users implicit voting page importance experimental results browserank indeed outperforms baseline methods pagerank trustrank tasks

paper study web forum crawling web forum data source web applications forum crawling challenging task due complex site link structures login controls forum sites carefully selecting traversal path generic crawler usually downloads duplicate invalid pages forums wastes precious bandwidth limited storage space crawl forum data effectively efficiently paper propose automatic approach exploring appropriate traversal strategy direct crawling target forum detail traversal strategy consists identification skeleton links detection page flipping links skeleton links instruct crawler crawl valuable pages meanwhile avoid duplicate uninformative ones page flipping links tell crawler completely download discussion thread usually shown multiple pages web forums extensive experimental results forums encouraging performance approach following discovered traversal strategy forum crawler archive informative pages comparison previous related commercial generic crawler

online forums contain huge amount valuable user generated content paper address extracting question answer pairs forums question answer pairs extracted forums help question answering services e.g yahoo answers applications propose sequential patterns based classification method detect questions forum thread graph based propagation method detect answers questions thread experimental results techniques promising

retrieval question answer archive involves finding answers user's question contrast typical document retrieval retrieval model task exploit question similarity ranking associated answers paper propose retrieval model combines translation based language model question query likelihood approach answer proposed model incorporates word word translation probabilities learned exploiting sources information experiments proposed translation based language model question outperforms baseline methods significantly combining query likelihood language model answer substantial additional effectiveness improvements obtained

question answering communities naver yahoo answers emerged popular effective means information seeking web posting questions participants answer information seekers obtain specific answers questions users popular portals yahoo answers submitted millions questions received hundreds millions answers participants hours sometime days satisfactory answer posted paper introduce predicting information seeker satisfaction collaborative question answering communities attempt predict question author satisfied answers submitted community participants prediction model develop variety content structure community focused features task experimental results obtained largescale evaluation thousands real questions user ratings demonstrate feasibility modeling predicting asker satisfaction complement results thorough investigation interactions information seeking patterns question answering communities correlate information seeker satisfaction models predictions useful variety applications user intent inference answer ranking interface design query suggestion routing

current search engines perform verbose queries main issues processing queries identifying key concepts impact effectiveness paper develop evaluate technique query dependent corpus dependent corpus independent features automatic extraction key concepts verbose queries method achieves accuracy identification key concepts standard weighting methods inverse document frequency finally propose probabilistic model integrating weighted key concepts identified method query demonstrate integration significantly improves retrieval effectiveness set natural language description queries derived trec topics newswire web collections

papers examining ambiguity information retrieval paper class ambiguous word past research barely explored shown class ambiguous word types commonly queries lack test collections containing ambiguous queries highlighted method creating collections existing resources described tests using collection impact query ambiguity ir system shown conventional systems incapable dealing effectively queries current assumptions improve search effectiveness hold searching common query type

personalization web search results technique improving user satisfaction received notable attention research community past decade focuses modeling establishing profile user aid personalization takes query centric approach paper method efficient automatic identification class queries define localizable web search engine query log determine set relevant features conventional machine learning techniques classify queries experiments technique able identify localizable queries 94 accuracy

tags user generated labels entities existing research tag recommendation focuses improving accuracy automating process ignoring efficiency issue propose highly automated novel framework real time tag recommendation tagged training documents treated triplets words docs tags represented bipartite graphs partitioned clusters spectral recursive embedding sre tags topical cluster ranked novel ranking algorithm poisson mixture model pmm proposed model document distribution mixture components cluster aggregate words word clusters simultaneously document classified mixture model based posterior probabilities tags recommended according ranks experiments scale tagging datasets scientific documents citeulike web pages del.icio.us indicate framework capable tag recommendation efficiently effectively average tagging time testing document 1 88 test documents correctly labeled top nine tags suggested

online communities popular publishing searching content finding connecting users user generated content includes example personal blogs bookmarks digital photos items annotated rated users social tags derived user specific scores leveraged searching relevant content discovering subjectively items moreover relationships users consideration ranking search results intuition trust recommendations close friends casual acquaintances queries tag keyword combinations compute rank top results variety options complicate query processing pose efficiency challenges paper addresses issues developing incremental top algorithm dimensional expansions social expansion considers strength relations users semantic expansion considers relatedness tags algorithm based principles threshold algorithms folding friends related tags search space incremental demand manner excellent performance method demonstrated experimental evaluation real world datasets crawled deli.cio.us flickr librarything

paper look social tag prediction set objects set tags applied objects users predict tag applied particular object investigated question using largest crawls social bookmarking system del.icio.us gathered date urls del.icio.us predicted tags based page text anchor text surrounding hosts tags applied url found entropy based metric captures generality particular tag informs analysis tag predicted found tag based association rules produce precision predictions giving deeper understanding relationships tags results implications study tagging systems potential information retrieval tools design systems

query search results search engines information retrieval systems single query retrieves results simply list provide users poor overview nowadays ranking clustering query search results useful separate post processing techniques organize retrieved documents paper proposed spectral analysis method based content similarity networks integrate clustering ranking techniques improving literature search approach organizes search results categories intelligently simultaneously rank results category variety theoretical empirical studies demonstrated method performs real applications especially biomedical literature retrieval moreover free text information analyzed method i.e proposed approach applied various information systems web search engines literature search service

improve precision top ranks document list response query researchers suggested exploit information induced clustering documents highly ranked initial search propose novel model ranking query specific clusters presumed percentage relevant documents contain model based proposing palette witness cluster properties purportedly correlate percentage ii devising concrete quantitative measures properties iii clusters via aggregation rankings induced individual measures empirical evaluation model consistently effective previously suggested methods detecting clusters containing relevant document percentage furthermore precision top ranks performance model transcends standard document based retrieval competes art document based retrieval approach

growing utilizing link information enhancing document clustering comparative evaluation impacts link types document clustering various types links text documents including explicit links citation links hyperlinks implicit links co authorship links pseudo links content similarity links convey topic similarity topic transferring patterns useful document clustering study adopt relaxation labeling rl based clustering algorithm employs content linkage information evaluate effectiveness aforementioned types links document clustering eight datasets experimental results linkage effective improving content based document clustering furthermore series findings regarding impacts link types document clustering discovered experiments

motivated political scientists manually analyze web archives news sites spotsigs algorithm extracting matching signatures near duplicate detection web crawls spot signatures designed favor natural language portions web pages advertisements navigational bars contributions spotsigs twofold 1 combining stopword antecedents short chains adjacent content terms create robust document signatures natural ability filter noisy components web pages otherwise distract pure gram based approaches shingling 2 provide exact efficient self tuning matching algorithm exploits novel combination collection partitioning inverted index pruning dimensional similarity search experiments confirm increase combined precision recall 24 percent art approaches shingling match factor 3 faster execution times locality sensitive hashing lsh demonstrative gold set manually assessed near duplicate news articles trec wt10g web collection

text reuse occurs types documents reasons form reuse duplicate near duplicate documents focus researchers importance web search local text reuse occurs sentences passages documents reused modified detecting type reuse basis tools text analysis paper introduce approach detecting local text reuse compare approaches comparison involves study amount type reuse occurs real documents including trec newswire blog collections

topic defined seminal event activity directly related events activities represented chronological sequence documents authors published internet paper define task called topic anatomy summarizes associates core topic graphically readers understand content easily proposed topic anatomy model called tscan derives major themes topic eigenvectors temporal block association matrix significant events themes summaries extracted examining constitution eigenvectors finally extracted events associated temporal closeness context similarity form evolution graph topic experiments based official tdt4 corpus demonstrate generated evolution graphs comprehensibly describe storylines topics moreover terms content coverage consistency produced summaries superior summarization methods based human composed reference summaries

field information retrieval faced computing correlation ranked lists commonly statistic quantifies correlation kendall's 932 times information retrieval community discrepancies items rankings items low rankings kendall's 932 statistic distinctions equally penalizes errors low rankings paper propose rank correlation coefficient ap correlation 932 ap based average precision probabilistic interpretation proposed statistic weight errors rankings nice mathematical properties easy interpret validate applicability statistic using experimental data

difficult apply machine learning domains lack labeled instances paper provide solution leverages domain knowledge form affinities input features classes example baseball vs hockey text classification labeled data presence word puck strong indicator hockey refer type domain knowledge labeled feature paper propose method training discriminative probabilistic models labeled features unlabeled instances unlike previous approaches labeled features create labeled pseudo instances labeled features directly constrain model's predictions unlabeled instances express soft constraints using generalized expectation ge criteria terms parameter estimation objective function express preferences values model expectation paper train multinomial logistic regression models using ge criteria method develop applicable discriminative probabilistic models complete objective function includes gaussian prior parameters encourages generalization spreading parameter weight unlabeled features experimental results text classification data sets method outperforms heuristic approaches training classifiers labeled features experiments human annotators beneficial spend limited annotation time labeling features labeling instances example minute labeling features achieve 80 accuracy ibm vs mac text classification using ge fl whereas ten minutes labeling documents results accuracy 77

consider scale retrieval evaluation recently methods based random sampling proposed solution extensive effort required judge tens thousands documents method proposed aslam et al 1 accurate efficient overly complex difficult community method proposed yilmaz et al infap 14 relatively simple efficient former employs uniform random sampling set complete judgments none methods provide confidence intervals estimated values contribution paper threefold 1 derive confidence intervals infap 2 extend infap incorporate nonrandom relevance judgments employing stratified random sampling hence combining efficiency stratification simplicity random sampling 3 describe approach utilized estimate ndcg incomplete judgments validate proposed methods using trec data demonstrate methods incorporate nonrandom samples available trec terabyte track 06

recent language models information retrieval shown smoothing language models crucial achieving retrieval performance effective smoothing methods proposed implement various heuristics exploit corpus structures paper propose unified optimization framework smoothing language models graph structures framework provides unified formulation existing smoothing heuristics serves road map systematically exploring smoothing methods language models follow road map derive instantiations framework instantiations lead novel smoothing methods empirical results instantiations effective outperforming art smoothing methods

classification algorithms categorizing web documents categories top levels directory project classification method detailed topic related class information user levels coarse classification scale hierarchy intractable target categories cross link relationships paper propose novel deep classification approach categorize web documents categories scale taxonomy approach consists stages search stage classification stage stage category search algorithm acquire category candidates document based category candidates prune scale hierarchy focus classification effort subset original hierarchy result classification model trained subset applied assign category document category candidates sufficiently close hierarchy statistical language model based classifier using gram features exploited furthermore structure taxonomy utilized stage improve performance classification demonstrate performance proposed algorithms directory project 130,000 categories experimental results proposed approach reach 51.8 measure mi f1 5th level 77.7 improvement top based svm classification algorithms

web applications blog classification sgroup classification labeled data short supply happens obtaining labeled data domain expensive time consuming plenty labeled data related domain traditional text classification ap proaches able cope learning domains paper propose novel cross domain text classification algorithm extends traditional probabilistic latent semantic analysis plsa algorithm integrate labeled unlabeled data related domains unified probabilistic model call model topic bridged plsa tplsa exploiting common topics domains transfer knowledge domains topic bridge help text classification target domain unique advantage method ability maximally mine knowledge transferred domains resulting superior performance compared art text classification approaches experimental eval uation kinds datasets proposed algorithm improve performance cross domain text classification significantly

paper propose greedy active learning method text categorization using squares support vector machines lssvm based transductive experimental design ted active learning formulation effectively explores information unlabeled data despite appealing properties optimization np hard active learning methods greedy sequential strategy select data example suggested suboptimum paper formulate continuous optimization prove convexity meaning set data examples selected guarantee global optimum develop iterative algorithm efficiently solve optimization easy implement text categorization experiments text corpora empirically demonstrated active learning algorithm outperforms sequential greedy algorithm promising active text categorization applications

accurate web page classification depends crucially information gained neighboring pages local web graph prior exploited class labels nearby pages improve performance contrast utilize weighted combination contents neighbors generate virtual document classification addition break pages fields finding weighted combination text target fields neighboring pages able reduce classification error third demonstrate performance dataset pages directory project validate approach using pages crawl stanford webbase interestingly value anchor text unexpected value page titles especially titles parent pages virtual document

information retrieval evaluation typically performed dozen queries judged near completeness deal recent evaluation judgment sets select set documents judge estimate evaluation measures judgments available light evaluate queries total judging effort million query track trec 2007 document selection algorithms acquire relevance judgments 1,800 queries results track deeper analysis investigating tradeoffs queries judgments evaluation queries fewer judgments cost effective reliable fewer queries judgments total assessor effort reduced 95 appreciable increase evaluation errors

evaluation measures act objective functions optimized information retrieval systems objective functions accurately reflect user requirements particularly tuning ir systems learning ranking functions ambiguity queries redundancy retrieved documents poorly reflected current evaluation measures paper framework evaluation systematically rewards novelty diversity develop framework specific evaluation measure based cumulative gain demonstrate feasibility approach using test collection based trec question answering track

investigate extent people relevance judgements reusable ir test collection exchangeable consider classes judge gold standard judges topic originators experts particular information seeking task silver standard judges task experts create topics bronze standard judges define topics experts task analysis low levels agreement relevance judgements report experiments determine sufficient invalidate test collection measuring system performance relevance assessments created silver standard bronze standard judges system scores system rankings subject consistent differences assessment sets appears test collections completely robust changes judge judges vary widely task topic expertise bronze standard judges able substitute topic task experts due changes relative performance assessed systems gold standard judges preferred

modeling beyond topical aspects relevance currently gaining popularity ir evaluation example discounted cumulated gain dcg measure implicitly models aspects relevance via diminishing value relevant documents seen retrieval e.g due information cumulated redundancy effort paper focus concept negative relevance nhor explicit via negative gain values ir evaluation extend computation dcg allow negative gain values perform experiment laboratory setting demonstrate characteristics nhor evaluation approach leads intuitively reasonable performance curves emphasizing user's view progression retrieval towards success failure discuss normalization issues positive negative gain values allowed conclude discussing usage nhor characterize test collections

paper investigates agreement relevance assessments official trec judgments generated interactive ir experiment results 63 documents judged relevant users matched official trec judgments factors contributed differences agreements retrieved relevant documents relevant documents judged system effectiveness topic ranking relevant documents

recent collecting user assessor preferences absolute judgments relevance evaluation learning ranking algorithms measures precision recall dcg defined absolute judgments evaluation preferences require evaluation measures explicitly model describe class measures compare absolute preference measures trec collection

retrieval experiments effectiveness metrics generate score system topic pair tested usual average system topic scores obtain system score purpose system comparison paper explore ramifications using geometric mean gmap arithmetic mean map computing aggregate system score set system topic scores gmap indeed handle variability topic difficulty consistently usual map aggregation method

consider question average precision measure retrieval effectiveness regarded deriving model user searching behaviour indeed regarded simple stochastic model user behaviour

noabstract

introduce explore concept individual's relevance threshold reconciling differences outcomes batch user experiments

information retrieval systems compared using evaluation metrics researchers commonly reporting results simple metrics precision 10 reciprocal rank complex ones average precision discounted cumulative gain paper demonstrate complex metrics simple metrics predicting performance simple metrics topics reporting results simple metrics alongside complex ones redundant

major component sense organizing labeling summarizing data hand form useful mental model precursor identifying missing information reasoning data previous shown scatter gather model useful exploratory activities occur users encounter unknown document collections topic structure communicated scatter gather closely tied behavior underlying clustering algorithm structure reflect mental model applicable information paper describe initial design mixed initiative information structuring tool leverages aspects studied scatter gather model permits user impose own desired structure

aim forum information retrieval evaluation fire create cranfield evaluation framework spirit trec clef ntcir indian language information retrieval six indian languages selected bengali hindi marathi punjabi tamil telugu poster describes tasks document topic collections fire workshop

findings log based study designed track adoption features real time query refinement interface deployed yahoo search engine trends months noted discussed

noabstract

traditional information retrieval models assume users express information via text queries i.e talk poster consider web browsing behavior outside interactions retrieval systems i.e users walk alternative source signal describing users information compare query expressed information dataset findings demonstrate information expressed behavior modalities overlapping past behavior modality accurate predictor future behavior modality results browsing data provides stronger source signal search queries due volume explains previous found implicit behavioral data valuable source information user modeling personalization

clickthrough search results successfully infer user preferences noisy potentially ambiguous explore potential complementary sensitive signal mouse movements providing insights intent web search query report preliminary results studying user mouse movements search result pages goal inferring user intent particular explore automatically distinguish query classes navigational vs informational preliminary exploration confirms value studying mouse movements user intent inference suggests avenues future exploration

generating query biased summaries response time interactive information retrieval iir systems paper proposes document titles alternative queries generation summaries document titles allows pre generate summaries statically improve response speed iir systems experiments suggest title biased summaries promising alternative query biased summaries

paper user profile enhanced detailed description products included main assumptions considered implies set features describe item organized defined set components categories user's rating item obtained combining user opinions relevance component

paper propose topical pagerank based algorithm recommender systems aim rank products analyzing previous user item relationships recommend top rank items potentially users evaluate algorithm movielens dataset empirical experiments demonstrate outperforms art recommending algorithms

personalized search promising serve users information search history major information sources search personalization investigated impact history length effectiveness personalized ranking carried task based user study web search obtained ranked relevance judgments queries query contexts derived previous queries task re rank results current query experimental results performance personalization improves queries accumulated benefits immediately preceding queries

question answering systems increasingly deal complex information require simple factoid answers evaluation systems usually carried using precision recall based system performance metrics previous demonstrated users shown search result lists reliably differentiate qualities lists investigate consistency user based approach system oriented metrics question answering environment initial results indicate methodologies level disagreement

aim investigate performance distributed information retrieval dir systems improved personalization aim building testbed document collections corresponding personalized relevance judgments paper discuss intended approach personalizing phases dir process describe test collection building discuss methodology evaluating personalized dir using relevance information social bookmarking data

search personalization pursued provide result rankings overall search experience individual users 5 blindly applying personalization user queries example background model derived user's term query click history appropriate aiding user accomplishing actual task user change time user sometimes categories tasks short timespan history based personalization impede user's desire discovering topics paper propose personalization framework selective twofold sense selectively employs personalization techniques queries expected benefit prior history information refraining undue actions otherwise introduce notion tasks representing granularity levels user profile ranging specific search goals broad topics base reasoning selectively query relevant user tasks considerations cast statistical language model tasks queries documents supporting judicious query expansion result re ranking effectiveness method demonstrated empirical user study

address task separating personal personal blogs report set baseline experiments compare performance set features set five classifiers limited set features performance 90 obtained

paper address relatively text categorization classify political blog liberal conservative based political leaning subjectivity analysis based method twofold 1 identify subjective sentences contain strong subjective clues based inquirer dictionary 2 subjective sentences identified extract opinion expressions features build political leaning classifiers experimental results political blog corpus built using features subjective sentences significantly improve classification performance addition extracting opinion expressions subjective sentences able reveal opinions characteristic specific political leaning extent

aim opinion finding system retrieve relevant documents retrieve documents express opinion towards query target entity propose integrate opinion identification toolkit opinionfinder retrieval process information retrieval ir system opinionated relevant documents retrieved response query experiments vary top ranked documents parsed response query investigate effect opinion retrieval performance required parsing time opinion finding retrieval performance improved integrating opinionfinder retrieval system retrieval performance grows posts parsed opinionfinder benefit eventually tails deep rank suggesting optimal setting system achieved

blog news search engines channels reach information real time happenings paper study popular queries collected period compare search results returned blog search engine i.e technorati news search engine i.e google news observed hits returned search engines set queries highly correlated suggesting blogs provide commentary current events reported news popular queries related events observed cohesiveness returned search results queries

proposed methods using clickthrough data common queries improve quality search results returned query study examine search behaviour users close knit community queries argue benefit using aggregated clickthrough data varies task task improve document rankings navigational specific informational queries value users issuing broad informational query

hamlet suite principles scoring models algorithms automatically propagate metadata edges document neighborhood showcase scenario consider tag prediction community based web 2.0 tagging applications experiments using real world data demonstrate viability approach scale environments tags scarce knowledge hamlet system promote efficient precise reuse shared metadata highly dynamic scale web 2.0 tagging systems

paper begin investigate automatically determine subjectivity orientation questions posted real users community question answering cqa portals subjective questions seek answers containing private personal opinion experience contrast objective questions request objective verifiable information support reliable sources knowing question orientation helpful evaluating answers provided users guiding cqa engine process questions intelligently experiments yahoo answers data method exhibits promising performance

question answering communities gaining popularity wonder increased popularity actually improves degrades user experience addition automatic qa systems utilize sources search engines social media emerging rapidly qa communities created abundant resources millions questions hundreds millions answers question continue serve effective source information web search question answering vital importance poster investigate temporal evolution popular qa community yahoo answers respect effectiveness answering basic types questions factoid opinion complex questions experiments yahoo answers growing rapidly overall quality information source factoid question answering degrades instead answering factoid questions effective answer opinion complex questions

collaborative tagging online social content systems naturally characterized synonyms causing low precision retrieval propose mechanism based user preference profiles identify synonyms retrieve relevant documents expanding user's query using popular online book catalog discuss effectiveness method usual similarity based expansion methods

booming development web popular chinese forums enable people experienced customers reviews products opinion product users plenty web pages time consuming inefficient consequently automatic review mining summarization hot research topic recently previous approaches applicable mining chinese customer reviews paper introduce soping chinese customer review mining system mines reviews forums specifically propose novel search based approach extract product features feature oriented sentence orientation determination method experimental results proposed techniques highly effective

propose novel scheme sentiment classification labeled examples combines strengths learn based lexicon based approaches follows lexicon based technique label portion informative examples task domain learn supervised classifier based labeled ones finally apply classifier task experimental results indicate proposed scheme dramatically outperform learn based lexicon based techniques

results 2006 ecml pkdd discovery challenge suggest semi supervised learning methods spam filtering source available labeled examples differs classified attempted reproduce results using data 2005 2007 trec spam track found opposite effect methods self training transductive support vector machines yield inferior classifiers constructed using supervised learning labeled data investigate differences ecml pkdd trec data sets methodologies account opposite results

opinion finding retrieval system tasked retrieving relevant documents express opinion towards query target entity opinion finding systems based stage approach initially system aims retrieve relevant documents re ranked according extent detected opinionated nature investigate underlying baseline retrieval system performance affects overall opinion finding performance apply effective opinion finding techniques baseline runs submitted trec 2007 blog track draw insights conclusions

paper describes method automatically acquire query translation pairs mining web click data extraction requires crawling chinese words segmentation capture popular translations experimental results real click data 17.4 extracted queries dictionary method achieve 62.2 top 1 80.0 top 5 precision translating web queries moreover extracted translations semantically relevant source query particularly useful cross lingual information retrieval clir

introduce statistical model abbreviation disambiguation web search based analysis web data resources including anchor text click log query log combining evidence multiple sources able accurately disambiguate abbreviation queries experiments real web search queries promising results

address task blog feed distillation blogs principally devoted topic task viewed association finding task topics bloggers view resembles expert finding task range models proposed adopt language modeling based approaches expert finding determine effectiveness feed distillation strategies models capture idea human search key blogs spotting highly relevant posts posting model taking global aspects blog account blogger model results blogger model outperforms posting model delivers art performance box

previous scalability experiments found precision improves collection size increases assumption collection's documents sampled uniform probability population contrast breadth web crawl scenario real world web search documents characteristics documents

focused crawling critical technique topical resource discovery web propose frontier prioritizing algorithm namely otie line topical importance estimation algorithm efficiently effectively combines link based content based analysis evaluate priority uncrawled url frontier demonstrate otie's advantages traditional prioritizing algorithms real crawling experiments

paper proposes concept query free web search daily living ordinarily benefit additional information daily activities currently engaged washing coffee maker example receive benefit obtain information cleaning coffee maker vinegar removes stain proposed method automatically searches web page including information relates activity daily living activity performed assume wireless sensor nodes attached daily objects detect object method makes query names objects method retrieves web page relates activity daily living using query

document prior features pagerank url depth improve retrieval effectiveness web information retrieval ir systems queries equally benefit application document prior feature paper aims investigate retrieval performance enhanced selecting document prior feature query basis novel method selecting document prior feature query basis evaluate technique trec gov web test collection associated trec 2003 web search tasks experiments demonstrate effectiveness robustness proposed selection method

paper explore parsimonious language models web retrieval models efficient standard language models suited scale web retrieval conducted experiments trec topic sets found parsimonious language model results improvement retrieval effectiveness standard language model data sets measures improvement significant substantial earlier experiments newspaper newswire data

poster paper propose novel approach improve web search relevancy tokenizing vietnamese query text prior submitting search engine evaluations demonstrate effectiveness practical value

prevalence recording devices ease media sharing consumers embracing huge amounts internet videos arise effective video advertisement systems following phenomenal success text propose novel advertising system adimage automatically associates relevant ads matching characteristic images referred adimages analogous adwords proposed image matching method invariant distortions commonly observed shared videos adimage avoids pitfalls poor tagging qualities shared videos provides brand venue specify ad targets image objects moreover formulate image matching scores parameterized bidding information nonlinear optimization maximizing system revenues user perception

bag visual words bow popular visual classification recent paper propose novel bow expansion method alleviate effect visual word correlation achieve diffusing weights visual words bow based visual word relatedness rigorously defined visual ontology proposed method tested video indexing experiment trecvid 2006 video retrieval benchmark improvement 7 traditional bow reported

paper reports word shape coding method facilitate retrieval camera based document images ocr due perspective distortion reported word shape coding methods fail camera based images paper addressed approximating perspective transformation affine transformation employing affine invariant namely length ratio represent connected components components document image classified clusters assigned representative symbol retrieval based words comprising symbols experiment results proposed method achieved average retrieval precision 93.43 recall 94.22

user generated spoken audio remains challenge automatic speech recognition asr technology content based audio surrogates derived asr transcripts error robust investigation term clouds surrogates podcasts demonstrates asr term clouds closely approximate term clouds derived human generated transcripts range cloud sizes user study confirms conclusion asr clouds viable surrogates depicting content podcasts

rapid increase online video services video retrieval systems becoming increasingly search tools users fields poster novel video retrieval interface supports creation multiple search facets aid users carrying complex multi faceted search tasks interface allows multiple searches executed viewed simultaneously allows material reorganized facets experiment compares faceted interface tabbed interface similar modern web browsers preliminary results

novel web image semantic analysis wisa system explores adaptively modeling distributions semantic labels web image surrounding text deal employ piecewise penalty weighted regression model learn weights contributions surrounding text semantic labels images experimental results real web image data set improve performance web image semantic annotation significantly

poster overview characteristics button information retrieval interface closed captions tv watching activities intended lighten burden remembering entering query terms watching tv investigated interface experimental system named video bookmarking search estimates query terms closed captions named entity recognition sentence labeling techniques according empirical evaluation 1,138 search queries 206 bookmarks using seven actual tv city life travel health cuisine found wider queries search results acceptable query input free interface despite queries search results directly relevant users original intentions main reason watching user's wider expressed query terms

introduce grocery retrieval system maps shopping lists written natural language actual products grocery store developed system using nine months shopping basket data finnish supermarket evaluate system 70 real shopping lists gathered customers supermarket system achieves 80 precision products rank precision 70 products rank 5

paper propose reranking model improve aspect level performance biomedical domain model iteratively computes maximum hidden aspect retrieved passage reranks passages aspect subsets experimental results improvements aspect level performance 27.14 2006 genomics topics 27.09 2007 genomics topics

research articles typically introduce results findings relate knowledge entities immediate relevance body context knowledge related results explicitly mentioned article overcome limitation art information retrieval approaches rely latent semantic analysis terms articles projected lower dimensional latent space matches space identified approach perform explicit knowledge entities articles compared amount knowledge domain address exploiting domain knowledge layer rich network relations knowledge entities domain extracted corpus documents knowledge layer supplies context knowledge relate knowledge entities hence improve information retrieval performance develop study framework learning aggregating relations knowledge layer literature corpus ii exploiting relations improve information retrieval relevant documents

kleio advanced information retrieval ir system developed uk national centre text mining nactem sup 1 sup system offers textual metadata searches medline provides enhanced searching functionality leveraging terminology management technologies

keyword based retrieval matches search terms documents via term co occurrence approach allow matching based specific plant characteristic descriptions botanical text retrieval study applies information extraction techniques automatically extract plant characteristic information text allows users search using information combination keywords evaluation experiment conducted using actual users results indicate approach enhances task based retrieval performance

domain expertise influence people search poster findings log based study medical domain experts search web information related expertise compared experts differences sites visited query vocabulary search behavior findings implications automatic identification domain experts interaction logs domain knowledge applications query suggestion page recommendation support experts

japanese common word written especially true katakana words typically transliterating foreign languages ambiguity critical automatic processing information retrieval ir tackle propose simple effective approach generating katakana variants considering phonemic representation original language word proposed approach evaluated assessment variants generates impact generated variants ir studied comparison existing approach using katakana rewriting rules

propose expert finding method based assumption sequential dependence candidate expert query terms scope document assume strength relation candidate document's content depends position document respect positions query terms experiments official enterprise trec data demonstrate advantage method method based independence query terms persons document

introduce novel approach expert finding based multi step relevance propagation documents related candidates relevance propagation modeled absorbing random walk evaluation official enterprise trec data sets demonstrates advantage method art method based step propagation

paper discuss progress towards scalable hierarchical classification system books using library congress subject hierarchy examine characteristics domain challenging look appropriate performance measurements hieron hierarchical support vector machines perform moderately

bug locating usually involves intensive search activities incurs unpredictable cost labor time issue information retrieval bug locations particularly addressed facilitate identifying bugs software code paper novel bug retrieval approach co location shrinkage cs proposed proposed approach implemented source software projects collected real world repositories consistently improves retrieval accuracy art support vector machine svm model

automatic summarization jbig2 coded textual images discussed compressed images partially decompressed compute relevant features feature extraction method free using character recognition module summary sentences ranked experiment considers documents indic scripts lack efficient ocr systems script independent aspect approach highlighted popular indic scripts sentence selection efficiency 61 achieved judged summarization nonparametric distribution free rank statistic correlation coefficient 0.33 measure minimum strength associations sentence ranking machine human

method projecting retrieval scores corpora shared parallel corpus

demonstrate regularization improve feedback language modeling framework

theoretical bounds empirical robustness score regularization changes similarity measure

analyse query length fit power law poisson distributions query sets provide practical model query length based truncation poisson distribution short queries power law distribution queries fits real query length distributions earlier proposals

choice indexing terms represent documents crucially determines ective subsequent retrieval ir systems commonly rule based stemmers normalize surface word forms combat finding documents contain words related query terms inflectional derivational morphology stemmers available languages paper explore effectiveness unsupervised morphological segmentation alternative stemming using test sets thirteen european languages unsupervised segmentation significantly unnormalized words 20 rule based stemming available low complexity languages compare methods character grams finding average grams yield performance

noabstract

describe method applying parsimonious language models re estimate term probabilities assigned relevance models apply method six topic sets test collections five genres parsimonious relevance models improve retrieval effectiveness terms map collections ii significantly outperform parsimonious counterparts measures iii precision enhancing effect unlike blind relevance feedback methods

analyzing data attracted lot attention recently due intrinsic rich structures real world datasets paratucker model proposed combine axis capabilities parafac model structural generality tucker model algorithms developed fitting paratucker model paper propose tanpt algorithm solve paratucker model apply algorithm temporal relation co clustering author topic evolution experiments dblp datasets demonstrate effectiveness

language modeling approaches effectively dealing dependency query terms based gram bigram trigram models bigram language models suffer adjacency sparseness means dependent terms adjacent documents sometimes distance sentences document resolve adjacency sparseness paper proposes type bigram language model explicitly incorporating proximity feature adjacent terms query experimental results test collections proposed bigram language model significantly improves previous bigram model tao's approach art method proximity based method

notion concept relatedness attracted considerable attention variety approaches based ontology structure information content association context proposed indicate relatedness abstract ideas propose method based cross entropy reduction language models concepts estimated based document concept assignments approach improved competitive results compared art methods test sets biomedical domain

pseudo feedback based automatic query expansion yields effective retrieval performance average results performance inferior using original query information address cause robustness issue namely query drift fusing results retrieved response original query expanded form approach posts performance significantly retrieval based original query robust retrieval using expanded query

paper targets enhancing latent semantic indexing lsi exploiting category labels specifically term document matrix vector term appearing labels semantically close labels scaled performing singular value decomposition svd boost impact generated left singular vectors result similarities documents category increased furthermore adaptive scaling strategy designed utilize hierarchical structure categories experimental results proposed approach able significantly improve performance hierarchical text categorization

paper describe fixed threshold sequential minimal optimization fsmo joint constraint learning algorithm structural classification svm fsmo joint constraint formulation structural svm 0 fsmo breaks quadratic programming qp structural svm series qp involving variable using variable fsmo advantageous qp sub subset selection

introduce posterior probabilistic clustering ppc provides rigorous posterior probability interpretation nonnegative matrix factorization nmf removes uncertainty clustering assignment furthermore ppc closely related probabilistic latent semantic indexing plsi

passages hidden text circumvent disallowed transfer release compartmentalized information concern corporate governmental organization explore methodology detect hidden passages document document divided passages using various document splitting techniques text classifier categorize passages novel document splitting technique called dynamic windowing significantly improves precision recall f1 measure

explore alternative information retrieval paradigm called query multiple examples qbme information described set terms set documents intuitive ideas qbme include using centroid documents rocchio algorithm construct query vector consider perspective text classification query vector obtained learning support vector machines svms online queries svms learned class examples linear time offline queries svms learned positive unlabeled examples linear polynomial time effectiveness efficiency proposed approaches confirmed experiments real world datasets

question classification plays role question answering systems paper exploit semantic features support vector machines svms question classification propose semantic tree kernel incorporate semantic similarity information diverse set semantic features evaluated experimental results svms semantic features especially semantic classes significantly outperform art systems

algorithms distributed information retrieval rely accurate knowledge size collection multiple capture recapture method shokouhi et al reliable algorithms determining collection size relies samples uniform documents uniform samples hard obtain system simple generalisation multiple capture recapture rely uniform sample sizes simulations accurate original method sample sizes vary considerably useful technique real tools

investigate representative sudden information change web users analyzing search engine query logs majority queries submitted users browsing documents news domain related recently browsed document investigate identifying query candidate contextualization conditioned recently browsed document user build successful classifier task achieves 96 precision 90 recall

combine techniques xml mining text mining benefit information retrieval manipulating word sequence according xml structure marked text strengthen phrase boundaries obvious algorithms extract multiword sequences text consequently quality indexed phrases improves positive effect average precision measured inex 2007 standards

proximity aware scoring functions lead significant effectiveness improvements text retrieval xml ir sometimes enhance retrieval quality exploiting knowledge document structure combined established text ir methods paper introduces modified proximity scores document structure account demonstrates effect inex benchmark

traditional document retrieval shown competitive approach xml element retrieval counter intuitive element retrieval task requests relevant document retrieved paper conducts comparative analysis document element retrieval highlights relative strengths weaknesses approaches explains relative effectiveness document retrieval approaches element retrieval tasks

flexible xml selection language flexpath allows formulation flexible constraints structure content xml documents experimental results obtained preliminary prototype described idea promises results

study entity ranking inex entity track propose simple graph based ranking approach enables combine scores document paragraph level combined approach improves retrieval results inex testset similarly trec's expert finding task

novel passage based approach re ranking documents initially retrieved list improve precision top ranks passage based document retrieval ranks document based query similarity constituent passages approach leverages information centrality document passages respect initial document list passage centrality induced bipartite document passage graph wherein edge weights represent document passage similarities empirical evaluation approach yields effective re ranking performance furthermore performance superior previously proposed passage based document ranking methods

previous term dependency account semantic information underlying query phrases study impact utilizing phrase based concepts term dependency wikipedia separate term dependencies treat accordingly features linear feature based retrieval model compare method markov random field mrf model trec document collections experimental results utilizing phrase based concepts improves retrieval effectiveness term dependency reduces size feature set extent

paper technique ranking types categories query trying category query query categorization approach seeks types related query results necessarily query category falls ranking types approach complementary

previously 2 postulated advantage using entity extraction implement peer peer p2p search framework reducing network traffic providing trade precision recall propose entity ranking method designed short documents characteristic p2p significantly improves precision recall top results p2p search construct dynamic entity corpus using grams statistics metadata study reliability identify correlations user query terms

web search engines typically index retrieve page level study investigate dynamic pruning strategy allows query processor determine promising websites proceed similarity computations pages sites

paper describe preliminary using monolingual projections document collections performing cross language information retrieval tasks proposed methodology multidimensional scaling projecting vector space representations multilingual document collection spaces lower dimensionality independent projection computed language structural similarities resulting projections exploited information retrieval tasks

consider approximating pagerank target node using local information provided link server prove local approximation pagerank feasible graph low degree admits fast pagerank convergence natural graphs web graph abundant degree nodes local pagerank approximation costly reverse natural graphs tend low indegree maintaining fast pagerank convergence follows calculating reverse pagerank locally frequently feasible computing pagerank locally finally demonstrate usefulness reverse pagerank five applications

world wide web document repositories act valuable sources additional data various machine learning tasks paper propose method improving text classification accuracy using additional corpus easily obtained web additional corpus unlabeled independent classification task method proposed topic modeling extract set topics additional corpus extracted topics act additional features data classification task evaluation rcv1 dataset significant improvement baseline method

novel efficient learning algorithm proposed binary linear classification algorithm trained using rocchio's relevance feedback technique builds classifier intermediate hyperplane common tangent hyperplanes category complement experimental results encouraging justify research

hypergraph partitioning considered promising method address challenges dimensionality document clustering documents modeled vertices relationship documents captured hyperedges goal graph partitioning minimize edge cut definition hyperedges vital clustering performance definitions hyperedges proposed systematic understanding desired characteristics hyperedges missing paper provide unified clique perspective definition hyperedges serves guide define hyperedges perspective based concepts hypercliques shared reverse nearest neighbors propose types clique hyperedges analyze properties regarding purity size issues finally extensive evaluation using real world document datasets experimental results shared reverse nearest neighbor based hyperedges clustering performance improved significantly terms various external validation measures fine tuning parameters

clustering hypertext document collection task information retrieval clustering methods based document content account hyper text links propose novel pagerank based clustering prc algorithm hypertext structure prc algorithm produces graph partitioning modularity coverage comparison prc algorithm content based clustering algorithms match prc clustering content based clustering

noabstract

paper novel hybrid recommender system called relationalcf integrate content demographic information collaborative filtering framework using relational distance computation approaches effort form transformation feature construction experiments suggest effective combination various kinds information based relational distance approaches provides improved accurate recommendations approaches

noabstract

mobile terminals cell phones restricted terms input output functionality special techniques incorporated enable easily web searching searching location name related dazzling variety topics relate factors yield search system map text information search results clusters helpful users especially mobile environment system makes mobile web searching easier efficient

noabstract

noabstract

paper system cross lingual information retrieval handle tens languages millions documents functioning system demonstrated corpus european legislation 22 languages 400,000 documents language system interactive web interface advantage predefined thesaurus allowing user dynamically re rank retrieval results based mapping onto predefined thesaurus

online communities popular publishing searching content connecting users user generated content includes example personal blogs bookmarks digital photos items annotated rated users users connect usually friends share common demonstrate social recommendation system takes advantages users connections tagging behavior compute recommendations items communities advantages verified via comparison standard ir technique

noabstract

demonstration graphical geospatial query specifications obtain sets georeferenced data ranked probability relevance displayed geographically temporally geospatial browser temporal support

demonstration xml ir system allows users feedback granularities types using dempster shafer theory evidence compute expanded reweighted queries

noabstract

researchers studying developing teaching materials information retrieval ir 3 toolkits built provide hands experience students example ir toolbox 4 effort close gap students understanding ir concepts real life indexing search systems tools helping students technical library information science field develop conceptual model search engines cover emerging topics skills content based image retrieval cbir fusion search source software http www.searchtools.com tools tools opensource.html teach basic advanced ir topics require student level technical knowledge spend time gain practical understanding topics rapid approach teach basic advanced ir topics text retrieval web based ir cbir fusion search computer science cs graduate students designed projects help students grasp abovementioned ir topics students teams practical application start superimposed application image description retrieval 5 saidr earlier sierra allows users associate images multimedia information text annotations users retrieve information 2 1 perform text based retrieval annotations 2 perform cbir images images look query image query image team build enhancement application involving text retrieval cbir weeks time sub projects described table 1 outcome activity students learned ir concepts able relate applicability real world figure 1 details projects found http collab.dlib.vt.edu runwiki wiki.pl tabletpcimageretrievalsuperimposedinformation demonstrate tools developed ir concepts illustrate table 1 believe tools aid learn basic advanced topics ir

advent xml standard representation exchange structured documents growing amount xml documents stored peer peer p2p networks cur 172 rent research p2p search engines proposes informa 172 tion retrieval ir techniques perform content based search account structural features documents p2p systems typically central index avoiding single failures distribute information participating peers accordingly querying peer limited access index information select carefully peers help answering query contributing resources local index information cpu time ranking computations bandwidth consumption major issue guarantee scalability p2p systems reduce peers involved retrieval process result retrieval quality terms recall precision suffer substantially proposed thesis document structure considered extra source information improve retrieval quality xml documents p2p environment thesis centres following questions structural information help improve retrieval xml documents terms result quality precision recall specificity xml structure support routing queries distributed environments especially selection promising peers xml ir techniques p2p network minimizing bandwidth consumption considering performance aspects answer questions analyze achievements search engine proposed exploits structural hints expressed explicitly user implicitly self describing structure xml documents additionally focused specific results obtained providing ranked retrieval units xml documents relevant passages theses documents xml information retrieval techniques applied select peers participating retrieval process compute relevance documents indexing approach includes content structural information documents support efficient execution multi term queries index keys consist rare combinations content structure tuples performance increased using fixedsized posting lists frequent index keys combined iteratively combination rare posting list size pre set threshold posting lists sorted taking account classical ir measures term frequency inverted term frequency weights potential retrieval units document slight bias towards documents peers collections regarding current index key peer characteristics online times available bandwidth latency extracting posting list specific query re posting list performed takes account structural similarity key query according preranking peers selected expected hold information potentially relevant documents retrieval units final ranking computed parallel selected peers computation based extension vector space model distinguishes weights structures content allows weighting xml elements respect discriminative power e.g title weighted footnote additionally relevance computed mixture content relevance structural similarity query potential retrieval unit currently prototype p2p information retrieval xml documents called spirix implemented experiments evaluate proposed techniques structural hints performed distributed version inex wikipedia collection

user feedback considered critical element information seeking process aspect feedback cycle relevance assessment progressively popular practice web searching activities interactive information retrieval ir value relevance assessment lies disambiguation user's information achieved applying various feedback techniques techniques vary explicit implicit help determine relevance retrieved documents former type feedback usually obtained explicit intended indication documents relevant positive feedback irrelevant negative feedback explicit feedback robust method improving system's overall retrieval performance producing query reformulations 1 expense users cognitive resources hand implicit feedback techniques tend collect information search behavior intelligent unobtrusive manner doing disengage users cognitive burden document rating relevance judgments information seeking activities reading time saving printing selecting referencing treated indicators relevance despite lack sufficient evidence support effectiveness 2 besides apparent differences categories feedback techniques determine document relevance respect cognitive situational levels interactive dialogue occurs user retrieval system 5 approach account dynamic interplay adaptation takes dialogue levels importantly consider affective dimension interaction users interact intentions motivations feelings apart real life information objects critical aspects cognition decision 3 4 evaluating users affective response towards information object e.g document prior post exposure accurate understanding object's properties degree relevance current information facilitated furthermore systems detect respond accordingly user emotions potentially improve naturalness human computer interaction progressively optimize retrieval strategy current study investigates role emotions information seeking process latter communicated multi modal interaction reconsiders relevance feedback respect occurs affective level interaction

natural language processing techniques believed hold tremendous potential supplement purely quantitative methods text information retrieval led emergence nlp based ir research projects empirical evidence support inadequate contributions nlp ir mainly concentrate document representation compound term matching strategies researchers noted simple term based representation document content vector representation usually inadequate accurate discrimination bag words representation invoke linguistic considerations allow modelling relationships subsets words variety content indicator syntactic phrase tried investigated representing documents single terms ir systems matching strategy representation beyond traditional statistical techniques measure term co occurrence characteristics proximity analyzing text structure paper propose novel ir strategy sir nlp techniques involved syntactic level sir documents query representation built basis syntactic data structure natural language text dependency tree syntactic relationships words identified structured form tree capture syntactic relations words hierarchical structural representation matching strategy sir upgrades traditional statistical techniques introducing similarity measure method executing graph representation level key determiner basic ir experiment designed implemented trec data evaluate novel ir model feasible experimental results indicate approach potential outperform standard bag words ir model especially response syntactical structured queries

noabstract

desktop search tools provide powerful query capabilities result presentation techniques user context account propose exploit collected information user activities desktop files applications activity based desktop search prepare project review type search box name colleague expect deliverable draft email paper review joint conference presentation ideally desktop search system able infer current task logs previous activities task specific search results

noabstract

noabstract

noabstract

noabstract

advances digital capture storage technologies mean capture store one's entire life experiences personal digital archives vast personal archives human digital memories hdms pose challenges opportunities research community developing effective means retrieval hdms personal archive retrieval research infancy scope novel research phd proposes develop effective hdm retrieval algorithms combining rich sources context associated items location people data information obtained linking hdm items novel

users searching information digital library www modeled individuals moving semantic space issuing queries clicking hyperlinks emit stream interaction data linguistic data lots captured logs guess user searching information retrieval systems user interaction stateless space timeline connecting systems seldom sequence data systematically characterize meaningful relations sequence user activity pragmatics semantics user clicked particular link added particular term query meaning primarily relation preceding actions remaining challenge ir extract features user interaction data meaning relations meanwhile user's perspective time semantic space path exploration user exact terms query specific words surrounding hypertext link trajectory terms establish relation user's path identifying meaningful relations queries page views sequence activity increases understanding users information formally model query browsing behaviors surface forms hidden process missing layer abstraction mapping sequences interaction descriptive users useful automation describe effort identify features data logs query browsing activity highly predictive types behavior sequences interaction data individual users modeled sequences expression statistical modeling techniques effective modeling sequences natural language processing bioinformatics examined ability model sequences interaction information searcher information retrieval system queries click throughs stream interaction tagged features semantic coordinates timing frequency type action analyzing collections interaction sequences identify frequent patterns user behavior patterns predictions future interactions example patterns link following digital library highly predictive users steps patterns models user interaction useful design evaluation search interfaces individual models user interaction useful personalized search customized content little research investigate features optimal modeling user queries browsing interaction sequences step identify informative features relationships features propose construct models user behavior based user data logs query browsing activity identify features highly predictive types user behaviors examine activity search sessions digital library microcosm larger systems expect features useful predictive models user behavior individual aggregate level hope identify meaningful relationships features implications beyond scope digital libraries larger systems broader search domains

