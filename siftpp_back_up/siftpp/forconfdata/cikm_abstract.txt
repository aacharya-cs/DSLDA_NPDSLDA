document centric xml mixture text structure increased availability document centric xml content comes query facilities structural constraints constraints content documents expressed expressiveness languages querying xml documents help users express information address question experimental theoretical view experimental analysis compares structure ignorant structure aware retrieval approach using test suite 2004 edition inex xml retrieval evaluation initiative theoretically create mathematical models users knowledge set documents define query languages exactly fit models languages corresponds xml version fielded search inex query language main findings structure varying degrees complexity half queries expressed fielded search format hierarchical structure documents structure search hint strict requirement judged underlying information third structure queries functions precision enhancing device

unified database framework enable comprehension ranked xml retrieval challenge xml database field propose logical algebra named score region algebra enables transparent specification information retrieval ir models xml databases transparency achieved possibility instantiate various retrieval models using abstract score functions algebra operators logical query plan operator definitions remain unchanged algebra operators model aspects xml retrieval element relevance score computation element score propagation element score combination illustrate usefulness algebra instantiate ir scoring models combine score propagation combination functions implemented algebra operators prototype system top low level database kernel evaluation system performed collection ieee articles xml format provided inex argue art xml ir models transparently implemented using score region algebra framework top low level physical database engine existing rdbms allowing systematic investigation retrieval model behavior

re weighting method called contextualization efficient element ranking xml retrieval introduced re weighting based idea using ancestors element context element appears context interpreted probability relevance weight increased relevance scoring element appears bad context weight decreased formal presentation contextualization xml representation manipulation frame based utilization structural indices provides approach independent weighting schemas query languages.contextualization evaluated inex test collection tested runs contextualization parent root tower contextualizations contextualization runs significantly contextualization root contextualization re weighted runs

business processes executing peer peer environments usually invoke web services independent peers peer peer environments inherently lack global control business processes nevertheless require global transactional guarantees i.e atomicity isolation applied level processes paper introduces decentralized serialization graph testing protocol ensure concurrency control recovery peer peer environments uniqueness proposed protocol ensures global correctness relying global serialization graph essentially transactional process equipped partial knowledge allows transactional processes coordinate globally correct execution achieved communication dependent transactional processes peers accessed failures combination partial backward forward recovery applied experimental results exhibit significant performance gain traditional distributed locking based protocols respect execution transactions encompassing web service requests

peer peer p2p data integration systems recently attracted significant attention ability manage share data dispersed peer sources integrating data answering user queries happens inconsistencies arise integrity constraints specified peers global schemas violated semantics inconsistent system suitably repairing retrieved data typically context traditional data integration systems specific features p2p systems peer autonomy peer preferences e.g source trusting properly addressed approach effective paper issues marginally considered literature formal framework reasoning autonomous peers exploit individual preference criteria repairing data idea queries answered database repairs respect preferences peers i.e able agreement investigate computational complexity dealing peer agreements answering queries p2p data integration systems considering peer preferences makes mildly harder traditional data integration systems

aim fold contribution address issue supporting efficiently queries string attributes involving prefix suffix containment equality operators scale data networks design decision employ distributed hash tables dhts data network's topology harnessing desirable properties design decision derive dht independent solutions treating dht black box exploit infrastructure develop efficient content based publish subscribe systems main contribution algorithms efficient processing queries subscriptions events publications specifically subscription processing algorithms require logn messages node network event processing algorithms require logn messages average string length third develop algorithms optimizing processing multi dimensional events involving string attributes analysis provide simulation based experiments promising performance results terms messages required bandwidth load balancing response times

feature information systems ability inform users changes stored information systems changes user informed field publish subscribe architectures paper propose solution information system designers extend information model notification mechanism consider semantic knowledge determining parties inform kinds implementations introduced evaluated based aspect oriented programming aop based traditional database triggers evaluation approaches leads combined approach preserving advantages techniques using model driven architecture mda create triggers uml model enhanced stereotypes

data mining techniques frequently patterns rules difficult human analyst interpret results truly actionable rules due subjective nature interestingness human involvement analysis process crucial paper propose novel visual data mining framework purpose identifying actionable knowledge quickly easily discovered rules data framework called opportunity map inspired ideas quality engineering particular quality function deployment qfd house quality associates summarized data discovered rules application objective using interactive matrix enables user quickly identify opportunities proposed system visually analyze discovered rules statistical properties data user interactively actionable attributes values affect targets combined drill comparative analysis user analyze rules data levels detail proposed visualization framework represents systematic flexible method rule analysis applications system scale data sets industrial partner yielded promising results

paper value mapping algorithm rely syntactic similarity semantic interpretation values algorithm constructs statistical model e.g co occurrence frequency entropy vector captures unique characteristics values co occurrence matching values computing distances models refining models using user feedback iterations experimental results suggest approach successfully establishes value mappings presence opaque data values useful addition existing data integration techniques

address task answering natural language questions using frequently questions faq pages available web task involves steps 1 fetching faq pages web 2 automatic extraction question answer pairs collected pages 3 answering users questions retrieving appropriate pairs discuss solutions tasks detailed evaluation results collected corpus 3.6gb text data 293k pages 2.8m pairs real users questions sampled web search engine log specifically propose simple effective methods extraction investigate task specific retrieval models answering questions model answers 36 test questions top 20 results overall conclusion faq pages web provide excellent resource addressing real users information highly focused manner

recently significant increase community based question answer services web people answer peoples questions services rapidly build archives questions answers archives valuable linguistic resource major tasks question answer service questions archive semantically similar user's question enables quality answers archive retrieved removes time lag associated community based system paper discuss methods question retrieval based using similarity answers archive estimate probabilities translation based retrieval model model semantically similar questions relatively little word overlap

paper stepping stones pathways ssp alternative model building answers queries document collections answered ranked list stepping stones handle questions relation topics ssp addresses contents set related documents answer single document query splitting required satisfactorily explore document space query results networks document representing topics relating connecting documents network network answers user's information devise effective representations techniques visualize answers involve users answer finding process verify validity approach questions aim answer involve multiple topics performed study involving custom built broad collection operating systems research papers evaluated results computer science students using multiple measures

web based third party architectures data publishing receiving growing attention due scalability ability efficiently manage users amounts data third party architecture relies distinction owner publisher information owner producer information whereas publisher provides data management services query processing functions portion owner's information architecture security concerns especially assumption trustworthy publishers approaches proposed 4 5 providing partial solutions comprehensive framework developed able support security properties presence untrusted publisher paper develop xml based solution makes conventional digital signature techniques queries encrypted data

emergence xml de facto standard exchange disseminate information regulating access xml documents attracted considerable attention recent existing models attach authorizations nodes xml document disregard relationships ancestor sibling relationships reveal information sensitive carried nodes themselves e.g classification paper advocates integration relationships class citizen access control models xml makes following contributions characterizes relationship authorizations identifies mechanisms required translate accurately authorized view source document introduces rule based formulation expressing classes relationship authorizations defines associated conflict resolution strategy xml access control model proposed approach allows seamless integration relationship authorizations existing xml access control model

xml documents frequently applications business transactions medical records involving sensitive information typically documents visible users depending roles instance insurance agent billing information medical document details patient's medical history access control basis data location value xml document essential practice access control rules millions product document types 1000 user roles 100 solution requires scalability performance current approaches access control xml documents suffered scalability tend individual documents paper propose novel approach xml access control rule functions managed separately documents rule function executable code fragment encapsulates access rules paths predicates shared documents document type runtime rule functions corresponding access request executed determine accessibility document fragments using synthetic real data scalability scheme comparing accessibility evaluation cost rule function models rule functions generated user basis efficient xml databases

study suitable indexing techniques support efficient exact match search biological sequence databases propose suffix tree st representation called sta df alternative array representation st sta proposed 7 utilized 18 study performance sta sta df develop memory efficient st based exact match stem search algorithm implemented stem representations st conducted extensive experiments results indicate sta sta df representations similar construction time storage utilization search time using stem terms access patterns stem results compared sta sta df representation exhibits spatial sequential locality reference suggests sta df require disk os hence amenable efficient scalable disk based computation

data representations distance measures organizational structures fast efficient retrieval similar shapes image databases using hough transform extract shape signatures correspond features image shape descriptor robust line discontinuities takes consideration shape boundaries content inside object perimeter object signatures eventually projected space renders invariant translation scaling rotation provide support real time query content introduce index structure hierarchically organizes compressed versions extracted object signatures manner achieve significant performance boost multimedia retrieval experiments suggest exploiting proposed framework similarity search database 100,000 images require 1 sec using shelf personal computer

consider tracking moving objects sensor networks specific application consider tracking chemical plume moving infrastructure network distributed index structure dist stores updates distributed summaries plume moves algorithms range queries history plume dist localizes information respect time space using hierarchy scales plume size highlight analytical model predict cost query algorithms based query location query size plume's spatio temporal distribution using model adaptive scheme chooses optimal scheme experimental results dist outperforms alternative techniques query update storage costs scales plumes

subject specific search facilities health sites usually built using manual inclusion exclusion rules expensive maintain provide incomplete coverage web resources hand health information obtained web search scientifically based potentially harmful.to address cost coverage quality built focused crawler mental health topic depression able selectively fetch quality relevant information found relevance unfetched pages predicted based link anchor context quality estimated quality entire linking page using learned ir style query weighted single words word pairs predict quality links overall crawler priority determined product link relevance source quality.we evaluated crawler baseline crawls using relevance judgments objective site quality scores obtained using evidence based rating scale relevance focused crawler quality focused crawler retrieved twice relevant pages breadth control quality focused crawler effective reducing amount low quality material fetched crawling quality content relative relevance focused crawler.analysis suggests quality content improved post filtering breadth crawl cost substantially increased network traffic

commercial research location based web search i.e finding web content topic related particular region type search location information indexed text information index conventional text search engine set oriented location information dimensional euclidean space brings research efficiently represent location attributes web pages combine types indexes paper propose hybrid index structure integrates inverted files trees handle textual location aware queries combining schemes studied 1 inverted file tree double index 2 inverted file tree 3 tree inverted file validate performance proposed index structures design implement complete location based web search engine mainly consists 1 extractor detects geographical scopes web pages represents geographical scopes multiple mbrs based geographical coordinates 2 indexer builds hybrid index structures integrate text location information 3 ranker ranks results geographical relevance geographical relevance 4 interface friendly users input location based search queries obtain geographical textual relevant results experiments real world web dataset third structures superior query time slightly third additionally indexes based trees proven efficient indexes based grid structures

finding information people web using search engine difficult mapping person names specific persons i.e referents paper describes person resolution system called webhawk list pages obtained submitting person query search engine webhawk facilitates person search steps filter removes pages contain information person secondly cluster remaining pages clusters specific person resulting clusters meaningful extractor induce query oriented personal information page finally namer generates informative description cluster users specific person easily architecture webhawk components discussed detail separate evaluation component appropriate user study webhawk complements existing search engines successfully improves users experience person search web

adaptive load shedding approach windowed stream joins contrast conventional approach dropping tuples input streams explore concept selective processing load shedding allow stream tuples stored windows shed excessive cpu load performing join operations entire set tuples windows dynamically changing subset tuples learned highly beneficial support dynamic selective processing forms runtime adaptations adaptation input stream rates adaptation time correlation streams adaptation join directions indexes speed execution stream joins experiments conducted evaluate adaptive load shedding terms output rate results selective processing approach load shedding effective significantly outperforms approach drops tuples input streams

time relevant multi dimensional data sets mds users usually pose huge amount data due dimensionality approximating query processing emerged viable solution specifically cube streams handle mdss continuous manner traditional cube approximation focuses generating single snapshots continuous ones address issue application generating snapshots cube streams called scs investigated paper application collects data events cube streams line generates snapshots limited resources approximated information synopsis memory analysis compared olap applications scs ones subject resource constraints processing time memory dealt existing methods due limited resources paper dawa algorithm standing hybrid algorithm dct data discrete wavelet transform proposed approximate cube streams dawa algorithm combines advantage compression rate dwt low memory cost dct consequently dawa costs buffer outperforms dwt based dct based methods execution efficiency shown dawa provides answers quality scs applications buffer short execution time optimality algorithm dawa theoretically proved empirically demonstrated experiments

sensor networks autonomous devices able collect store process share data devices spatiotemporal region queries retrieving information networks queries require answers subset network nodes fall query region network redundant sense measurements nodes substituted nodes degree confidence subset nodes sufficient answer query lower energy cost investigate advantage data redundancy propose techniques process spatiotemporal region queries conditions techniques reduce twenty times energy cost query processing compared typical network flooding prolonging lifetime sensor network

common approaches multi label classification learn independent classifiers category employ ranking thresholding schemes classification exploit dependencies labels techniques suited categories independent domains labels highly interdependent paper explores multi label conditional random field crf classification models directly parameterize label co occurrences multi label classification experiments models outperform single label counterparts standard text corpora multi labels sparse models improve subset classification error 40

paper introduces algorithm clustering data dimensional feature spaces called gardenhd algorithm organized notion data space reduction i.e process detecting dense dense cells space performs effective efficient elimination empty characterize typical dimensional spaces efficient adjacency connected agglomeration dense cells larger clusters produces compact representation effectively capture essence data gardenhd hybrid cell based density based clustering unlike typical clustering methods class applies recursive partition sparse regions space using space partitioning strategy properties partitioning strategy greatly facilitate data space reduction experiments synthetic real data sets reveal gardenhd data space reduction effective efficient scalable

paper propose versatile disambiguation approach explicit meaning structure based information xml schemas xml document structures web directories ontologies support semantic awareness wide range applications schema matching query rewriting peer data management systems xml data clustering ontology based automatic annotation web pages query expansion effectiveness achieved results experimentally proved founded flexible exploitation structure context extraction tailored specific application information provided commonly available thesauri wordnet

noabstract

existing xml query pattern based caching strategies focus extracting set frequently issued query pattern trees based occurrences query pattern trees history occurrence query pattern tree considered equally caching strategy query pattern tree occur timepoints history xml queries temporal feature improve caching strategy paper propose novel type query pattern called conserved query paths efficient caching integrating support temporal features conserved query paths paths query pattern trees change change significantly time terms support values specific time period proposed algorithm extract conserved query paths ranking conserved query paths dynamic conscious caching dcc strategy proposed efficient xml query processing experiments dcc caching strategy outperforms existing xml query pattern tree based caching strategies

noabstract

range labeling structural joins studied techniques efficiently processing xpath queries xpath queries times structural joins required solve developed method reduce joins nodes read disk using strong dataguides method process single paths joins twig patterns joins amongst branching nodes leaves queries experimental results verified approach outperforms optimization technique structural joins factors hundreds times

basic relationships determined xml query processing ancestor descendant parent child sibling relationships containment labeling scheme determine relationships fast expensive determining sibling relationship prefix labeling scheme determine basic relationships fast xml tree shallow xml tree deep prefix scheme inefficient prefix furthermore prefix label repeated siblings self labels siblings paper propose containment prefix schemes determine basic relationships faster matter xml structure meanwhile prefix reduce redundancies prefix labeling scheme

paper provides overview innovative applications cosine series xml joins data stream joins

noabstract

summarization text documents increasingly amount data available internet majority current approaches view documents linear sequences words create query independent summaries ignoring structure document degrades quality summaries furthermore popularity web search engines requires query specific summaries method create query specific summaries adding structure documents extracting associations fragments

framework functionally modeling query languages data models data queries uniformly represented functions query language constructs polymorphic functions functions typed database oriented type system supports polymorphism nesting types perform static type checking type inferencing query expressions query language freely extended introducing querying constructs polymorphic functions.while type information input output description functions semantic information captured equational specifications knowledge functions represented equalities functional expressions form equations equational axiomatization query language database query equivalence answering query views posed equational word equational matching

database outsourcing trend involves data owners farming data management external service provider requirement maintain integrity authenticity outsourced data whenever outsourced database queried corresponding query reply demonstrably authentic furthermore reply include proof completeness convince querier data matching query predicate omitted paper suggest techniques support efficient authenticity completeness guarantees query replies

study answer aggregation queries hierarchical web sites using adaptive sampling

noabstract

infer query language allows users express queries referencing relations specifying joins infer syntax similar restrictive sql users easily write highly expressive queries automatically completed infer's inference engine infer's sql based syntax familiar current database users improved ranking query explanation system makes easier

location dependent data central emerging applications ranging traffic information services sensor networks standard pull push based data dissemination models unworkable data volumes clients high.we address using locale covers subset original set locations chosen include location suitably defined neighborhood client location dependent values highly correlated location query answered using location close query point.we location dependent queries answered satisfactorily using locale covers loss accuracy approach independent locations speeds clients applicable mobile clients

paper scheme incremental evaluation xpath queries focus monotone fragment xpath i.e data deleted inserted database deletion insertion resp occur query answers efficiently processing deletions store information partial matchings i.e elements participating matchings query answers store counters matchings query answer information partial matchings skipping computation data insertion investigate properties xpath fragment amount information store

current skyline computation arise due relation preference queries guaraneed skyline lose dimensions compared data set means skyline exists set weight assignments dimensions top user preference.we believe usefulness skyline limited application extended data analysis knowledge discovery skyline dimensional datasets common data analysis applications contain various means developed filter skyline dimensions paper propose algorithms set skyline called strong skyline extensive experiments proposal effective efficient

complementary interactive literature sets articles considered reveal useful information scientific apparent document sets swanson called existence knowledge undiscovered public knowledge udpk paper proposes semantic based mining model udpk method replaces manual ad hoc pruning using semantic knowledge biomedical ontologies using semantic types semantic relationships biomedical concepts prototype system identify relevant concepts collected medline generate novel hypothesis concepts system successfully replicates swanson's famous discoveries raynaud disease fish oils migraine magnesium compared previous approaches methods generate fewer relevant novel hypotheses require human intervention discovery procedure

able express enforce role based access control xml data critical component xml data management semi structured nature xml trivial access control applied values nodes structural relationship nodes context adopt extend graph editing language specifying role based access constraints form security views security annotated schema sas proposed internal representation security views automatically constructed original schema security view specification enforce access constraints user queries propose secure query rewrite sqr set rules rewrite user xpath query security view equivalent xquery expression original data guarantee users information view data blocked experimental evaluation demonstrates efficiency expressiveness approach

develop fixpoint operator computing item sets demonstrate query paradigm solutions association rule mining idea fixpoint computation indicates optimisation issues results research provide theoretical foundation relational computation association rules application xml mining

efficient algorithm finding maximal frequent word sequences set sentences word sequence considered frequent words occur 963 sentences words occur sentences frequency threshold 963 hence words sequence occur consecutively sentences

record deduplication task merging database records refer underlying entity relational data bases accurate deduplication records type dependent decisions records types whereas nearly previous approaches merged records types independently models inter dependencies explicitly collectively deduplicate records multiple types construct conditional random field model deduplication captures relational dependencies employ novel relational partitioning algorithm jointly deduplicate records citation matching datasets collectively deduplicating paper venue records results 30 error reduction venue deduplication 20 error reduction paper deduplication

paper propose novel energy efficient approach localized routing tree lrt coupled route redirection rr strategy support various types queries lrts care sensors near sink reduce energy consumption sensors rr reduces energy cost data receptions compared existing approaches simulation studies lrt rr significant improvement query capacity

latent semantic indexing lsi successfully applied information retrieval text classification lsi classification features classes ignored feature values solve propose latent semantic classification lsc model extends lsi model following classification information training documents introduced latent semantic structure via set latent variables indexing classification information account classification process experiments reuters model performs existing classification methods knn svm

investigate support ranked keyword search parallel search cluster network newly proposed peer peer network overlay particular study efficiently acquire distribute global information required ranked keyword search taking advantage architectural features pscns

noabstract

statistical relationship determination terms key issues automatic thesaurus construction systematically analyze existing relevant approaches based underlying probabilistic assumptions propose combined approach overcomes limitations

intelligence analysis aided guided models analysts priorities paper describes approach analyst modeling ant caf 201 project analyst models guide searching behavior swarm intelligent agents structural elements analyst model include concepts relations help capture analyst's current concerns addition concepts relationships associated scalar parameters provide quantitative measure user's level developed algorithms dynamically adapting weights evolving elements model evaluate algorithms built analyst modeling environment workbench tested approach workbench using traces generated human analysts demonstrated improvements current art search engines

depending web searcher's familiarity query's target topic appropriate introductory advanced documents trec hard 1 track defined topic familiarity meta data associated user's query instead define user independent query independent model topic familiarity required read document matched user response query introductory web page defined web page doesn't presuppose background knowledge topic extent introduces defines key terms topic advanced web page defined web page assumes sufficient background knowledge topic familiarity key technical terms topic potentially builds develop method biasing initial mix documents returned search engine increase documents desired familiarity level position 5 position 10 method involves building supervised text classifier incorporating features based reading level distribution stop words text text features average line length using familiarity classifier achieve statistically significant improvements reranking result set introductory documents ranked list classifier seamlessly integrated current search engine technology involving major modifications existing architectures

noabstract

community discovery studied extensively web environment limited research free text co occurrence words entities sentences documents usually implies connections paper investigate co occurrences named entities text mine communities entities identifying communities free text transformed graph clustering hierarchical clustering algorithm proposed experiment algorithm effective discover named entity communities text documents

keyphrases facilitate web users grasping main topic web page practical system automatic keyphrase extraction web pages system regression model trained based set human labeled documents extract keyphrases pages automatically paper makes contributions structure information web page investigated keyphrase extraction task query log data associated web page collected search engine server help keyphrase extraction third method forward paper evaluate similarity phrases

sb tree binary tree data structure proposed represent time series according importance data stock data management distinguished preserving critical data attribute values retrieving time series data according importance data facilitating multi resolution time series retrieval stock data available continuously effective updating mechanism sb tree paper study updating approaches reported families updating methods proposed periodic rebuild batch update update efficiency effectiveness characteristics compared reported

paper hybrid concept hierarchy development technique web returned documents retrieved meta search engine aim technique separate initial retrieved documents topical oriented categories prior actual concept hierarchy generation topical categories correspond semantic aspects query using 1 automatic document classification initial set returned documents individual topical concept hierarchy automatically generated inside resulted categories steps executed fly retrieval time due efficiency constraints imposed web retrieval context algorithm document snippets web pages document classification concept hierarchy generation experimental results algorithm able improve quality concept hierarchy searcher time efficiency parameters kept reasonable intervals

document keyphrases provide semantic metadata characterizing documents producing overview content document text mining knowledge management related applications paper describes keyphrase identification program kip extracts document keyphrases using prior positive samples human identified domain keyphrases assign weights candidate keyphrases logic algorithm keywords candidate keyphrase contains significant keywords candidate phrase keyphrase obtain prior positive inputs kip populates glossary database using manually identified keyphrases keywords checks composition noun phrases document looks database calculates scores noun phrases ones scores extracted keyphrases

time bound hierarchical key assignment assign time sensitive keys security classes partially hierarchy legal data accesses classes enforced time bound hierarchical key assignment schemes proposed literature proved insecure collusive attacks paper propose rsa based time bound hierarchical key assignment scheme describe application security analysis scheme safe collusive attacks

search engines process queries conjunctively restrict size answer set rare observe mismatch vocabulary text web pages terms compose web queries combination features lead irrelevant query results particularly specific queries composed terms deal propose technique automatically structuring web queries set subqueries select representative subqueries information distributions document collection adequately modeled using concept maximal termsets derived formalism association rules theory experimentation technique leads improved results trec 8 test collection instance technique led gains average precision roughly 28 regard bm25 ranking formula

paper principled method accurately extracting coherent relevant passages variable lengths using hmms appropriate parameter estimation hmm method outperforms strong baseline methods data sets

structural features xml components extra source information content oriented retrieval task type documents paper explore structural features inex collection 1 content oriented search analyse gain knowledge add performance information retrieval system approach structural information extracted relevance feedback process priors language modelling framework

paper propose text clustering algorithm named clustering based frequent word sequences cfws word sequence frequent occurs percentage documents text database past vector space model commonly information retrieval treats documents bags words ignoring sequential pattern word occurrences documents meaning natural languages strongly depends word sequences frequent word sequences provide compact valuable information text database bisecting means fihc algorithms evaluated performance text clustering compared proposed cfws algorithm shown cfws performance

schema matching finding correspondences mapping rules e.g logical formulae heterogeneous schemas paper probabilistic framework called splmap automatically learning schema mapping rules similar lsd techniques ir field combined.our approach able probabilistic interpretation prediction weights candidates select rule set matching probability

documents formatted extensible markup language xml becoming increasingly available collections various document types paper approach summarisation xml documents novelty approach lies based features content documents logical structure follow machine learning sentence extraction based summarisation technique features effective producing summaries approach views sentence extraction task evaluated summarisation model using inex dataset results demonstrate inclusion features logical structure documents increases effectiveness summariser learnable system effective suited task summarisation context xml documents

noabstract

words usually assumed semantically independent existing similarity measures true practice semantic relatedness words conveniently employed existing measures propose novel similarity measure based earth mover's distance emd proposed measure semantic distances words computed based electronic lexical database wordnet emd employed calculate document similarity matching words experiments results demonstrate effectiveness proposed similarity measure

propose web page transformation method browsing mobile devices displays approach original web page fit screen transformed set pages fits screen transformation slicing original page resulting set transformed pages form multi level tree structure called slicing tree internal node consists thumbnail image hyperlinks leaf node block original web page slicing tree based web page transformation eases web browsing displays providing screen fitting visual context reducing page scrolling effort

paper evaluation evolved term weighting schemes short medium trec queries previously evolved global collection wide term weighting scheme evaluated unseen trec data shown increase mean average precision idf local document evolved term weighting scheme dependent performing global scheme evolved scheme i.e combined local global scheme compared bm25 scheme pivoted normalisation scheme.our results local evolved solution perform collections due document normalisation properties conclude okapi tf tuned interact effectively evolved global weighting scheme increase mean average precision standard bm25 scheme

investigate language models informational navigational web search retrieval web task differs substantially ordinary ad hoc retrieval perform analysis prior probability relevance wide range content features shedding light importance content features web retrieval directly explains success failure various techniques e.g link topology particularly helpful single sites language models naturally incorporate multiple document representations content information former employ mixture language models based document text incoming anchor text document titles latter study range priors based document length url structure link topology look types topics distillation home page named page mixed query set mixture models lead considerable improvement retrieval effectiveness topic types web centric priors lead improvement retrieval effectiveness

paper pooling method constructing assessment sets evaluation retrieval systems proposal based rankboost machine learning voting algorithm leads pools classical pooling reduces manual assessment workload building test collections experimental results obtained xml document collection demonstrate effectiveness approach according evaluation criteria

maintaining strict static score inverted lists heuristic search engines improve quality query results entire inverted lists processed heuristic increases cost index generation requires complex index build algorithms paper study index organization based static score bucketing technique significantly improves index build performance minimal impact quality search results

top preference queries multiple attributes critical decision applications previous research concentrated improving computational efficiency mainly using novel index structures search strategies current applications scale terabytes data thousands users performance systems strongly impacted amount available memory paper proposes scalable approach memory bounded top query processing

noabstract

examine issues design dynamic information retrieval systems supporting document insertions deletions main components system index maintenance query processing affect query performance usually paid additional update operations aspects system incremental updates garbage collection delayed document deletions discussed focus respective indexing vs query performance trade offs depending relative queries update operations strategies lead optimal overall performance

noabstract

designed representation scheme based discrete representation document ranking function capable reproducing enhancing properties popular ranking functions tf.idf bm25 based language models tests demonstrated capability approach achieve performance scoring functions solely training using heuristic analytic formulas

architecture web question answering seeking system introduce novel algorithm validate semantic categories expected answers tested questions prior research system demonstrated performance comparable current art systems semantic verification algorithm improved accuracy answers affected questions 30

demonstrate usefulness uniform resource locator url performing web page classification approach faster typical web page classification pages fetched analyzed approach segments url meaningful chunks adds component sequential orthographic features model salient patterns resulting features supervised maximum entropy modeling analyze approach's effectiveness standardized domains results scenarios url based methods approach performance current art text link based methods

paper devise method estimation true support itemsets data streams objective maximize chosen criterion precision recall ensuring degradation reduced criterion discuss strengths weaknesses range applicability method relies conventional uniform convergence results guarantees statistical optimality standpoints

information generated multiple authors independently times analyzed synergistically reveals information apparent example traditional search connections trucking industry iraqi banks produce documents mentioning search follows trails associations documents suggest connection auto manufacturer exports iraq iraqi bank providing loans buy cars described extends link analysis based named entities labeled relationships concepts unnamed associations unapparent information revelation involves finding chains connecting concepts documents representation formalism called concept chain graphs

quality document content issue usually ignored traditional ad hoc retrieval task critical issue web search web pages huge variation quality relative example newswire articles address propose document quality language model approach incorporated basic query likelihood retrieval model form prior probability results demonstrate average model significantly baseline query likelihood model terms precision top ranks

mobile ad hoc networks multiple limitations performing similarity based nearest neighbor search dynamic topology frequent disconnections limited power restricted bandwidth cooperative caching effective technique reduce network traffic increase accessibility paper propose solve nearest neighbor search ad hoc networks using semantic based caching scheme reflects content distribution network proposed scheme describes semantic similarity data objects using constraints employs cooperative caching estimate content distribution network query resolution based cooperative caching scheme flooding hierarchy free

paper propose novel framework using genetic programming combine image database descriptors content based image retrieval cbir framework validated experiments involving image databases specific domains images retrieved based shape objects

paper framework called structure sensitive categorization sscat exploits document structure improved categorization framework viz 1 documents layout structure logically coherent text fields using mark language log linear model associates features field weights associated field features learnt training data weights quantify class importance field features determining category document 2 employ technique exploits parse tree fields phrasal constructs title associates weights words constructs boosting weights words called focus words weights learnt example instances phrasal constructs marked corresponding focus words learning accomplished training classifier linguistic features obtained text's parse structure weighted words fields phrasal constructs obtaining features corresponding fields overall framework sscat tested supervised categorization task million products yahoo line shopping data accuracy 90 classifier outperforms naive bayes support vector machines effectiveness sscat strengthens belief linguistic features based natural language structure improve tasks text categorization

clustering increasingly task modern application domains data originally located sites create central clustering clients transmit data central server due technical limitations security aspects central site vague object descriptions available server carry clustering based vague uncertain data recent paper approach clustering uncertain data proposed based concept medoid clusterings idea approach create sample clusterings based suitable distance functions clusterings average clustering i.e medoid clustering determined paper extend approach partitioning clustering algorithms propose compute centroid clustering based input sample clusterings centroid clusterings artificial clusterings minimize distance sample clusterings

olap context exploration huge sparse data cubes tedious task lead efficient results propose multiple correspondence analysis mca enhance data cube representations suitable visualization easier analyze provide original quality criterion measure relevance obtained data representations experimental results led real data samples shown efficiency approach

bioinformatics applications benefit comparing proteins based biological role sequence biological databases proteins annotated ontology terms previous studies identified correlation sequence similarity semantic similarity proteins semantic similarity proteins computed annotated terms proteins sharing biological role necessarily similar sequence.this paper introduces study correlation family similarity family similarity overcomes limitations sequence similarity obtained strong correlation family similarity additionally paper introduces grasm novel method information graph structure instead considering hierarchical tree calculating semantic similarity concepts grasm selects disjunctive common ancestors using informative common ancestor grasm produced family similarity correlation original semantic similarity measures

hierarchical models commonly organize website's content website's content structure represented topic hierarchy directed tree rooted website's homepage vertices edges correspond web pages hyperlinks propose algorithm extracting website's topic hierarchy link structure proposed algorithm consists construction stage refining stage analyze semantic relationships web pages based link structure web page content directory structure we've extensive experiments using websites obtained promising results

explore paper practicably mining task retrieve frequent itemsets memory constraint opposed previous concentrate improving mining efficiency reducing memory size effort attempt constrain upper memory size utilized mining frequent itemsets paper

information enterprises comes documents data bases semantic viewpoint kinds information usually tightly connected paper propose enhance common search engines contextual information retrieved databases establish system requirements anecdotally demonstrate documents database information represented nodes graph example exploit graph information document retrieval

privacy preserving distributed data mining promising research paper addresses association rule mining global database vertically partitioned transactions distributed sites scalar product feasible tool discover frequent itemsets protocol compute scalar product parties permutation approach analyze protocol detail demonstrate effectiveness privacy properties compare published protocols

focus detecting insider access violations topic documents previously utilized information retrieval techniques e.g clustering relevance feedback warn potential misuse relevance feedback approach minimize indicative features detection using data mining techniques derived reduced feature subset achieves equivalent performance previously derived set features

multi relational databases view context content dependent subset tables views preserve privacy hiding sensitive information recent developments data mining challenge database security traditional database security techniques database access control employed paper data mining framework using semi supervised learning demonstrates potential privacy leakage multi relational databases types semi supervised learning techniques nearest neighbor knn method demonstrate privacy leakage introduce approach semi supervised learning hyperclique pattern based semi supervised learning hpsl differs traditional semi supervised learning approaches considers similarity objects instead pairs objects experimental results knn hpsl methods ability compromise database security hpsl privacy violation knn method

propose novel method document clustering using character grams traditional vector space model documents represented vectors dimension corresponds word propose document representation based frequent character grams window size 10 characters derive distance measure produces uniformly results compared word based term based methods result significant light robustness gram method language dependent preprocessing experiments performance clustering algorithm variety test document corpora demonstrate gram representation 3 outperforms word term representations comparison word term representations depends data set selected dimensionality

assessing semantic similarity text documents crucial aspect information retrieval systems propose hyperlink information derive similarity measure applied compare text documents hyperlinks linked documents semantically closer unlinked documents training corpus hyperlinks infer function 8594 sim assigns value linked documents unlinked ones sets experiments corpora function compares favorably okapi matching document retrieval tasks

paper describes framework defining domain specific feature functions user friendly form maximum entropy markov model memm named entity recognition ner task system called merge allows defining feature function templates linguistic rules incorporated classifier simple translating rules specific feature functions shown merge perform purely machine learning based systems purely knowledge based approaches expert interaction rule tuning

novel approach enable decision highly distributed multiagent environment individual agents act autonomous fashion architecture framework integrates risk management knowledge management agent deliberation enable sophisticated autonomous decision instead centralized knowledge repository approach supports highly distributed knowledge base agent manages fraction knowledge entire system

consider finding officially unrecognized effects drugs submitting queries web involving drug name retrieve pages concerning drug retrieved pages irrelevant relevant pages retrieved relevant pages obtained adding active ingredient drug query eliminate irrelevant pages propose machine learning process filter undesirable pages process shown experimentally effective obtaining training data machine learning process time consuming expensive provide automatic method generate training data method shown accurate effects drugs recognized fda validated expert believe approach applied real life yield precision lead perform retrieval accuracy

social networks combat spam paper investigates feasibility mailrank email ranking classification scheme exploiting social communication network created via email interactions underlying email network data collected email contacts mailrank users updated automatically based email activities achieve easy maintenance mailrank rate sender address arriving emails emails trustworthy senders ranked classified spam spam paper variants basic mailrank computes global reputation score email address whereas personalized mailrank score email address mailrank user evaluation mailrank highly resistant spammer attacks obviously considered beginning application scenario mailrank performs sparse networks i.e set peers actually ranking email addresses

paper address unsupervised web data extraction unsupervised web data extraction feasible supposing pages repetitive patterns e.g search engine result pages hereby extraction rules generated automatically training human interaction means operating dom tree respectively flat tag token sequence single page.our contribution automatic data extraction paper twofold identify rank potential repetitive patterns respect user's visual perception web page aware location size matching elements web page constitute criteria defining relevance matching sub sequences pattern weightiness aligned global multiple sequence alignment techniques experimental results system able achieve accuracy distilling aligning regularly structured objects inside complex web pages

framework describing semantic relationships nodes xml documents contrast earlier xml documents id references i.e correspond graphs trees specific interconnection semantics framework defined explicitly derived automatically main advantage interconnection semantics ability pose queries xml data style keyword search methods automatically deriving interconnection semantics complexity evaluation satisfiability derived semantics analyzed complexity tractable hence proposed interconnection semantics efficiently applied real world xml documents

advent xml standard information representation exchange indexing querying xml data major concern paper propose method representing xml document sequence based variation pr 252 fer sequences incorporate components node encodings level descendants develop methods holistic processing tree pattern queries query processing involves converting query sequence performing subsequence matching document sequence establish properties proposed method sequencing rise efficient pattern matching algorithm sequence data stored level sup sup trees support query processing propose optimization parent child axis speed query processing approach require post processing guarantees results free false positives duplicates experimental results system performs significantly previous systems

faced growing knowledge management enterprises increasingly realizing importance seamlessly integrating critical business information distributed structured unstructured data sources existing information integration solutions application formulate sql logic retrieve structured data hand identify set keywords retrieve related unstructured data paper proposes novel approach wherein application specifies information using sql query structured data query automatically translated set keywords retrieve relevant unstructured data describe techniques obtaining keywords query result ii additional related information underlying database techniques achieve accuracy reasonable overheads

exploiting lexical semantic relationships unstructured text collections significantly enhance managing integrating querying information locked unstructured text notably named entities relations entities crucial effective question answering information retrieval knowledge management tasks unfortunately success extracting relationships vary domains languages document collections predicting extraction performance step towards scalable intelligent knowledge management information retrieval information integration language modeling method quantifying difficulty information extraction tasks demonstrate viability approach predicting performance real world information extraction tasks named entity recognition relation extraction

existing web usage mining techniques focus discovering knowledge based statistical measures obtained static characteristics web usage data consider dynamic nature web usage data paper focus discovering novel knowledge analyzing change patterns historical web access sequence data algorithm called lt gt am lt gt lt gt iner lt gt discover web access motifs wams wams web access patterns change change significantly time terms support values specific time period wams useful applications intelligent web advertisement web site restructuring business intelligence intelligent web caching

mining topological patterns spatial databases received lot attention existing typically ignores temporal aspect suffers efficiency scalable mining topological patterns spatio temporal databases paper study mining topological patterns incorporating temporal aspect mining process introduce summary structure records instances count information feature region time window using structure design algorithm topologyminer topological patterns generate candidates experimental results topologyminer effective scalable finding topological patterns outperforms apriori algorithm magnitudes

development aggregate view procurement spend enterprise using transactional data increasingly becoming strategic activity provide complete accurate picture enterprise buying whom allows consolidate suppliers negotiate prices importance complexity cleansing exercise magnified increasing popularity business transformation outsourcing bto wherein enterprises core activities indirect procurement third parties develop integrated view spend multiple enterprises optimize procurement generate maximum savings creation integrated view procurement spend requires creation homogeneous data repository disparate heterogeneous data sources various geographic functional organizations throughout enterprise repositories transactional data various sources invoices purchase account ledgers transactions cross indexed refer suppliers names representing information commodities aggregated spend view developed data cleansed primarily normalize supplier names correctly map transaction appropriate commodity code commodity mapping particular difficult basis unstructured text descriptions found various data sources describe demand system automatically perform cleansing activity using techniques information retrieval machine learning built standard integration application infrastructure software system provides enterprises fast reliable accurate demand cleansing transactional data generating integrated view spend system currently process deployed ibm bto practice

explosive growth world wide web emergence commerce led development recommender systems personalized information filtering technology identify set items user user based model based collaborative filtering successful technology building recommender systems date extensively commercial recommender systems basic assumption algorithms sufficient historical data measuring similarity products users assumption hold various application domains electronics retail home shopping network line retail products introduced existing products disappear catalog application domains home improvement retail industry lot products window treatments bathroom kitchen deck custom product unique little duplicate products domain probability exact products bought close zero paper discuss challenges providing recommendation domains sufficient historical data exist measuring similarity products users feature based recommendation algorithms overcome limitations existing top recommendation algorithms experimental evaluation proposed algorithms real life data sets promise pilot project deploying proposed feature based recommendation algorithms line retail web site 75 increase recommendation revenue 2 month period

describe system automating call center analysis monitoring system integrates transcription incoming calls analysis content analysis introduce novel method estimating domain specific importance conversation fragments based divergence corpus statistics combining method information retrieval approaches provide knowledge mining tools call center agents administrators center

paper concerned intranet search intranet search mean searching information intranet organization found search intranet categorized types analysis survey results analysis search log data types include searching definitions persons experts homepages traditional information retrieval focuses search relevant documents search special types information propose approach intranet search search information special types addition traditional relevance search information extraction technologies play key roles search type approach extract documents information type developed intranet search system called information desk system try address types search finding term definitions homepages topics employees personal information experts topics type search information extraction technologies extract fuse summarize information advance system operation intranet microsoft receives accesses 500 employees month feedbacks users system logs users consider approach useful system help people information paper describes architecture features component technologies evaluation results system

paper novel strategy dragpushing improving performance text classifiers strategy generic takes advantage training errors successively refine classification model base classifier describe applied generate classification algorithms refined centroid classifier refined na 239 ve bayes classifier extensive experimental evaluation algorithms english collections chinese corpus results indicate refined classifiers achieve significant performance improvement base classifiers furthermore performance refined centroid classifier implemented comparable art support vector machine svm based classifier offers lower computational cost

paper citation based information structural content e.g title abstract combined improve classification text documents predefined categories evaluate measures similarity five derived citation information collection derived structural content determine fused improve classification effectiveness discover fusion framework apply genetic programming gp techniques experiments acm computing classification scheme using documents acm digital library indicate gp discover similarity functions superior based solely single type evidence effectiveness similarity functions discovered simple majority voting content based combination based support vector machine classifiers experiments conducted compare performance gp techniques fusion techniques genetic algorithms ga linear fusion empirical results gp able discover similarity functions ga fusion techniques

collaborative filtering regarded promising recommendation algorithms item based approaches collaborative filtering identify similarity items comparing users ratings approaches ratings produced times weighted equally changes user purchase consideration example item rated recently user bigger impact prediction future user behaviour item rated time ago paper novel algorithm compute time weights items manner assign decreasing weight data specifically users purchase habits vary user attitudes towards items proposed algorithm clustering discriminate kinds items item cluster trace user's purchase change introduce personalized decay factor according user own purchase behaviour empirical studies shown algorithm substantially improves precision item based collaborative filtering introducing computational complexity

critical issue moving object databases develop appropriate indexing structures continuously moving object locations queries performed efficiently location changes typically cause volume updates poses serious maintaining index structures paper propose lazy update lgu algorithm disk based index structures moving objects lgu contains key additional structures similar updates performed disk based insertion buffer buffer internal node memory based deletion table table entire tree strategies pushing overflow buffer level studied comprehensive empirical studies uniform skewed datasets simulated street traffic data lgu achieves significant improvement update throughput allowing reasonable performance queries

method assigning labels nodes xml tree called labeling scheme based labels un queries processed accessing original xml file labeling scheme label update cost inserting deleting node xml tree current labeling schemes update cost paper propose novel quaternary encoding approach labeling schemes based encoding approach re label existing nodes update performed extensive experimental results xml datasets illustrate qed existing labeling schemes label updates considering nodes time re labeling

relational approaches proposed detect changes xml documents using relational databases approaches store xml documents relational database issue sql queries whenever appropriate detect changes relational based approaches schema oblivious xml storage strategy detecting changes growing evidence schema conscious storage approaches perform significantly schema oblivious approaches xml query processing concerned paper study relational based unordered xml change detection technique called lt gt elios lt gt schema conscious approach shared inlining underlying storage strategy lt gt elios lt gt 52 times faster diff 7 datasets 1000 nodes 6.7 times faster lt gt andy lt gt 4 result quality deltas detected lt gt elios lt gt comparable result quality deltas detected inf andy inf

text similarity spans spectrum broad topical similarity near extreme document identity intermediate levels similarity resulting summarization paraphrasing copying stronger forms topical relevance useful applications information flow analysis question answering tasks paper explore mechanisms measuring intermediate kinds similarity focusing task identifying particular piece information originated consider sentence sentence document document comparison incorporated algorithms lt gt recap lt gt prototype information flow analysis tool experimental results lt gt recap lt gt indicate mechanisms propose appropriate existing methods identifying intermediate forms similarity

paper approach determine senses words queries using wordnet approach noun phrases query determined word query information associated including synonyms hyponyms hypernyms definitions synonyms hyponyms domains word sense disambiguation comparing pieces information associated words form phrase assign senses words disambiguation fails query words exist exactly process sense query word determined manner guess sense word guess 50 chance correct sense word 50 chance apply web search assist word sense disambiguation process experimental results approach 100 applicability 90 accuracy recent robust track trec collection 250 queries combine disambiguation algorithm retrieval system examine effect word sense disambiguation text retrieval experimental results disambiguation algorithm components retrieval system yield result 13.7 produced system disambiguation 9.2 produced using lesk's algorithm retrieval effectiveness 7 reported result literature

reverse nearest neighbors rknn queries profile based marketing information retrieval decision support data mining systems expensive existing algorithms scalable queries dimensional spaces values paper describes efficient estimation based rknn search algorithm erknn answers rknn queries based local knn distance estimation methods proposed approach utilizes estimation based filtering strategy lower computation cost rknn queries results extensive experiments synthetic real life datasets demonstrate erknn algorithm retrieves rknn efficiently scalable respect data dimensionality data size

paper outlines system architecture core data structures kalchas fulltext search engine xml data emphasis dynamic indexing identifies features worth demonstrating concept dynamic index implies aim re ect creation deletion updates relevant files search index achieved techniques including ideas drawn partitioned trees inverted indices actual ranked retrieval document implemented xml specific query operators lowest common ancestor queries.a live demonstration discuss kalchas behaviour typical interactive editing sessions bulk loading amounts static files querying contents indexed files tries clarify short comings advantages method

paper experience applying event analyzer processing engine developed extract patterns sequence events checking medical cpoe system extensions implemented event analyzer fulfill checking performance evaluation results outline facing adapt event analyzer's pattern detection engine support streaming line cpoe checking system

paper describe knowledge management approach biomedical scientific community developed syynx solutions gmbh 1

organizations begin deploy taxonomies categorization faceted search cost producing knowledge models becoming largest expense project cost 200 300 dollars topic manually developing subject taxonomies scale projects paper discuss approach called orthogonal corpus indexing oci oci leverages existing published knowledge subject taxonomy model knowledge algorithmically mapped multiple taxonomies via oci algorithm resulting taxonomy costs 1 100th cost manual methods created embedded rule sets categorization engines paper discuss theory oci practical examples knowledge management techniques taxonomies detailed inexpensive

paper describe system construction taxonomies yield accuracies automated categorization systems web intranet documents particular describe measurement five key features system predict categories sufficiently defined yield accuracy categorization describe system construct 8800 category purpose taxonomy categorization system

pagerank widely major factor search engine ranking systems global link graph information required computing pagerank causes prohibitive communication cost achieve accurate results distributed solution paper propose distributed pagerank computation algorithm based iterative aggregation disaggregation iad method block jacobi smoothing basic idea divide conquer treat web site node explore block structure hyperlinks local pagerank computed node updated low communication cost coordinator prove global convergence block jacobi method analyze communication overhead major advantages algorithm experiments real web graphs method converges 5 7 times faster traditional power method believe provides efficient practical distributed solution pagerank scale web graphs

p2p ir literature focused distributed indexing structures paper approach based replication peer data summaries via rumor spreading multicast structured overlay.we describe rumorama p2p framework similar ity queries inspired gloss cori p2p adaptation planetp rumorama achieves hierarchization planetp summary based p2p ir networks rumorama network peer views network planetp network connections peers planetp networks aspect peer choose size planetp network according local processing power bandwidth adaptive environment rumorama manages process query summary peer considered exactly network churn actual peers contacted query fraction total peers network.within article rumorama base protocol experiments demonstrating scalability viability approach churn

testing reachability nodes graph applications including knowledge representation program analysis recently biological ontology databases inferencing xml query processing various approaches proposed encode graph reachability information using node labeling schemes existing schemes specific types graphs paper propose novel approach hlss hierarchical labeling sub structures identifies types substructures graph encodes using techniques suitable characteristics implement hlss efficient phase algorithm phase identifies encodes strongly connected components tree substructures phase encodes remaining reachability relationships compressing dense rectangular submatrices transitive closure matrix subproblem finding densest submatrices demonstrate hardness propose practical algorithms experiments hlss handles types graphs existing approaches fall prey graphs substructures designed handle

pivot relational operation allows data rows exchanged columns current relational database management systems support pivot type operations date purely formal algebraic characterization pivot lacking paper characterization terms extended relational algebra operators 964 transpose 928 drop projection 956 unique optimal tuple merge enables 1 draw parallels pivot existing operators employed dynamic data mapping systems ddms 2 formally characterize invertible pivot instances 3 provide complexity results pivot type operations contributions ongoing formal models relational olap

support privacy preserving video sharing proposed novel framework able protect video content privacy individual video clip level prevent statistical inferences video collections protect video content privacy individual video clip level developed effective algorithm automatically detect privacy sensitive video objects video events prevent statistical inferences video collections developed distributed framework privacy preserving classifier training able significantly reduce costs data transmission reliably limit privacy breaches determining optimal size blurred test samples classifier validation experiments specific domain patient training counseling videos convincing results

sentiment classification recent subdiscipline text classification concerned topic document opinion expresses rich set applications ranging tracking users opinions products political candidates expressed online forums customer relationship management functional extraction opinions text determination orientation subjective terms contained text i.e determination term carries opinionated content positive negative connotation paper method determining orientation subjective terms method based quantitative analysis glosses terms i.e definitions terms line dictionaries resulting term representations semi supervised term classification method outperforms methods tested recognized standard benchmarks task

little date sentiment analysis classifying texts positive negative orientation attempted fine grained semantic distinctions features classification method sentiment classification based extracting analyzing appraisal terribly funny appraisal represented set attribute values task independent semantic taxonomies based appraisal theory semi automated methods build lexicon appraising adjectives modifiers classify movie reviews using features based taxonomies combined standard bag words features report art accuracy 90.2 addition types appraisal appear significant sentiment classification

world wide web massive corpus constantly evolves classification experiments usually grab snapshot temporally spatially web corpus paper examine effects page evolution genre classification web pages web genre refers type page characterized features style form presentation layout meta content web genre tune spider crawling re visits inform relevance judgments search engines found pages genres change rarely day research experiments requiring updated version corpus training testing web pages marginal drop accuracy rates genre classification features found useful corpus transfer corpora genres

peer peer p2p systems efficient means data sharing dynamically changing set tonomous nodes.each node p2p system connected nodes creating overlay network nodes query posed node routed overlay network towards nodes hosting data items satisfy paper consider building overlays exploit query workload nodes clustered based results query workload motivation create overlays nodes match similar queries fewlinks apart query frequency account popular queries effect formation overlay unpopular ones focus range selection queries se histograms estimate query results node nodes clustered based similarity histograms introd ce workload aware edit distance metric histograms takes account query workload experimental results workload aware overlays increase percentage query results returned nodes visited compared random i.e unclustered overlays workload aware clustered overlays i.e overlays cluster nodes based solely nodes content

paper propose strategy optimizing placement bin boundaries minimize cost query evaluation using bitmap indices binning attributes distinct values efficient index scheme bitmap index binning type index able resolve user queries resolve queries access original data check candidate records actually satisfy specified conditions call procedure candidate check usually dominates total query processing time set user queries seek minimize total time required swer queries optimally placing bin boundaries dynamic programming based algorithm efficiently determine bin boundaries verify analysis real user queries sloan digital sky survey queries require significant amount time perform candidate check using optimal bin boundaries reduces candidate check time factor 2 total query processing time 40

accurately efficiently estimating distinct values attribute sets attributes data set critical importance database operations query optimization approximation query answering previous focused estimation distinct values single attribute existing adopts data sampling approach paper addresses equally issue estimating distinct value combinations multiple attributes call colscard column set cardinality takes approach existing statistical information e.g histograms available individual attributes assist estimation start exact frequency information individual attributes available pair lower upper bounds colscard consistent available information estimator colscard based probability proceed study partial information form histograms available individual attributes proposed estimator adapted consider types widely histograms constructed obtain optimal approximation experimental evaluation proposed estimation method synthetic real data sets provided

average precision precision commonly cited measures overall retrieval performance correlation defied explanation recently devised geometric interpretation precision suggests reasonable set assumptions precision approximates precision recall curve average precision explaining correlation paper consider assumptions geometric interpretation precision understand reasonable information precision provides geometric interpretation precision precision highly informative demonstrating 1 accurately infer precision recall curves 2 accurately infer measures retrieval performance 3 devise measures retrieval performance analysis conditions precision informative

cluster hypothesis closely related documents tend relevant request exploit hypothesis directly adjusting ad hoc retrieval scores initial retrieval topically related documents receive similar scores refer process score regularization score regularization optimization allowing results semi supervised learning demonstrate regularized scores consistently significantly rank documents unregularized scores variety initial retrieval algorithms evaluate method corpora substantial topics

corpora topics readily available information retrieval research relevance judgments system evaluation expensive cost obtaining prohibits house evaluation retrieval systems corpora topics algorithm cheaply constructing sets relevance judgments method intelligently selects documents judged decides stop little degree confidence result evaluation demonstrate algorithm's effectiveness produces sets relevance judgments reliably discriminate systems algorithm incrementally design retrieval systems simultaneously comparing sets systems additional judgments incremental design change decreases rate reciprocal systems compared demonstrate effectiveness method evaluate trec ad hoc submissions 95 fewer relevance judgments reach kendall's tau rank correlation 0.9

language modeling lm successfully applied information retrieval ir existing lm approaches rely term occurrences documents queries document collections traditional unigram based models terms words usually considered independent recent studies dependence models proposed incorporate term relationships lm links created words sentence term relationships e.g synonymy expand document model study extend family dependence models following 1 term relationships expand query model instead document model query expansion process naturally implemented 2 exploit sophisticated inferential relationships extracted information flow information flow relationships simply pairwise term relationships previous studies set terms term allow context dependent query expansion experiments conducted trec collections obtain significant improvements approach study lm appropriate framework implement effective query expansion

despite recent advances search quality fast increase size web collection introduced challenges web ranking algorithms situations users imprecise poor results key difficulties users usually submit short ambiguous queries specify information improve query formation process answers provided propose novel concept based query expansion technique allows disambiguating queries submitted search engines concepts extracted analyzing locating cycles special type query relations graph directed graph built query relations mined using association rules concepts related current query shown user selects concept interprets related query concept expand original query expanded query processed instead using web test collection approach leads gains average precision figures roughly 32 user provides information type relation query selected concept gains average precision roughly 52

recognized capturing term relationships aspect information retrieval amounts data usually significant evidence fraction potential term pairs consider multiple sources evidence combined predict term relations accurately particularly trying predict probability relevance set terms query involve lexical semantic relations terms.we describe markov chain framework combines multiple sources knowledge term associations stationary distribution model obtain probability estimates potential expansion term reflects aspects original query model query expansion evaluate effectiveness model examining accuracy robustness expansion methods investigate relative effectiveness various sources term evidence statistically significant differences accuracy observed depending weighting evidence random walk example using co occurrence data walk using suggesting improvements effectiveness learning walk behaviors

nowadays huge volumes data organized exported tree structured form querying capabilities provided queries based branching path expression single knowledge domain structural differences raise difficulties querying data sources uniform paper method semantically querying tree structured data sources using partially specified tree patterns based dimensions sets semantically related nodes tree structures define dimension graphs dimension graphs automatically extracted trees abstract structural information semantically rich constructs support formulation queries efficient evaluation design tree pattern query language query multiple tree structured data sources central feature language structure specified partially queries query multiple trees structural differences study derivation structural expressions queries introducing set inference rules structural expressions define types query unsatisfiability provide sufficient conditions checking approach validated experimental evaluation

modern query optimizers select efficient join physical execution plan based essentially average join selectivity factors referenced tables paper argue monolithic approach miss opportunities effective optimization relational queries propose selectivity based partitioning novel optimization paradigm takes account join correlations relation fragments essentially enable multiple effective join evaluation single query nutshell basic idea carefully partition relation according selectivities join operations subsequently rewrite query union constituent queries computed partitions provide formal definition related optimization derive properties characterize set optimal solutions based analysis develop heuristic algorithm computing efficiently effective partitioning input query results preliminary experimental study verify effectiveness proposed approach demonstrate potential effective optimization technique

applications rely sequence databases extensively pattern matching queries retrieve data paper extends traditional pattern matching expressions parameterized patterns featuring variables parameterized patterns expressive allow define concisely regular expressions complex describe variables express additional constraints patterns variables.we evaluated additional cost respect traditional techniques e.g knuth morris pratt algorithm describe algorithm enjoys low memory cpu time requirements provide experimental results illustrate gain optimized solution

web contains documents content equivalent informationally redundant respect presence mutually redundant documents search results degrade user search experience previous attempts address issue notably trec novelty track characterized difficulties accuracy evaluation paper explore syntactic techniques particularly document fingerprinting detecting content equivalence using techniques trec gov1 gov2 corpora revealed degree redundancy user study confirmed metrics accurately identifying content equivalence moreover content equivalent documents significant effect search experience found 16.6 relevant documents runs submitted trec 2004 terabyte track redundant

detection information document stream component potential applications paper novelty detection approach based identification sentence level patterns proposed user's information patterns sentences combinations query words named entities phrases contain relevant information single words proposed novelty detection approach focuses identification previously unseen query related patterns sentences specifically query preprocessed represented patterns include query words required answer types patterns retrieve sentences determined novel answer analysis patterns sentences performed data trec 2002 novelty track experiments novelty detection carried data trec 2003 2004 novelty tracks experimental results proposed pattern based approach significantly outperforms baselines terms precision top ranks

paper novel formulation approach minimal document set retrieval minimal document set retrieval mdsr promising information retrieval task query topic assumed subtopics task retrieve rank relevant document sets maximum coverage minimum redundancy subtopics set task propose document set retrieval ranking algorithms novelty based method cluster based method subtopic extraction based method evaluate system performance design evaluation framework document set ranking evaluates relevance set query topic redundancy set finally compare performance algorithms using trec interactive track dataset experimental results effectiveness algorithms

paper contribution image indexing consisting weighting model visible objects image objects home photographs improve effectiveness weighting model designed according human perception criteria estimated photographs basic hypotheses related human perception validity estimated compared actual observations user study finally formal definition weighting model consistence user study evaluated

databases text text annotated data constitute significant fraction information available electronic form searching browsing typical users locate items databases interfaces multifaceted hierarchies represent powerful browsing paradigm proven successful complement keyword searching multifaceted hierarchies created manually semi automatically difficult deploy multifaceted interfaces databases automatic scalable methods creation multifaceted interfaces methods integrated traditional relational databases scale databases furthermore methods selecting portions generated hierarchies screen space sufficient displaying hierarchy apply technique range data sets including annotated images television programming schedules web pages results promising suggest directions future research

inverted index structures mainstay modern text retrieval systems constructed quickly using line merge based methods provide efficient support variety querying modes paper examine task line index construction build inverted index underlying data continuously queryable documents indexed available search soon inserted straightforward approaches document insertions increasingly expensive size database grows paper describes mechanism based controlled partitioning adapted suit balances insertion querying operations faster scales previous methods using experiments 100gb web data demonstrate efficiency methods practice dramatically reduce cost line index construction

holistic twig join algorithms represent art evaluating path expressions xml queries using inverted indexes xml elements holistic twig joins move set index cursors coordinated quickly structural matches cursor move trigger performance holistic twig join determined cursor moves makes surprisingly existing join algorithms optimized lines paper describe twigoptimal holistic twig join algorithm optimal cursor movement sketch proof twigoptimal's optimality describe twigoptimal information return clause xquery boost performance finally experimental results twigoptimal's superiority existing holistic twig join algorithms

research consistent query answering studies definition computation meaningful answers queries posed inconsistent databases i.e databases data satisfy integrity constraints ics declared schema computing consistent answers conjunctive queries conp hard data complexity presence restricted forms ics single unary keys recent studies consistent query answering database schemas containing key dependencies analyzed possibility identifying classes queries consistent answers obtained rewriting query easily formulated sql directly evaluated relational dbms paper study consistent query answering presence key dependencies exclusion dependencies prove presence exclusion dependencies conp hard data complexity define method consistent answering conjunctive queries key exclusion dependencies based rewriting query datalog negation identify subclass conjunctive queries rewritten presence key exclusion dependencies define algorithm computing rewriting query belonging class queries finally compare relative efficiency methods processing queries subclass mentioned experimental results conducted real database computer science engineering degrees university rome la sapienza computational advantage based technique

studies performance issues i.e access latency energy conservation wireless data broadcast appeared literature security issues addressed paper investigates tradeoff performance security signature based air index schemes wireless data broadcast performance perspective keeping low false drop probability helps clients retrieve information broadcast channel efficiently meanwhile security perspective achieving false guess probability prevents hacker guessing information easily tradeoff aspects administrator wireless broadcast system balance tradeoff carefully configuring signatures broadcast study provides guidance parameter settings signature schemes meet performance security requirements experiments performed validate analytical results obtain optimal signature configuration corresponding application criteria

paper context modeled vector space bases evolution modeled linear transformations base document query associated distinct base corresponds context algorithms proposed discover contexts document query linear algebra employed mathematical framework process context evolution application

contextual search tries capture user's information augmenting user's query contextual information extracted search context example terms web page user currently reading file user currently editing paper scale contextual search system provides overview system design architecture solves major capture quality search context context improve relevancy search queries address introduces information widget captures precise search context provides convenient access functionality inspiration example easily embedded web pages using web api integrated web browser toolbar paper provides overview q's user interaction design highlighting novel aspects capturing quality search context.to address semantic network analyzing search context possibly resolving ambiguous terms generating contextual digest comprising key concepts digest passed query planner rewriting framework augmenting user's search query relevant context terms improve overall search relevancy experience experimental results comparing contextual search results regular yahoo web search results evaluation suggests results considered significantly relevant.the paper identifies research argues contextual search represent major step evolution web search engines

information retrieval systems e.g web search engines critical overcoming information overload major deficiency existing retrieval systems lack user modeling adaptive individual users resulting inherently optimal retrieval performance example tourist programmer word java search information current search systems return results paper study infer user's user's search context inferred implicit user model personalized search decision theoretic framework develop techniques implicit user modeling information retrieval develop intelligent client web search agent ucair perform eager implicit feedback e.g query expansion based previous queries immediate result reranking based clickthrough information experiments web search search agent improve search accuracy popular google search engine

information integration computer science information diverse sources combined users access manipulate information unified central information integration entity resolution er sometimes referred deduplication er process identifying merging incoming records judged represent real world entity.for example consider company customer databases e.g subsidiary integrate identifying matching records challenging unique identifiers sources databases customer appear database fair amount guesswork determining customers match deciding records match computationally expensive e.g involve finding maximal common subsequences strings combine matching records application dependent example phone appear records merged wish pick consolidated number.another source complexity newly merged records match records instance combine records r1 r2 obtain record r12 matches r3 original records r1 r2 match r3 r12 contains information real word entity r1 r2 represent connection r3 apparent chained matches imply merged records recursively compared records.there perform er talk explore approach decision records represent real world entity pair wise fashion furthermore assume matching black box function makes approach generic applicable domains records r1 r2 match function r1 r2 returns true evidence records refer real world entity assume black box merge function combines pair matching records.in talk discuss advantages disadvantages generic pair wise approach er approach relatively simple challenges instance minimize invocations match merge black boxes properties functions significantly reduce calls available multiple processors distribute computational load records confidences associated complexity change efficiently confidence resolved records talk address challenges report preliminary stanford stanford joint omar benjelloun tyson condie johnson heng gong jeff jonas hideki kawai tait larson david menestrina nicolas pombourcq qi su steven whang jennifer widom.for additional information er stanford serf project please visit http www db.stanford.edu serf

1993 verner vinge 3 introduced notion singularity step function nearly unlimited technological capability realized acceleration scientific progress continues produce strong ai nanotechnology super human intelligence introduction idea singularity met evangelism ray kurzweil 2 apocalyptic warnings bill joy 1 talk introduce modest version idea call internet singularity original internet singularity suggests continued acceleration progress makes emphasis ability improve science analytic methods engineering data opposed physical world internet singularity steps.first trend capabilities available people increasing capabilities span content creation community commerce yielding power today's amateur yesterday's professional result boundary producers consumers becoming increasingly blurred time.second internet power law distributions heavy tail implication heavy tail distributions aggregate impact participants participants.third internet comes entirely means authoring derivative aggregations mashups tagging remixing emphasis collaboration sharing yields direct indirect network effects network effects produce entirely utility online activities potentially efficient valuable offline equivalent.fourth internet advances effectively decoupled physical constraints offline world startups costs customer collaborator audience pools dramatically larger improvements happen continuous discreet manner result effective clock cycle progress potentially faster online.putting pieces reveals compelling pattern people contribute collective pool collective pool contains entirely value derived data value data increases individual aggregate capabilities combination components mutually reinforce forming virtuous cycle internet singularity.conceptually consider engineering ability create artifacts mathematical analysis ability analyze numerical properties science pursuit knowledge activities focused digital objects exist internet amplified manner consistent internet singularity.the implications internet singularity profound suggest evolution scientific method moreover trends imply moment history universe computer scientist

ensuring security homeland depends measure distinct factors knowledge prevent predict prepare respond manner terrorist attack natural manmade disaster collaborating sharing knowledge broad range international federal local tribal agencies private public organizations essential adequately addressing factors despite advancements past decade twofold mass diffuse nature complexity data information knowledge required understanding terrorism accounting manifold consequences disasters possession knowledge difficult diffuseness complexity magnified extreme diversity wide distribution potential homeland security collaborators retrospective analysis knowledge discovery useful conditions prospective real time synthesis information multiple users privacy security suggests database designs techniques information retrieval knowledge management advantage technologies semantic nets visualization discrete mathematics build knowledge systems capable homeland security applications

multimedia complex data usually queried similarity predicates whereas dealing algorithms answer basic similarity predicates generic algorithms able efficiently handle similarity complex queries combining basic similarity predicates propose simple effective set algorithms combined answer complex similarity queries set algebraic rules useful rewrite similarity query expressions adequate format algorithms rules algorithms allow relational database management systems complex queries efficient query execution plans experiments highlight scenarios proposed algorithms magnitude faster traditional similarity algorithms moreover linearly scalable considering database size

paper introduce distributed spatio temporal similarity search query trajectory trajectories follow motion similar target trajectories segmented distributed nodes propose novel algorithms ub ublb combine local computations lower upper bounds matching distributed subsequences operation generates desired result pulling distributed subsequences fundamentally expensive communication medium solutions applications wide array domains cellular networks wild life monitoring video surveillance experimental evaluation using realistic data demonstrates framework efficient robust variety conditions

ability retrieve molecules based structural similarity applications disease diagnosis treatment drug discovery design paper method represent protein molecules allows fast flexible efficient retrieval similar structures based global local attributes begin computing pair wise distance amino acids transforming 3d structure 2d distance matrix normalize matrix specific size apply 2d wavelet decomposition generate set approximation coefficients serves global feature vector transformation reduces overall dimensionality data preserving spatial features correlations test method running queries protein data sets previously literature basing comparisons labels scop database method significantly outperforms existing approaches terms retrieval accuracy memory utilization execution time specifically using tree running 10 nearest neighbor search dataset 33,000 proteins average accuracy 89 scop superfamily level total query time 350 times faster previously published techniques addition processing queries based global similarity propose innovative extensions effectively match proteins based solely shared local substructures allowing flexible query interface

document generation low level data utilization challenging tasks document engineering word occurrence detection fundamental recognized document utilization obtained recognizer ocr speech recognition set words dictionary paper proposes efficient dynamic programming dp algorithm occurrences word text paper string similarity measured statistical similarity model enables definition similarities character level edit operation level proposed algorithm tree structures measure similarities avoid measuring similarities substrings appearing text words time complexity proposed algorithm 8901 8901 resp denote nodes trees representing word set resp text donotes model string similarity paper proposed algorithm experimentally six times faster naive dp algorithm

flourish web online review becoming useful information resource people result automatic review mining summarization hot research topic recently traditional text summarization review mining summarization aims extracting features reviewers express opinions determining opinions positive negative paper focus specific domain movie review multi knowledge based approach proposed integrates wordnet statistical analysis movie knowledge experimental results effectiveness proposed approach movie review mining summarization

identify task ongoing research text sentiment analysis predicting utility product reviews orthogonal polarity classification opinion extraction build regression models incorporating diverse set features achieve highly competitive performance utility scoring real world data sets

recent weblogs blogs short form online content personal nature blogs online interactions bloggers temporal nature blog entries differentiate blogs kinds web content bloggers interact linking other's posts forming online communities communities bloggers engage discussions issues entries blogs discussions initiated response online offline events discussion typically lasts limited time duration wish extract temporal discussions stories occurring blogger communities based query keywords propose content community time model leverage content entries timestamps community structure blogs automatically discover stories doing allows discover hot stories demonstrate effectiveness model studies using real world data collected blogosphere

blogosphere totality blog related web sites source trend analysis product survey customer relationship marketing existing approaches based simple counts entries links paper introduce novel concept coined eigen trend represent temporal trend blogs common propose techniques extracting eigen trends blogs propose trend analysis technique based singular value decomposition extracted eigen trends provide insights multiple trends keyword propose trend analysis technique based singular value decomposition analyzes blogosphere dynamic graph structure extracts eigen trends reflect structural changes blogosphere time experimental studies based synthetic data sets real blog data set techniques reveal lot trend information insights blogosphere obtainable traditional count based methods

alternative usual mean average precision currently geometric mean average precision gmap measure average search effectiveness topics gmap specifically emphasise lower average precision scale shed light poor performance search engines paper discusses status measure understood

inex evaluation initiative content oriented xml retrieval establishment defined relevance element according graded dimensions exhaustivity specificity former measures exhaustively xml element discusses topic request whereas specificity measures focused element topic request reason dimensions provide stable measure relevance assessors rate relevance element single scale obtaining relevance assessments costly task document assessed relevance human assessor xml retrieval exacerbated elements document assessed respect exhaustivity specificity dimensions continuous discussion inex sophisticated definition relevance particular exhaustivity dimension paper attempts answer question extensive statistical tests compare conclusions system performance assessment scenarios

familiar evaluation methodologies information retrieval ir suited task comparing systems real settings systems evaluation methods support contextual interactive retrieval changing heterogeneous data collections including private confidential information.we implemented comparison tool inserted natural ir process provides familiar search interface result sets panels elicits searcher judgments logs interaction events tool permits study real information occur documents actually available time search records judgments taking account instantaneous searcher.we validated proposed evaluation approach explored potential biases comparing web search facilities using web based version tool experiments supplied queries laboratory real queries workplace subjects discernable left bias able reliably distinguish low quality result sets found judgments strongly predicted simple implicit measures.following validation undertook study comparing leading web search engines approach ongoing investigations

consider evaluating retrieval systems using incomplete judgment information buckley voorhees recently demonstrated retrieval systems efficiently effectively evaluated using incomplete judgments via bpref measure 6 relevance judgments complete value bpref approximation value average precision using complete judgments relevance judgments incomplete value bpref deviates value continues rank systems manner similar average precision evaluated complete judgment set propose evaluation measures 1 approximations average precision relevance judgments incomplete 2 robust incomplete imperfect relevance judgments bpref proposed estimates average precision simple accurate demonstrate utility estimates using trec data

load shedding techniques generate approximate sliding window join results memory constraints prevent exact computation previously proposed random load shedding method drops input tuples consideration outputs created recently proposed semantic load shedding technique aims produce largest result set consider model data stream tuples contain numerical importance values relevant query source seek maximize importance approximate join result random load shedding semantic load shedding sub optimal situation techniques paper satisfy objective function considering tuple importance join attribute distributions extend existing offline semantic approximation technique compatible objective function space time efficient optimal offline algorithm join memory allotments introduce efficient online algorithms promising maximizing importance approximate join result foreknowledge input streams

data stream clustering emerged challenging past due evolving nature pass restriction imposed data stream model traditional clustering algorithms inapplicable stream clustering challenging data dimensional clusters linearly separable input space paper propose nonlinear stream clustering algorithm adapts stream's evolutionary changes using kernel methods dealing linearity data separation propose novel 2 tier stream clustering architecture tier 1 captures temporal locality stream partitioning segments using kernel based novelty detection approach tier 2 exploits segment structure continuously project streaming data nonlinearly onto low dimensional space lds assigning cluster demonstrate effectiveness approach extensive experimental evaluation various real world datasets

applications classifiers built based multiple related data streams example stock streams news streams related classification patterns involve features streams instead mining single isolated stream examine multiple related data streams patterns build accurate classifier examples related streams include traffic reports car accidents sensor readings types locations paper consider classification defined sliding window join input data streams data streams arrive fast pace join relationship blows data arrival rate impractical compute join build classifier time window slides forward efficient algorithm build na 239 ve bayesian classifier context method perform join operations able build exactly classifier built joined result examines input tuple twice independent tuples joins streams able pace fast arriving data streams presence join relationships experiments confirmed classification algorithm efficient conventional methods maintaining classification accuracy

erroneous data found databases detecting normally trivial task example cope amount biological sequences produced significant genes proteins annotated automated tools protein annotation association protein term describing role tools produced significant misannotations biological databases paper proposes method automatically scoring associations comparing preexisting curated associations association pair links entities score filter incorrect uncommon associations.we evaluated method using automated protein annotations submitted biocreative international evaluation art text mining systems biology method scored annotations scored below threshold discarded results shown trade recall improvement precision example able discard 44.6 66.8 81 misannotations maintaining 96.9 84.2 47.8 correct annotations respectively moreover able outperform individual submission biocreative proper adjustment threshold

consider finding highly correlated pairs data set threshold wish report pairs items binary attributes pearson correlation coefficients threshold correlation analysis step statistical knowledge discovery tasks normally highly correlated pairs compared total pairs identifying highly correlated pairs naive computing correlation coefficients pairs wasteful massive data sets total pairs exceed main memory capacity computational cost naive method prohibitive kdd 04 paper 15 hui xiong et al address proposing taper algorithm algorithm goes data set passes pass generate set candidate pairs correlation coefficients computed directly pass efficiency algorithm depends greatly selectivity pruning power candidate generating stage.in adopt framework taper algorithm propose candidate generation method pair items taper's candidate generation method considers frequencies supports individual items method considers frequency support pair explicitly count frequency support simple randomized algorithm false negative probability negligible space time complexities generating candidate set algorithm asymptotically taper's conduct experiments synthesized real data results algorithm produces greatly reduced candidate set magnitude generated taper algorithm memory faster former critical dealing massive data

radio frequency identification rfid technology fast becoming prevalent tool tracking commodities supply chain management applications movement commodities supply chain forms gigantic workflow mined discovery trends flow correlations outlier paths valuable understanding optimizing business processes.in paper propose method construct compressed probabilistic workflows capture movement trends significant exceptions overall data sets size substantially complete rfid workflow compression achieved based following observations 1 relatively minority items deviate trend 2 truly redundant deviations ie substantially deviate previously recorded ones 3 rfid data registered primitive level data analysis usually takes abstraction level techniques workflow compression based redundant transition emission probabilities derived algorithm computing approximate path probabilities developed experiments demonstrate utility feasibility design data structure algorithms

peer peer p2p search requires intelligent decisions query routing selecting peers query initiated peer forwarded retrieving additional search results decisions based statistical summaries peer usually organized keyword basis managed distributed directory routing indices architectures disregard correlations keywords coarse granularity peer summaries mandated scalability limitation lead poor search result quality.this paper develops evaluates solutions sk stat based single key statistics mk stat based additional multi key statistics hash sketch synopses compactly represent peer's data items efficiently disseminated p2p network form decentralized directory experimental studies gnutella web data demonstrate viability trade offs approaches

static index pruning method ad hoc document retrieval tasks follows document centric approach decide posting term remain index decision based term's contribution document's kullback leibler divergence text collection's global language model technique decrease size index 90 minor decrease retrieval effectiveness allows index fit entirely main memory single pc text collections containing millions documents results efficiency gains superior earlier pruning methods average response time 20 ms gov2 document collection

web information retrieval systems range unique challenges sheer scale data handled specific web retrieval queries mix boolean ranked features documents static score components factored ranking process paper consider range query semantics web retrieval systems impact sorted indexes provide support dynamic pruning mechanisms doing allow fast document time resolution typical mixed mode queries relatively volumes data techniques extend complex query semantics including phrase proximity structural constraints

classification 3d objects remains task data management engineering medicine biology common preprocessing step current approaches classification voxelized 3d objects voxel representations transformed feature vector description.in article introduce approach transforming 3d objects feature strings represent distribution voxels voxel grid attractively feature string extraction performed linear runtime respect voxels define similarity measure feature strings counts common mers input strings referred spectrum kernel field kernel methods prove feature strings similarity measure computed time linear characters strings linear runtime behavior makes kernel attractive datasets occur application domains furthermore explain similarity measure induces metric allows combine tree handling volumes data classification experiments published benchmark datasets novel approach competitive art methods 3d object classification

traditional document classification frameworks apply learned classifier document corpus infeasible extremely document corpora web corporate intranets consider classification corpus processed primarily purpose searching access documents solely inverted index scale search engine main goal build short query characterizes document class using operators normally available engines surprisingly classification accuracy achieved average multiple classes queries 10 terms moreover optimizing efficiency query execution careful selection terms reduce query costs precisely set 10 terms query canachieve 90 accuracy svm classifier 14000 terms willing tolerate reduction 86 svm build 10 terms query executed twice fast 10 terms query

novel approach classifying documents combines pieces evidence e.g textual features documents links citations transparently data mining technique generates rules associating pieces evidence predefined classes rules contain mixture available evidence associated quality criteria conjunction choose rule applied classification time method able perform evidence enhancement link forwarding backwarding i.e navigating documents related citation pieces link based evidence derived furthermore instead inducing single model rule set average predictions proposed approach employs lazy method delays inductive process document classification taking advantage qualitative evidence coming document conducted systematic evaluation proposed approach using documents acm digital library brazilian web directory approach able outperform collections classifiers based available evidence isolation art multi evidence classifiers evaluated approach using standard webkb collection approach gains 1 accuracy 25 times faster approach extremely efficient terms computational performance gains magnitude compared multi evidence classifiers

web page classification tasks information retrieval web mining applying traditional textual classifiers web data produces unsatisfying results fortunately hyperlink information provides clues categorization web page paper improved method proposed enhance web page classification utilizing class information neighboring pages link graph categories represented kinds neighbors parents children siblings spouses combined help page question experiments study effect factors algorithm method proposed able boost classification accuracy common textual classifiers 70 90 dataset pages directory project outperforms existing algorithms unlike prior techniques approach utilizes host links improve classification accuracy neighboring pages unlabeled finally neighbor types contribute sibling pages found

detection information document stream component potential applications novelty detection approach based identification sentence level information patterns proposed information pattern concept novelty detection emphasis information patterns topics queries simply specific questions answers specific named entities nes elaborate thorough analysis sentence level information patterns data trec novelty tracks including sentence lengths named entities sentence level opinion patterns analysis provides guidelines applying patterns novelty detection particularly topics finally unified pattern based approach novelty detection specific topics method dealing topics focus experimental results proposed approach significantly improves performance novelty detection topics overall performance topics 2002 2004 trec novelty tracks

propose method discovering dependency relationships topics documents shared social networks using latent social interactions attempting answer question seemingly topic topic evolve particular seek discover pair wise probabilistic dependency topics documents associate social actors latent social network documents shared viewing evolution topics markov chain estimate markov transition matrix topics leveraging social interactions topic semantics metastable markov chain applied clustering topics applied citeseer dataset collection documents academia trends research topics research topics related stable social actors authors impact topics propose evaluating author impact

widespread templates web considered harmful main reasons compromise relevance judgment web ir web mining methods clustering classification negatively impact performance resource usage tools process web pages paper method efficiently accurately removes templates found collections web pages method steps costly process template detection performed set sample pages derived template removed remaining pages collection leads substantial performance gains compared previous approaches combine template detection removal experimental evaluation approach effective identifying terms occurring templates obtaining measure values 0.9 boosts accuracy web page clustering classification methods

acquisition semantic knowledge paramount application requires deep understanding natural language text motivated building noun phrase level semantic parser adapting various applications machine translation multilingual question answering paper domain independent model noun phrase semantic interpretation investigate based cross linguistic evidence set romance languages spanish italian french romanian focus romance languages motivated english noun phrases translate constructions form romance languages preposition varies correlate semantics based set 22 semantic interpretation categories agent possession empirical observations regarding distribution semantic categories cross lingual corpus mapping various syntactic constructions english romance furthermore training set english noun phrases translations romance languages algorithm automatically learns classification rules applies unseen noun phrase instances semantic interpretation experimental results compared art model reported literature

describe evaluate approach capturing re using search expertise community minded searchers employees company organisation knowledge based industries search expertise ability quickly accurately locate information according specific information corporate asset approach attempt capture knowledge mining title snippet texts results selected community response queries assumption snippet text result play role helping users judge initial relevance result snippet terms selected results contain especially informative terms goals preferences searchers words results selected user recognises combinations terms snippets related information approach seeks build community based snippet index reflects evolving searchers index re rank results returned underlying search engine boosting ranking key results frequently selected similar queries community past

pc desktop rich repository personal information efficiently capturing user's paper propose approach towards automatic personalization web search user specific information extracted local desktops allowing increased quality user profiling sharing private information search engine specifically investigate opportunities select personalized query expansion terms web search using desktop oriented approaches summarizing entire desktop data summarizing desktop documents relevant user query applying natural language processing techniques extract dispersive lexical compounds relevant desktop resources experiments google api latter techniques produce strong improvement current web search

implicit relevance feedback irf algorithms exploit users interactions information customize support offered users search systems unclear individual task differences impact effectiveness algorithms paper describe study effect retrieval performance using additional information user search tasks developing irf algorithms tested algorithms document display time estimate relevance tailored threshold times i.e time distinguishing relevance relevance task user combination neither interaction logs gathered longitudinal naturalistic study online information seeking behavior stimuli algorithms findings tailoring display time thresholds based task information improves irf algorithm performance doing based user information worsens performance implications development effective irf algorithms

discuss information retrieval methods aim serving diverse stream user queries submitted commercial search engines propose methods emphasize importance taking consideration query difference learning effective retrieval functions formulate multi task learning using risk minimization framework particular calibrate empirical risk incorporate query difference terms introducing nuisance parameters statistical models propose alternating optimization method simultaneously learn retrieval function nuisance parameters details l1 l2 regularization provide convergence analysis alternating optimization method special retrieval functions belong reproducing kernel hilbert space illustrate effectiveness proposed methods using modeling data extracted commercial search engine current framework extended future research

propose network data centric storage indcs scheme answering ad hoc queries sensor networks previously proposed network storage ins schemes suffered storage hot spots formed sensors locations uniformly distributed coverage distribution sensor readings uniform range reading values tree based data centric storage kddcs scheme maintains invariant storage events distributed reasonably uniformly sensors kddcs composed set distributed algorithms running time poly log factor diameter network messages sensor send bits messages poly logarithmic sensors load balancing kddcs based defining distributively solving theoretical call weighted split median addition analytical bounds kddcs individual algorithms provide experimental evidence scheme's efficiency ability avoid formation storage hot spots various sizes unlike previous indcs schemes

due resource limitation data stream environment reported answering user queries according wavelet synopsis stream essential ability data stream management system dsms paper motivated user arbitrary range data streams investigate types range constrained queries time series streaming environments distance queries aim obtaining euclidean distance streams knn queries aim discovering nearest neighbors reference stream achieve efficiency processing types queries propose procedure red standing range constrained euclidean distance algorithm eks standing enhanced knn search compared existing methods prior research advantageous features approaches folds approaches capable processing queries directly wavelet synopses retained main memory using idwt reconstruct data cells feature allows save complexity memory time moreover approaches enable users query dsms range unlike conventional methods support range query processing feature enhance flexibility client evaluate procedure red algorithm eks live synthetic datasets empirically proposed approaches efficient similarity search knn discovery arbitrary ranges time series streaming environments

providing integrated optimized support user defined aggregates udas data stream management systems dsms achieve superior power generality preserving compatibility current sql standards demonstrated stream mill system expressive stream language esl efficiently supports wide range applications including advanced ones data stream mining streaming xml processing time series queries rfid event processing esl supports physical logical windows optional slides tumbles built aggregates udas using simple framework applies uniformly aggregate functions written external procedural languages natively written esl constructs introduced esl extend power generality dsms conducive uda specific optimization efficient execution demonstrated experiments

recent papers claimed result means clustering time series subsequences sts clustering independent time series created paper revisits claim particular consider following question time series sequences set sts cluster centroids generated means algorithm reliably determine sequences produced cluster centroids recent results suggest answer answer question affirmative.we cluster shape distance alternate distance measure time series subsequence clusters based cluster shapes set clusters shape sorted list pairwise euclidean distances centroids algorithms based distance measure match set sts cluster centroids time series produced algorithm creates dqg reuse term fingerprints sequences accurate experiments dataset 10 sequences produced correct match 100 time.furthermore offer analysis explains cluster shape distance provides reliable match sts clusters original sequences whereas cluster set distance fails establishes time strong relation result means sts clustering time series sequence created despite earlier predictions

incremental hierarchical text document clustering algorithms organizing documents generated streaming line sources newswire blogs relatively unexplored text document clustering literature popular incremental hierarchical clustering algorithms namely sc cobweb sc sc classit sc widely text document data discuss current form algorithms suitable text clustering propose alternative formulation includes changes underlying distributional assumption algorithm conform data original sc classit sc algorithm proposed algorithm evaluated using reuters newswire articles sc ohsumed sc dataset

widely recognized developing efficient automated algorithms clustering transactional datasets challenging paper propose fast memory efficient scalable clustering algorithm analyzing transactional data approach unique features concept weighted coverage density categorical similarity measure efficient clustering transactional datasets concept weighted coverage density intuitive allows weight item cluster changed dynamically according occurrences items develop transactional data clustering specific evaluation metrics based concept transactional items coverage density respectively third implement weighted coverage density clustering algorithm clustering validation metrics using automated transactional clustering framework called scale sampling clustering structure assessment clustering domain specific evaluation scale framework designed combine weighted coverage density measure clustering sample dataset self configuring methods automatically tune parameters clustering algorithms 1 candidates clusters 2 application domain specific cluster validity measures result set clustering results conducted experimental evaluation using synthetic real datasets results weighted coverage density approach powered scale framework efficiently generate quality clustering results automated manner

vertical search promising direction leverages domain specific knowledge provide precise information users paper study web object ranking key issues building vertical search engine specifically focus objects lack relationships web communities quality photo search test bed investigation proposed score fusion methods automatically integrate web communities web forums rating information proposed fusion methods leverage hidden links discovered duplicate photo detection algorithm aims minimizing score differences duplicate photos forums intermediate results user studies proposed fusion methods practical efficient solutions web object ranking described experiments conducted quality photo ranking proposed algorithms applicable ranking movie ranking music ranking

expert search task users identify people relevant expertise topic expert search system predicts ranks expertise set candidate persons respect users query paper propose novel approach predicting ranking candidate expertise respect query ranking experts voting model adapting eleven data fusion techniques.we investigate effectiveness voting approach associated data fusion techniques range document weighting models context trec 2005 enterprise track evaluation results voting paradigm effective using collection specific heuristics moreover improving quality underlying document representation significantly improve retrieval performance data fusion techniques expert search task particular demonstrate applying field based weighting models improves ranking candidates finally demonstrate relative performance adapted data fusion techniques proposed approach stable regardless weighting models

research information retrieval moving personalized scenario retrieval filtering system maintains separate user profile user framework information delivered user automatically personalized catered individual user's information practical concern personalized system cold start user system endure poor initial performance sufficient feedback user provided.to solve explicit implicit feedback build user's profile bayesian hierarchical methods borrow information existing users analyze usefulness implicit feedback adaptive performance model data sets gathered user studies users interaction document implicit feedback recorded explicit feedback results fold demonstrate bayesian modeling approach effectively trades shared user specific information alleviating poor initial performance user implicit feedback limited unstable predictive value marginal value combined explicit feedback

skyline queries compute set pareto optimal tuples relation ie tuples dominated tuple relation algorithms proposed efficiently evaluating skyline queries require extend relational server specialized access methods feasible perform dominance tests tuples determine result paper introduce salsa sort limit skyline algorithm exploits sorting machinery relational engine tuples subset examined computing skyline result makes salsa particularly attractive skyline queries executed top systems understand skyline semantics skyline logic runs clients limited power bandwidth

paper introduce constrained subspace skyline queries class queries generalization subspace skyline queries using range constraints constrained skyline queries subspace skyline queries addressed previously implications constrained subspace skyline queries examined constrained skyline queries usually expensive regular skylines constrained subspace skyline queries additional performance degradation caused projection support constrained skylines arbitrary subspaces approaches exploiting multiple low dimensional indexes instead relying single dimensional index effective pruning strategies applied discard dominated regions ingredient approach workload adaptive strategy determining indexes assignment dimensions indexes extensive performance evaluation superiority proposed technique compared related competitors

peer data management systems pdms natural extension heterogeneous database systems main tasks systems efficient query processing insisting complete answers leads peer network relaxing completeness requirements applying approximate query answering techniques significantly reduce costs users exact answers queries rank aware query operators top skyline play role query processing paper novel concept relaxed skylines combines advantages rank aware query operators approximate query processing techniques furthermore propose strategy processing relaxed skylines distributed environments allows giving guarantees completeness result using distributed data summaries routing indexes

growing competition telecommunications markets operators increasingly rely business intelligence offer incentives customers existing approaches solely focussed individual behaviour customers call graphs graphs induced people calling allow telecom operators understand interaction behaviour customers potentially provide major insights designing effective incentives.in paper call detail records mobile operator geographically disparate regions construct call graphs analyse structural properties findings provide business insights help devise strategies mobile telecom operators goal paper identify shape graphs extend reachability analysis approach own techniques reveal shape massive graphs based analysis introduce treasure hunt model describe shape mobile call graphs proposed techniques analysing graph finally proposed model captures shape mobile call graphs subject future studies

wide adoption xml increased database community tree structured data management techniques querying capabilities provided tree pattern queries querying tree structured data sources structure integrate multiple data sources tree structures driven recently suggestion query languages relax complete specification tree pattern paper query language allows partial tree pattern queries ptpqs structure ptpq flexibly specified partially evaluate ptpq exploit index graphs generate equivalent set complete tree pattern queries.in process ptpqs efficiently solve ptpq satisfiability containment complex context ptpqs partial specification structure allows trivial structural expressions derived explicitly specified ptpq address ptpq satisfiability containment absence presence index graphs provide sufficient conditions cope complexity ptpq containment presence index graphs study family heuristic approaches ptpq containment based structural information extracted index graph advance fly implement approaches report extensive experimental evaluation comparison

recent research data mining progressed mining frequent itemsets structured patterns trees graphs paper address frequent subtree mining proven viable wide range applications bioinformatics xml processing computational linguistics web usage mining propose novel algorithms mine frequent subtrees database rooted trees evaluate popular sequential encodings trees systematically generate evaluate candidate patterns proposed approach generic mine embedded induced subtrees labeled unlabeled unordered edge labeled algorithms highly cache conscious nature compact simple array based data structures typically l1 l2 hit rates 99 observed experimental evaluation algorithms achieve magnitude speedup real datasets compared art tree mining algorithms

taxonomic measures semantic proximity allow compute relatedness concepts metrics versatile instruments required diverse applications e.g semantic web linguistics text mining approaches geared towards hand crafted taxonomic dictionaries sc word sc sc net sc feature limited fraction real world concepts specific concepts particularly instances concepts i.e names artists locations brand names covered.the contributions paper fold introduce framework based google directory project odp enabling derive semantic proximity arbitrary concepts instances introduce taxonomy driven proximity metric tailored framework studies human subjects corroborate hypothesis metric outperforms benchmark semantic proximity metrics comes close human judgement

topic feature extraction step document classification text mining topics succinct representation content document collection hence effective content identifiers peer peer systems scale distributed content management systems effective topic extraction dependent accuracy term clustering deal synonymy polysemy retrieval techniques based spectral analysis latent semantic indexing lsi effectively solve spectral retrieval schemes produce term similarity measures symmetric accurate characterization term relationships drawback lsi running time polynomial dimensions matrix prohibitively ir applications paper efficient algorithms using technique locality sensitive hashing lsh extract topics document collection based asymmetric relationships terms collection relationship characterized term co occurrences similarity measures lsh based scheme viewed simple alternative lsi efficacy algorithms via experiments set documents feature algorithms produces natural hierarchical decomposition topic space instead flat clustering

domain specific ontologies heavily applications instance form bases similarity dissimilarity keywords extracted various knowledge discovery retrieval tasks existing similarity computation schemes categorized structure information based approaches structure based approaches compute dissimilarity keywords using weighted count edges keywords information base approaches hand leverage available corpora extract additional information keyword frequency achieve performance similarity computation structure based approaches unfortunately application domains applications rely unique keys relational database frequency information required information based approaches exist paper note third computing similarity node hierarchy represented vector related concepts vectors compared compute similarities requires mapping concept nodes hierarchy onto concept space paper propose concept propagation cp scheme relies semantical relationships concepts implied structure hierarchy annotate concept node concept vector cv refer approach cp cv comparison keyword similarity results cp cv provides significantly upto 33 results existing structure based schemes cp cv assume availability appropriate corpus extract keyword frequency information approach matches slightly improves performance information based approaches

document level security dls enforcing permissions prevailing time search specified mandatory requirement enterprise search applications unfortunately depending implementation details values key parameters dls price increased query processing time leading unacceptably slow search experience paper model method carrying secure search presence dls enterprise webs report alternative commercial dls search implementations using 10,000 document experimental dls environment graph dependence query processing time result set size visibility density classes user scaled collections tens thousands documents results suggest query times unacceptable exact counts matching documents required users view proportion documents time conduct access checks dramatically increased requests sent server local network discuss methods reducing cost security checks conclude enterprises effectively reduce dls overheads organizing documents access checking collection document level forgoing accurate match counts using caching batching hierarchical methods cut costs dls checking applicable using single portal access search documents

relational dbms provides limited capabilities perform multidimensional statistical analysis requires manipulating vectors matrices study extend dbms basic vector matrix operators programming user defined functions udfs carefully analyze udf features limitations implement vector matrix operations commonly statistics machine learning data mining paying attention dbms operating system computer architecture constraints udfs represent programming interface allows definition scalar aggregate functions sql udfs advantages limitations udf allows fast evaluation arithmetic expressions memory manipulation using multidimensional arrays exploiting language control statements nevertheless udf perform disk amount heap stack memory allocated udf code consider specific architecture characteristics dbms experimentally compare udfs sql respect performance ease flexibility scalability profile udfs based call overhead memory management interleaved disk access udfs faster standard sql aggregations fast sql arithmetic expressions

paper describe polestar policy explanation using stories arguments integrated suite knowledge management collaboration tools intelligence analysts.polestar provides built support analyst workflow including collection textual source documents structured argumentation automatic citation analytic product documents underlying polestar scalable dependency repository provides traceability product documents source snippets repository's notification engine allows polestar alert analysts dependent sources discredited aid repairing affected arguments paper discusses recent extensions polestar support collaborative analysis community finding portfolio sharing peer review arguments conclude preview future research summary polestar's primary benefits view deployed users

knowledge management approaches weakly structured adhoc knowledge processes lightweight i.e rely upfront modeling efforts paper tasknavigator novel prototype support weakly structured processes integrating standard task list application art document classification system resulting system allows task oriented view office workers personal knowledge spaces realize proactive contextsensitive information support daily knowledge intensive tasks moreover tasknavigator supports process reuse proactively suggesting similar tasks relevant process models based textual similarities finally report feasibility test study conducted evaluate system context daily research task management software requirements analysis

classical linear discriminant analysis lda applicable sample size due singularity scatter matrices involved regularized lda rlda provides simple strategy overcome singularity applying regularization term commonly estimated via cross validation set candidates cross validation computationally prohibitive candidate set efficient algorithm rlda computes optimal transformation rlda set parameter candidates approximately cost running rlda times facilitates efficient model selection rlda.an intrinsic relationship rlda uncorrelated lda ulda recently proposed dimension reduction classification specifically rlda shown approach ulda regularization value tends zero rlda regularization equivalent ulda shown ulda maps data class common mild condition shown hold dimensional datasets leads overfitting ulda observed applications thetheoretical analysis provides justification regularization rlda extensive experiments confirm claimed theoretical estimate efficiency experiments properly chosen regularization parameter rlda performs favorably classification comparison ulda existing lda based algorithms support vector machines svm

domain specific information retrieval demand domain experts average expert users searching domain specific e.g medical health information online resources typical average users search results mixture documents levels readability expert users documents readability top list consequently search results re ranked descending readability practical domain experts manually label readability documents databases computational models readability investigated traditional readability formulas designed purpose text insufficient deal technical materials domain specific information retrieval advanced algorithms textual coherence model computationally expensive re ranking retrieved documents paper propose effective computationally tractable concept based model text readability addition textual genres document model takes account domain specific knowledge i.e domain specific concepts contained document affect document's readability major readability formulas proposed applied health medical information retrieval experimental results proposed readability formulas lead remarkable improvements terms correlation users readability ratings traditional readability measures

major challenge developing models hypertext retrieval effectively combine content information link structure available hypertext collections link based ranking methods developed improve retrieval results none exploit discrimination power contents exploit useful link structures paper propose relevance propagation framework combining content link information framework probabilistic score document defined based probabilistic surfing model main characteristics framework probabilistic view relevance propagation model propagation multiple sets neighbors compare eight models derived probabilistic relevance propagation framework standard trec web test collections results eight relevance propagation models outperform baseline content ranking method wide range parameter values indicating relevance propagation framework provides effective robust exploiting link information experiments using multiple neighbor sets outperforms using type neighbors significantly taking probabilistic view propagation provides guidance setting propagation parameters

heart information retrieval models utilize form term frequency.the notion query term occurs document document meets information examine alternative propose model assesses presence term document looking actual occurrence term set independent supporting terms i.e context yields weighting terms documents complementary tf based methods beneficial retrieval

paper introduce notion ranking robustness refers property ranked list documents indicates stable ranking presence uncertainty ranked documents propose statistical measure called robustness score quantify notion demonstrate robustness score significantly consistently correlates query performance variety trec test collections including gov2 collection compare robustness score clarity score method art technique query performance prediction experimental results robustness score performs clarity score clarity score barely correlated query performance gov2 collection correlation robustness score query performance remains significant notice combination usually results prediction power

deal results returned commerce web database response user query paper proposes novel approach rank query results based user query speculate user cares attribute assign corresponding weight tuple query result attribute value assigned score according desirableness user attribute value scores combined according attribute weights final ranking score tuple tuples top ranking scores user ranking method domain independent requires user feedback experimental results demonstrate ranking method effectively capture user's preferences

optimising parameters ranking functions respect standard ir rank dependent cost functions eluded satisfactory analytical treatment build recent advances alternative differentiable pairwise cost functions techniques successfully applied tuning parameters existing family ir scoring functions bm25 sense using sensible search heuristics directly optimize rank based cost function ndcg demonstrate size training set affects parameters hope tune

consider estimating size collection documents using standard query interface main idea construct unbiased low variance estimator closely approximate size set documents defined conditions including document set match query uniformly sampleable query pool size fixed advance.using basic estimator propose approaches estimating corpus size approach requires uniform random sample documents corpus approach avoids notoriously difficult sample generation instead fairly uncorrelated sets terms query pools accuracy approach depends degree correlation sets terms.experiments trec collection major search engines demonstrates effectiveness algorithms

text summarization data reduction process text summarization enables users reduce amount text read assimilating core information data reduction offered text summarization particularly useful biomedical domain physicians continuously clinical trial study information incorporate patient treatment efforts efforts hampered volume publications contribution fold 1 propose frequency domain concepts method identify sentences text 2 propose novel frequency distribution model algorithm identifying sentences based term concept frequency distribution evaluation existing summarization systems using biomedical texts determine performance baseline domain concept comparison recent performing frequency based algorithm using terms adapted concepts evaluated using terms concepts shown concepts performs closely terms sentence selection proposed frequency distribution model algorithm outperforms art approach

study novel efficiently computing update distance pair relational databases analogy edit distance strings define update distance databases minimal set oriented insert delete modification operations transform database distance computed traversing search space database instances connected update operations insight leads family algorithms compute update distance approximations experiments observed simple heuristic performs surprisingly considered cases.our motivation studying distance measures databases stems field scientific databases replicas single database maintained sites typically leads accidental planned divergence content re create consistent view differences resolved effort requires understanding process produced found minimal update sequences set oriented update operations proper concise representation systematic errors giving valuable clues domain experts responsible conflict resolution

amount query independent summarization documents due success web search engines query specific document summarization query result snippets received little attention method create query specific summaries identifying query relevant fragments combining using semantic associations document particular add structure documents preprocessing stage convert document graphs summaries computed calculating top spanning trees document graphs experimentally evaluate efficient algorithms support computing summaries interactive time furthermore quality summarization method compared current approaches using user survey

paper revisits analysis annotation propagation source databases views defined terms conjunctive spj queries source database spj query view tuple 916 view view resp source effect minimal set 916 tuples deletion 916 results deletion 916 minimizing effects view resp source third referred annotation placement single base tuple 916 annotation field 916 propagates 916 minimizing propagation fields view data provenance management view updates unfortunately np hard subclasses spj views 5 annotation propagation analysis feasible practice propose key preserving condition spj views requires projection fields spj view retain key base relation involved condition restrictive proposals 11 14 simplifies annotation propagation analysis indeed key preserving spj views annotation placement coincides view effect view source effect tractable addition generalize setting 5 allowing 916 tuples deleted investigate insertion tuples view updates analysis harder np hard subclasses spj views spj views source view effect np hard single tuple insertion tractable subclasses spj insertions presence absence key preservation condition

study optimization relational queries using materialized views views regular restructured restructured view data base table represented metadata schema information table attribute names vice versa.using restructured views query optimization spectrum views previously available result significant additional savings query evaluation costs savings obtained due significantly larger set views choose involve reduced table sizes elimination self joins clustering produced restructuring horizontal partitioning.in paper propose query optimization framework treats regular restructured views uniform manner applicable sql select project join queries views aggregation framework provide 1 algorithms determine view regular restructured usable answering query 2 algorithms rewrite query using usable views.semantic information knowledge key view optimize rewritten query query optimization framework develop techniques determining key regular restructured view information optimize rewritten query straightforward integrate algorithms techniques standard query optimization algorithms

performance query processing improved using complementary approaches improve buffer file system management policies db buffer manager os file system manager e.g page replacement improve sequence requests submitted file system manager lead actual o's block request sequences paper takes latter approach exploiting common file system practices found linux propose techniques permuting refining block request sequences block level file level block recycling manifest techniques create plan operations sc mms sc sc shj sc adopts block request refinement techniques implement plan operations top postgres running linux experimental results demonstrate factor 4 performance benefit techniques

practical classification mix learnable unlearnable classes classifier minimum performance threshold deployed exacerbated training set created active learning bias actively learned training sets makes hard determine class learned evidence efficient method reducing bias correctly identifying classes learned characterize scenarios active learning succeed despite difficulties

classification algorithms document representation approaches key elements successful document classification system past conducted represent documents attempts rely extra resources wordnet extremely dimension paper propose document representation approach based multigram language models approach automatically discover hidden semantic sequences documents category based multigram language models gram language models forward text classification algorithms experiments rcv1 proposed algorithm based multigram models achieve similar classification performance compared classifier based gram models model size algorithm latter proposed algorithm based combination multigram models gram models improves micro f1 macro f1 values 89.5 92.6 87.2 91.1 respectively observations support validity proposed document representation approach

hard identify navigational queries web search due lack sufficient information web queries typically short paper study machine learning methods including naive bayes model maximum entropy model support vector machine svm stochastic gradient boosting tree sgbt navigational query identification web search boost performance machine techniques exploit feature selection methods propose coupling feature selection classification approaches achieve performance prior features paper study identifying navigational queries thousands available features extracted major commercial search engine results web search user click data query log web's relational content multi level feature extraction system constructed.our results real search data 1 features tested user click distribution features set features identifying navigational queries 2 achieve performance machine learning approaches coupled feature selection methods gradient boosting tree coupled linear svm feature selection effective 3 carefully coupled feature selection classification approaches navigational queries accurately identified 88.1 f1 score 33 error rate reduction compared uncoupled system 40 error rate reduction compared tuned system feature selection

paper proposes novel document re ranking approach information retrieval label propagation based semi supervised learning algorithm utilize intrinsic structure underlying document data labeled relevant irrelevant documents available ir approach tries extract pseudo labeled documents ranking list initial retrieval pseudo relevant documents determine cluster documents top ones via cluster validation based means clustering pseudo irrelevant ones pick set documents bottom ones ranking documents conducted via label propagation evaluation benchmark corpora approach achieve significant improvement standard baselines performs related approaches

understanding connections human activities content textual information generated regard activities firstly define motivate sense various life events secondly introduce domain massive online collaborative environments specifically online virtual worlds people meet exchange messages perform actions rich data source analysis finally outline experimental tasks statistical language modeling text clustering techniques allow explore connections successfully

scientific domains knowledge discovered experiments clustering based similarity output causes similarity analyzed based input conditions characterizing type output i.e cluster analysis helps applications decision support industry cluster representatives form glance depictions applications randomly selecting set conditions cluster representative sufficient distinct combinations inputs lead cluster paper approach called descond proposed design semantics preserving cluster representatives scientific input conditions define notion distance conditions capture semantics based types attributes relative importance using distance methods building candidate cluster representatives levels detail proposed candidates compared using descond encoding proposed paper assesses complexity information loss user candidate lowest encoding cluster returned designed representative descond evaluated real data materials science evaluation domain expert interviews formal user surveys designed representatives consistently outperform randomly selected ones candidates suit users

propose adapt newly emerged cache oblivious model relational query processing goal automatically achieve overall performance comparable fine tuned algorithms multi level memory hierarchy automaticity cache oblivious algorithms assume knowledge specific parameter values capacity block size level hierarchy step propose recursive partitioning implement cache oblivious nested loop joins nljs indexes recursive clustering buffering implement cache oblivious nljs indexes theoretical results empirical evaluation architectures cache oblivious nljs match performance manually optimized cache conscious counterparts

set valued attributes frequently occur contexts market basked analysis stock market trends late research literature mainly focused set containment joins data mining considering simple queries set valued attributes paper address superset subset equality queries propose novel indexing scheme answering set valued attributes proposed index superimposes trie tree top inverted file indexes relation set valued data efficiently answer aforementioned queries indexing subset frequent items occur indexed relation finally extensive experiments approach outperforms art mechanisms scales gracefully database size grows

applications data values inherently uncertain includes moving objects sensors biological databases recent development database management systems handle uncertain data proposals systems include attribute values uncertain particular attribute value modeled range values associated probability density function previous efforts type data addressed simple queries range nearest neighbor queries queries join multiple relations addressed earlier despite significance joins databases paper address join queries uncertain data propose semantics join operation define probabilistic operators uncertain data propose join algorithms provide efficient execution probabilistic joins paper focuses class joins termed probabilistic threshold joins avoid semantic complexities dealing uncertain data class joins develop sets optimization techniques item level page level index level pruning techniques facilitate pruning little space time overhead easily adapted join algorithms verify performance techniques experimentally

rapid growth transactional data brought soon attention exploitation paper investigate securing sensitive knowledge exposed patterns extracted association rule mining instead hiding produced rules directly decide hide sensitive frequent itemsets lead production rules step introduce notion distance databases measure quantifying trying minimize distance original database sanitized version safely released propose novel exact algorithm association rule hiding evaluate real world datasets demonstrating effectiveness towards solving

research privacy preserving techniques databases subsequently privacy enhancement technologies witnessed explosive growth spurt recent escalation fueled growing mistrust individuals towards organizations collecting disbursing personally identifiable information pii digital repositories increasingly susceptible intentional unintentional abuse resulting organizations liable privacy legislations adopted governments world privacy concerns necessitated advancements field distributed data mining wherein collaborating parties legally bound reveal private information customers paper algorithm sc pripsep sc sc pri sc vacy sc sc reserving sc se sc quential sc sc atterns mining sequential patterns distributed databases preserving privacy salient feature sc pripsep sc due flexibility pertinent mining operations real world applications terms efficiency functionality reasonable assumptions prove architecture protocol employed algorithm multi party computation secure

paper propose dictionary data structure string search errors query string didiffer expected matching string edits data structure database string common prefix errors specifically database random strings length perform string search query string differs closest match edits using data structure linear size query time equal 213 log sup 2 log klog 2m sup 2m means 60 log 2m log query time 213 1 significant practice applications relative approach converts strings bit vectors similar strings map similar bit vectors hamming distance simple reduction obtain similar results approximate prefix search

paper study learning block classification models estimate block functions distinguish models learned multiple sites site specific models learned individual sites consider factors affect learning process model effectiveness factors include layout features content features classifiers term selection methods empirically evaluated performance models factors varied main results layout features content features learning site specific models

paper addresses automatically structuring linked document collections using clustering contrast traditional clustering study clustering light available link structure information data set e.g hyperlinks web documents co authorship bibliographic data entries approach based iterative relaxation cluster assignments built top clustering algorithm technique results cluster purity overall accuracy self organization robust

relevance feedback rf technique allowing enrich initial query according user feedback goal express precisily user's issues appear considering semi structured documents xml documents rf approaches proposed xml retrieval simple adaptations traditional rf granularity information enrich queries adding terms extracted relevant elements instead terms extracted documents paper structural constraints rf propose approach able extend initial query adding generative structures approach applied unstructured queries experiments carried inex collection results method

association patterns text categorization attracted variety useful methods developed key characteristics pattern based text categorization remain unclear indeed concrete answers following questions association patterns candidate pattern based text categorization desirable patterns text categorization paper focus answering questions specifically hyperclique patterns desirable frequent patterns text categorization line develop algorithm text categorization using hyperclique patterns experimental results method provides performance art methods terms computational performance classification accuracy

hierarchical tree structure online maintenance time decaying synopses streaming data exemplify amnesic behavior streams locations numerous moving objects obtain reliable trajectory approximations affordable estimates regarding distinct count spatiotemporal queries

view inefficiency traditional phase twig stack algorithm propose single phase holistic twig pattern matching method based twigstack algorithm applying novel stack structure

paper propose approach efficient approximative rknn search arbitrary metric spaces value specified query time method approximation nearest neighbor distances prune search space experiments solution scales significantly existing approximative approaches producing approximation true query result recall

noabstract

noabstract

noabstract

noabstract

introduce methods combining feature selectors text classification results investigation combinations summarized easily constructed combinations feature selectors shown improve peak precision f1 statistically significant levels

document query expansions separately previous studies enhance representation documents queries paper propose method integrates expansion carried using multi stage markov chains experiments method significantly outperforms existing approaches

paper address issue continuous keyword queries multiple textual streams line represents significant departure previous keyword search models assumed static database model user poses query comprised collection keywords subsequently applied multiple text streams rss news feeds tv closed captions emails result query combination streams sufficiently correlated collectively contain query keywords specified time span

paper called density divergence explored related phenomenon densities clusters vary subspace cardinalities densities consideration subspace clustering explore algorithm adaptively determine density thresholds discover clusters subspace cardinalities

noabstract

dsa derivative time series segment approximation novel representation model time series designed effective efficient similarity search dsa substantially exploits derivative estimation segmentation dimensionality reduction meet requirements sensitivity main features trends time series robustness outliers experiments dsa drastically faster prominent art similarity methods

poster paper summarizes solution mining max frequent generalized itemsets itemsets compact representation frequent patterns generalized environment

investigate aspects parameterized retrieval models estimation sensitivity generalization parameterized models based heuristics inherent uncertainty study issues using statistical tools

noisy parallel corpora widely cross language information retrieval clir previous studies focus truly parallel corpus paper examine approaches exploit noisy corpora filtering noise corpora adapting training process translation model noise corpora experiments approach suited clir

noabstract

noabstract

relational databases widely mechanism providing access structured data suitable typical information finding tasks users semantic gap queries users express queries answered database paper propose system bridges semantic gap using domain knowledge contained ontologies system extends relational databases ability answer semantic queries represented sparql emerging semantic web query language users express queries sparql based semantic model data semantically relevant results define categories results semantically relevant users query system retrieves results evaluate performance system sample relational databases using combination standard custom ontologies

noabstract

paper design develop unified system ge miner gene expression miner integrate cluster ensemble text clustering multi document summarization provide environment comprehensive gene expression data analysis novel cluster ensemble approach generate quality gene cluster text summarization module gene cluster expectation maximization em based algorithm automatically identify subtopics extract probable terms topic extracted top topical terms subtopic combined form biological explanation gene cluster experimental results demonstrate system obtain quality clusters provide informative key terms gene clusters

poster farsi called persian stemmer dictionary introduced evaluation results significant improvement performance precision recall information retrieval ir system using stemmer

paper explore concept library brain images implies repository brain images efficient search retrieval mechanisms based models derived ir practice preliminary study collection functional mri brain images assembled study distinct cognitive tasks adapt classical ir methods inverted indexing tfidf latent semantic indexing lsi content based brain image retrieval results efficient accurate retrieval brain images representations motivated ir perspective somewhat effective methods based retaining image information

paper simple adaptable matching method dealing web directories catalogs owl ontologies using knowledge discovery databases model association rule paradigm method originality extensional asymmetric terminological level selecting concept relevant terms contained documents permits discover equivalence subsumption relations holding entities concepts properties method relies implication intensity measure probabilistic model deviation independence selection significant rules concepts properties lead criteria permitting assess respectively implication quality generativity rule finally proposed method evaluated benchmarks contains conceptual hierarchies populated textual documents composed owl ontologies

optimization techniques e.g data stream management systems dsmss treatment disjunctive predicates necessity paper introduce compare methods matching evaluating disjunctive predicates

monitoring systems involve continuous queries streaming data distributed collaborative system distribution query operators network processors processing sequence form query configuration inherent constraints throughput support paper propose optimize stream queries respect version throughput measure profiled input throughput measure focused matching expected behavior input streams prune search space hill climbing techniques proved efficient effective

combination proven methods time series analysis machine learning explore relationship temporal semantic similarity web query logs discover combination correlation cycles perfect sign semantic relationship

microarray technology powerful tool geneticists monitor interactions tens thousands genes simultaneously extensive research coherent subspace clustering gene expressions measured consistent experimental settings methods assume experiments run using batch microarray chips similar characteristics noise algorithms developed assumption applicable analyzing data collected heterogeneous settings set genes monitored expression levels directly comparable gene paper propose model cluster mining subspace coherent patterns heterogeneous gene expression data compare model previously proposed models analyze search space na 239 ve solution

distributed privacy preserving data mining tools critical mining multiple databases minimum information disclosure framework including model multi round algorithms mining horizontally partitioned databases using privacy preserving nearest neighbor knn classifier

noabstract

noabstract

text segmentation text analysis text alignment determine shared sub topics similar documents multi task text segmentation alignment extension single task segmentation utilize information multi source documents paper introduce novel domain independent unsupervised method multi task segmentation alignment based idea optimal segmentation alignment maximizes weighted mutual information mutual information term weights experiment results approach

noabstract

sequential patterns generated newly arriving patterns identified frequent sequential patterns due existence data sequences practice users usually recent data ones capture dynamic nature data addition deletion propose model sequential pattern mining progressive database addition progressive concept progressively discover sequential patterns recent time period

private data matching data sets potentially distrusted parties wide range applications existing solutions substantial weaknesses meet practical application scenarios particular practical private data matching applications require discouraging matching parties spoofing private inputs paper address challenge forcing matching parties escrow data matching auditorial agent period undertake liability attest genuineness escrowed data

paper novel relevance feedback rf algorithm probabilistic document context based retrieval model limited relevance judgments document re ranking probabilities document context based retrieval model estimated top 20 documents initial retrieval document context based cosine similarity measure similar data probability estimation reduce data scarcity negative weighting rf algorithm promising mean average precision statistically significantly baseline using trec 6 trec 7 data collections

noabstract

paper examines utilizing pseudo anchor text help ranking web objects vertical search adopt machine learning based approach extract pseudo anchor text vertical object candidate anchor blocks experiments academic search domain indicate approach able dramatically improve search performance

addresses common search frequently occurring underspecified user queries top ranked results queries contain documents relevant user's search intent fresh relevant pages ranks underspecified query due freshness pages match query despite users searched content recently propose novel method rank effectively refine ranking search results query constructing query context search query logs evaluation results rank gains considerable advantage current ranking system scale commercial web search engine able improve relevance search results 82 queries

propose approach organizes search result clusters hierarchical structure called query taxonomy user's perspective proposed approach based unsupervised classification method dynamic web training corpus query taxonomy users browse relevant web documents conveniently comprehensibly experimental results verify feasibility effectiveness proposed approach query taxonomy generation web search

noabstract

noabstract

information organization access filtering systems benefit document representations traditional information retrieval ir topic detection tracking tdt example application paper demonstrate named entities serve choices units document representation words test hypothesis study effect words based entity based representations story link detection sld core task tdt research experiments tdt corpora entity based representations significant improvements sld propose mechanism expand set named entities document representation enhances performance step analyze limitations using named entities document representation studies experiments indicate adding additional topical terms help addressing limitations

fundamental building block data mining analysis approaches density estimation provides comprehensive statistical model data distribution reason application transient data streams highly desirable convenient nonparametric method density estimation utilizes kernels computational complexity collides rigid processing requirements data streams approach combines linear processing cost constant amount allocated memory approach supports dynamic memory adaptation changing system resources

noabstract

periodicity detection pre processing step time series algorithms provides information structural properties time series feature vectors based periodicity clustering classification abnormality detection human motion understanding periodicity detection task difficult simple uncontaminated signal unfortunately real datasets exhibit following properties stationarity ii interlaced cyclic patterns iii data contamination makes period detection extremely challenging seemingly straightforward solution develop individual specialized algorithms handling separately determining time series stationary contaminated extremely difficult task article propose generic algorithms detect periods complex noisy incomplete datasets algorithm leverages frequency characterization autocorrelation structure inherent time series estimate periodicity extend methods handle stationary time series tracking candidate periods using kalman filter address finding multiple interlaced periodicities

topic hierarchies popular method summarizing results obtained response query various search applications topic hierarchies rigid pre defined somewhat unintuitive dynamically generated statistical techniques paper propose alternative approach query disambiguation result summarization placing results set contextual dimensions viewed facets generic search scenario illustrate approach using types contextual dimensions namely concepts features specializations nlp techniques data mining algorithm select distinct contexts

paper system danvideo implemented using j2se jmf annotate manually macro micro features dance videos dance experts mpeg 7 reached matured description multimedia structure semantics descriptors description schemes danvideo generates mpeg 7 instance conforms mpeg 7 schema semi automatically effortlessly dance annotations

noabstract

paper interactive visualization toolkit navigating analyzing national science foundation nsf funding information design builds improved 2.5d treemap layout stacked graph contribute customized techniques visually navigating interacting hierarchical data nsf programs proposals furthermore incremental layout method adopted handle information scale improved treemap visualization help visually analyze static funding related data stacked graph utilized analyze time series data visual analysis techniques research trends nsf popular nsf programs quickly identified

paper based novel shape similarity based retrieval method propose interactive partial distance map pdm based dimensional indexing scheme speed retrieval performance chinese calligraphic character databases specifically approximate minimal bounding hyper sphere query character search pdm utilize users relevance feedback refine search process conduct comprehensive experiments testify efficiency effectiveness proposed method

paper pilot study query specific clustering novel document context based similarity scores compared document similarity scores clustering applied top 1000 retrieved documents query clustering effectiveness evaluated based mk1 score trec 2 trec 6 trec 7 test collections encouraging results obtained whereby document context clustering produces mk1 scores document clustering 95 confidence level precision recall equally

observed queries xml data sources unsatisfiable unsatisfiability stem sources e.g user insufficiently familiar labels appearing documents intimately aware hierarchical structure documents difficulty compounded errors query formulation lead empty answer sort compilation error deal query document mismatches previous research considered returning answers maximally satisfy sense query instead returning strictly satisfying answers breaks golden database rule strictly satisfying answers returned querying indeed relationship query answers unsatisfying answers returned revive golden database rule paper proposes framework deriving self correcting queries xml framework generates similar satisfiable queries user query unsatisfiable user choose satisfiable query receive exactly satisfying answers query

xml query languages typically allow specification structural patterns elements finding occurrences patterns xml tree key operation xml query processing algorithms operation algorithms focus mainly evaluation path pattern tree pattern queries paper define partial path pattern query language address efficient evaluation xml data process partial path pattern queries introduce set sound complete inference rules characterize structural relationship derivation provide sufficient conditions detecting query unsatisfiability node redundancy partial path pattern queries equivalently canonical directed acyclic graph form developed stack based algorithms evaluation partial path pattern queries partialmj partialpathstack partialmj computes answers query merge joining results root leaf paths spanning tree query partialpathstack exploits topological nodes query graph match query pattern xml tree experimental evaluation algorithms partialpathstack independent intermediate results outperforms partialmj

paper study effective keyword search xml documents begin introducing notion valuable lowest common ancestor vlca accurately effectively answer keyword queries xml documents propose concept compact vlca cvlca compute meaningful compact connected trees rooted cvlcas answers keyword queries efficiently compute cvlcas devise effective optimization strategy speeding computation exploit key properties cvlca design stack based algorithm answering keyword queries conducted extensive experimental study experimental results proposed approach achieves efficiency effectiveness compared existing proposals

berners lee's compelling vision semantic web hindered chicken egg solved bootstrapping method creating structured data motivate development applications paper argues autonomously semantifying wikipedia solve choose wikipedia initial data source comprehensive quality contains manually derived structure bootstrap autonomous self supervised process identify types structures automatically enhanced wikipedia e.g link structure taxonomic data infoboxes describea prototype implementation self supervised machine learning system realizes vision preliminary experiments demonstrate precision system's extracted data equaling humans

evolutionary clustering method applied multi relational knowledge bases storing semantic resource annotations expressed standard languages semantic web method exploits effective language independent semi distance measure defined space individual resources based finite dimensions corresponding committee features represented concept descriptions discriminating features obtain maximally discriminating features feature construction method based genetic programming algorithm represents clusterings strings central elements medoids w.r.t metric variable length hence clusters parameter method optimize means mutation operators proper fitness function assign cluster newly constructed intensional definition employed concept language experimentation ontologies proves feasibility method effectiveness terms clustering validity indices

resulting management shared distributed knowledge led ontologies employed solution effectively integrate information applications dependent share reuse existing ontologies increased availability ontologies web include thousands concepts novel efficient methods reuse devised achieve efficient ontology reuse process ontology module extraction novel approach ontology module extraction aims achieve efficient reuse ontologies motivation drawn ontology engineering perspective paper provides definition ontology modules reuse perspective approach module extraction based definition abstract graph model module extraction defined module extraction algorithm novel contribution paper module extraction algorithm independent language ontology expressed implemented modtool tool produces ontology modules via extraction experiments conducted compare modtool modularisation methods

artificial intelligence tasks automated question answering reasoning heterogeneous database integration involve verification semantic category e.g coffee drink red color steak drink color novel algorithm automatically validate semantic category contrary methods suggested earlier approach rely manually codified knowledge instead capitalizes diversity topics word usage world wide web tested approach online seeking question answering environment tested trec questions expect answer belong specific semantic category approach improved accuracy 14 depending model metrics

date behaved topic based summarization systems built extractive framework score sentences based associated features manually assigning experimentally tuning weights features paper discuss develop learning strategies obtain optimal feature weights automatically assigning sound score sentence characterized set features fundamental issues training data learning models save costly manual annotation time effort construct training data labeling sentence true score calculated according human summaries support vector regression svr model learn relate true score sentence features relations mathematically modeled svr able predict estimated score sentence evaluations rouge 2 criterion duc 2006 duc 2005 document sets demonstrate competitiveness adaptability proposed approaches

answers seeking questions usually reside factual text nuggets hidden length documents relevance question necessarily correlated relevance length document question previous approaches domain textual question answering document collections quasi unanimously employ document retrieval stage apply widely expensive answer mining techniques subset documents depending collection size 95 documents collection web left selected subset query invisible subsequent processing stages actual answer mining paper introduces model answer retrieval question answering collection distilled offline repositories constitutes potential direct answer questions seeking particular entity relation questions date particular events question answering equivalent online retrieval greatly simplifies de facto system architecture seeking question answering addition simplicity experiments repository acquired approximately billion web documents illustrate impact repositories extracting accurate answers standard evaluation set domain test questions additional sets domain specific questions

commerce intranet search systems require newly arriving content indexed available search minutes hours arrival applications file system email search demand faster turnaround search systems requiring content available search instantaneously incrementally updating inverted indices predominant datastructure search engines expensive operation systems avoid performing rates jiti time indexing component allows searching incoming content nearly soon content reaches system jiti's main idea invest preprocessing arriving data expense tolerable latency query response time designed deployment search systems maintain main index rebuild stop press indices twice hour jiti augments systems instant retrieval capabilities content arriving stop press builds main design jiti demand computational resources particular ram experiments consisted injecting documents queries concurrently system half hour periods believe search applications combination workloads experimented response times measured viable solution pressing

common task database applications migration legacy data multiple sources requires identifying semantically related elements source target systems creation mapping expressions transform instances elements source format target format currently data migration typically manually tedious timeconsuming process difficult scale data sources paper describe quickmig semi automatic approach determining semantic correspondences schema elements data migration applications quickmig advances art set techniques exploiting sample instances domain ontologies reuse existing mappings detect element correspondences mapping expressions quickmig includes mechanisms effectively incorporate domain knowledge users matching process results comprehensive evaluation using real world schemas data indicate quality practicability overall approach

paper svm support vector machine classification system divides contact center call transcripts greeting question refine research resolution closing topic sections call section segmentation useful improve search retrieval functions provide detailed statistics calls shelf automatic speech recognition asr system generate call transcripts recorded calls customers service representatives classify individual utterance call section applying svm classifier merge adjacent utterances classified call section experiment proposed system 100 automatically transcribed calls 10 fold cross validation 87.2 classification accuracy compare proposed algorithm approaches frequent section method maximum entropy based segmentation evaluation system's accuracy 12 baseline system 6 baseline system respectively

paper concerned class imbalance hinder learning performance classification algorithms occurs significantly observations target concept various real world classification tasks medical diagnosis text categorization fraud detection suffer phenomenon standard machine learning algorithms yield prediction performance balanced datasets paper demonstrate active learning capable solving class imbalance providing learner balanced classes propose efficient selecting informative instances pool samples active learning necessitate search entire dataset proposed method yields efficient querying system allows active learning applied datasets experimental results stopping criteria active learning achieves fast solution competitive prediction performance imbalanced data classification

automated text categorization tc prominent progress recent seldom automatic classification library classification systems largest sophisticated classification systems people built dewey decimal classification ddc library classification laborious time consuming job requires qualification training scale classification schemes ddc impose obstacles art tc technologies including deep hierarchy data sparseness skewed category distribution characterize corpora real world applications hard impossible obtain satisfactory results paper propose novel algorithm reconstruct classification schemes according document density category distribution transform category hierarchy balanced virtual taxonomy merging sparse categories lopping dense branches flattening hierarchy classification performance acceptable real world applications propose interactive classification model times user interaction extensive experiments conducted 10 bibliographic data collection library congress verify proposed methodology

clustering common technique extract knowledge dataset unsupervised learning contrast classical propositional approaches focus simple flat datasets relational clustering handle multi type interrelated data objects directly adopt semantic information hidden linkage structure improve clustering result exploring linkage information greatly reduce scalability relational clustering moreover characteristics vector data space utilized accelerate propositional clustering procedure valid relational data space disadvantages restrain relational clustering techniques applied datasets time critical tasks online recommender systems paper propose variance based clustering algorithm address difficulties algorithm combines advantages divisive agglomerative clustering paradigms improve quality cluster results adopting idea representative object executed linear time complexity experimental results algorithm achieves accuracy efficiency robustness comparison relational clustering approaches

paper compares effectiveness query dependent link based ranking algorithms hyperlink induced topic search hits stochastic approach link structure analysis salsa algorithms evaluated web graph induced 463 million crawled web pages set 28,043 queries 485,656 results labeled human judges employed performance measures mean average precision map mean reciprocal rank mrr normalized discounted cumulative gain ndcg found isolated feature salsa substantially outperforms hits surprising algorithms operate neighborhood graph induced query result set studied combination salsa hits bm25f art text based scoring function incorporates anchor text found combination salsa bm25f outperforms combination hits bm25f finally broke query set query specificity found salsa lesser extent hits effective queries

paper consider using block structure web page improve ranking results searching information web sites block structure web pages input propose method computing importance block form block weights web collection experiments deployment method allow significant improvement quality search results ran experiments compare quality search results using method quality obtained using structure information compared ranking method considered pages monolithic units block based ranking method led improvements quality search results experiments sites heterogeneous structures method increase cost processing queries compared systems using structural information

web search engines consistently collect information users interaction system record query issued url selected documents ranking information valuable poll millions users various topics mine users preferences query logs potential partially alleviate search engines thousand searches providing predict answers subset queries users knowing content document predicted result rank analysis confidence user's click redirect user directly page link clicked paper models predicting user clicks ranging specific ones using past user history query ones aggregating data users query former model precision low recall values latter achieve recalls combine models predict accuracy 90 subset query sessions 24 sessions

spatio temporal databases deal geometries changing time geometries change discretely continuously hence dealing moving objects past moving object data models query languages proposed supports historical movements future movements consequently queries start past extend future supported model historical future movements object separate concepts properties required extra attention avoid conflicts furthermore current definitions moving objects vague unclear moving object allowed move space time instance continuity discontinuity motion specified paper propose moving object data model called balloon model provides integrated support historical future movements moving objects model provide formal definitions moving objects respect past future movements kinds queries including past queries future queries queries start past future supported model

time characterizes aspect life management storing querying data paper propose temporal query language called t4sql supporting multiple temporal dimensions data besides valid transaction times encompasses additional temporal dimensions namely availability event times availability time records information treated true information system event times record occurrence times event starts valid time event t4sql capable deal temporal semantics atemporal aka sequenced current sequenced respect temporal dimension moreover t4sql provides novel temporal clause orthogonal management temporal properties defining selection condition schema output relation

correlation analysis basic field data stream mining typical approaches add sliding window data streams recent results window length defined users fixed suitable changing stream environment propose boolean representation based data adaptive method correlation analysis time series streams periodical trends stream series monitored choose suitable window size series trends instead adopting complex pair wise calculation quickly correlation pairs series optimal window sizes processing realized simple boolean operations theory analysis experimental evaluations method computation efficiency accuracy

paper addresses text mining results comprehensible humanities scholars journalists intelligence analysts researchers support analysis text collections system featurelens sup 1 sup visualizes text collection levels granularity enables users explore text patterns current implementation focuses frequent itemsets grams capture repetition exact similar expressions collection users meaningful co occurrences text patterns visualizing documents collection permits users identify temporal evolution usage increasing decreasing sudden appearance text patterns interface explore text features initial studies suggest featurelens helped literary scholar 8 users generate hypotheses insights using 2 text collections

ontology evaluation maturing discipline methodologies measures developed proposed evaluation methods proposed applied specific examples paper art ontology evaluation current methodologies criteria measures analyse appropriate evaluations application browsing wikipedia apply evaluations context ontologies varied properties specifically seek evaluate ontologies based categories found wikipedia

paper introduces wikipedia resource automatic keyword extraction word sense disambiguation online encyclopedia achieve art results tasks paper methods combined system able automatically enrich text links encyclopedic knowledge input document system identifies concepts text automatically links concepts corresponding wikipedia pages evaluations system automatic annotations reliable hardly distinguishable manual annotations

wikipedia grown world largest busiest free encyclopedia articles collaboratively written maintained volunteers online despite success means knowledge sharing collaboration public stopped criticizing quality wikipedia articles edited experts inexperienced contributors paper investigate assessing quality articles collaborative authoring wikipedia propose article quality measurement models interaction data articles contributors derived article edit history lt scp gt asic lt scp gt model designed based mutual dependency article quality author authority lt scp gt eer lt scp gt lt scp gt eview lt scp gt model introduces review behavior measuring article quality finally lt scp gt rob lt scp gt lt scp gt eview lt scp gt models extend lt scp gt eer lt scp gt lt scp gt eview lt scp gt partial reviewership contributors edit various portions articles conduct experiments set labeled wikipedia articles evaluate effectiveness quality measurement models resembling human judgement

previous applications markov random field model information retrieval manually chosen features difficult impossible priori set features task data set develop automatic feature selection techniques paper describe greedy procedure automatically selecting features markov random field model information retrieval propose novel robust method describing classes textual information retrieval features experimental results evaluated standard trec test collections feature selection algorithm produces models significantly effective equally effective models manually selected features past

term frequency normalisation parameter sensitivity issue probabilistic model information retrieval parameter sensitivity indicates slight change parameter value considerably affect retrieval performance weighting model parameter sensitivity robust provide consistent retrieval performance collections queries paper suggest parameter sensitivity due query term weights adequate allow informative query terms informative ones query term reweighing relevance feedback process successfully reduce parameter sensitivity experiments five text retrieval conference trec collections parameter sensitivity remarkably decrease query terms reweighed

implicit feedback algorithms utilize interaction searchers search systems learn users expressed query statements additional information formulate improved queries directly improve retrieval performance paper geometric framework utilizes multiple sources evidence interaction context e.g display time document retention develop enhanced implicit feedback models personalized user tailored search task rich interaction logs associated metadata relevance judgments gathered longitudinal user study relevance stimuli compare implicit feedback algorithm developed using framework alternative algorithms findings demonstrate effectiveness proposed algorithm potential value incorporating multiple sources interaction evidence developing implicit feedback algorithms

study parallelization record linkage i.e identify matching records collections records main idiosyncrasies linkage compared database join records matched merged compared rest records incur matching re feeding stage linkage requires solution iterative complicates significantly discuss plausible scenarios inputs collections clean clean dirty intricate interplay match merge exploit characteristics scenario achieve parallelization parallel algorithms achieve 6.55 7.49 times faster speedup compared sequential ones 8 processors 11.15 18.56 improvement efficiency compared swoosh

fuzzy duplicate detection aims identifying multiple representations real world objects stored data source task critical practical relevance data cleaning data mining data integration history relational data stored single table multiple tables equal schema algorithms fuzzy duplicate detection complex structures e.g hierarchies data warehouse xml data graph data recently emerged algorithms similarity measures consider duplicate status direct neighbors e.g children hierarchical data improve duplicate detection effectiveness paper propose novel method fuzzy duplicate detection hierarchical semi structured xml data unlike previous approaches considers duplicate status children probability descendants duplicates probabilities computed efficiently using bayesian network experiments proposed algorithm able maintain precision recall values dealing data containing amount errors missing information proposal able outperform art duplicate detection system xml databases

goal approximate data matching assess distinct data instances represent real world object usually achieved similarity function returns score defines similar data instances score surpasses threshold data instances considered representing real world object score values returned similarity function depend algorithm implements function meaning user apart similarity value means data instances similar paper propose instead defining threshold terms scores returned similarity function user specifies precision expected matching process precision quality measure interpretation user's view approach relies mapping similarity scores precision values based training data set experimental results training executed representative data set reused databases domain

consider content based spam filtering short text messages arise contexts mobile sms communication blog comments email summary information displayed low bandwidth client short messages consist words challenge traditional bag words based spam filters using corpora short messages message fields derived real sms blog spam messages evaluate feature based compression model based spam filters observe bag words filters improved substantially using features compression model filters perform conclude content filtering short messages surprisingly effective

results merging distributed information retrieval environments approached directions research estimation approaches attempt calculate relevance returned documents ad hoc methodologies weighted score merging regression download approaches download documents locally partially completely estimate hand relevance advantages disadvantages assumed download algorithms effective expensive terms time bandwidth estimation approaches hand usually rely document relevance scores returned remote collections achieve maximum performance addition regression algorithms proved effective weighted scores merging rely significant overlap documents function effectively practically requiring multiple interactions remote collections algorithm introduced reconciles approaches combining strengths minimizing weaknesses based downloading limited selected documents remote collections estimating relevance rest regression methodologies proposed algorithm tested variety settings performance found estimation approaches approximating download

contextual advertising type web advertising url web page aims embed page typically via javascript relevant textual ads available static pages displayed repeatedly matching ads based prior analysis entire content ads matched dynamically created pages processed ahead time analyzing entire body pages fly entails prohibitive communication latency costs solve horned dilemma low relevance latency load propose text summarization techniques paired external knowledge exogenous page craft short page summaries real time empirical evaluation proves matching ads basis summaries sacrifice relevance competitive matching based entire page content specifically found analyzing carefully selected 5 fraction page text sacrifices 1 3 ad relevance furthermore summaries compatible standard javascript mechanisms ad placement produced ad display time simple additions usual script add 500 600 bytes usual request

pseudo relevance feedback query expansion shown improve retrieval performance adhoc retrieval task scenario top ranked documents assumed relevant expand refine initial user query retrieves quality ranking documents little applying query expansion expert search task setting query expansion applied assuming top ranked candidates relevant expertise using expand query nevertheless retrieval improved expected using approach success application query expansion hindered presence topic drift profiles experts system considers demonstrate topic drift occurs expert profiles moreover propose measures predict amount drift occurring expert's profile finally suggest evaluate enhancing query expansion expert search using insights results topic drift anticipated query expansion successfully applied manner expert search task

dictionary based approaches query translation widely cross language information retrieval clir experiments translation limited coverage dictionary affected translation ambiguities paper propose novel method query translation combines types term relation complement dictionary based translation allows extending literal query translation related words produce beneficial effect query expansion clir paper model query translation markov chains mc query translation viewed process expanding query terms semantically similar terms language mc terms relationships modeled directed graph query translation performed random walk graph propagates probabilities related terms framework allows incorporating types term relation languages source target languages addition iterative training process mc allows attribute probabilities target terms related original query offers solution translation ambiguity evaluated method clir benchmark collections obtained significant improvements traditional dictionary based approaches

effective query expansion approaches local feedback able automatically discover query terms improve retrieval accuracy retrieval models performance local feedback heavily dependent assumption top ranked documents relevant query topic assumption sensible ad hoc text retrieval usually violated retrieval tasks multimedia retrieval paper develop robust local analysis approach called probabilistic local feedback plf based discriminative probabilistic retrieval framework proposed model effective improving retrieval accuracy assuming top ranked documents relevant provides sound probabilistic interpretation convergence guarantee iterative result updating process derived variational techniques approach involves iterative process simple operations ranking features computed efficiently practice multimedia retrieval experiments trecvid 03 05 collections demonstrated advantage proposed plf approaches achieve noticeable gains terms mean average precision various baseline methods prf augmented results

investigate evaluate fast efficiently classes optimal route queries massive graph unified framework evaluate route query effectively network partitioned collection fragments distances optimal routes network pre computed setting unified algorithm evaluate classes optimal route queries classes processed efficiently called constraint preserving cp include shortest path forbidden edges forbidden nodes 945 autonomy optimal route query classes prove correctness unified algorithm attention optimization proposed algorithm pruning optimization techniques derived minimize search time accesses empirically techniques effective proposed optimal route query evaluation algorithm techniques incorporated compared main memory disk based brute force cp algorithms experimentally proposed unified algorithm outperforms brute force algorithms term cpu time cost wide margin

regular path queries rpq's means regular expressions matching patterns labeled graphs rpq's received attention context semistructured data data structure irregular partially subject frequent changes databases integration semistructured data multiple sources modeled views paradigm computing view based rewriting query evaluating rewriting view extensions indeed rpq's computing rewriting computationally hard worst 2exptime paper provide practical evidence computing rewriting hard average positive propose automata theoretic techniques efficiently compute utilize instead complement rewriting notably using latter answer query makes view based answering rpq's fairly feasible practice

spatial queries extracting data wireless sensor networks applications environmental monitoring military surveillance query nearest neighbor knn query facilitates sampling monitored sensor data correspondence query location recently itinerary based knn query processing techniques propagate queries collect data pre determined itinerary developed concurrently 12 14 research demonstrate itinerary based knn query processing algorithms able achieve energy efficiency existing algorithms derive itineraries based performance requirements remains challenging paper propose itinerary based knn query processing technique called pciknn derives itineraries aiming optimizing performance criteria response latency energy consumption performance pciknn analyzed mathematically evaluated extensive experiments experimental results pciknn performance scalability art

paper consider adapting statistical classifiers trained source domains labeled examples available target domain labeled example available characteristic domain adaptation examples source domains target domain follow distributions regular classification method tend overfit source domains stage approach domain adaptation lt generalization stage look set features generalizable domains adaptation stage pick useful features specific target domain observing exact objective function hard optimize propose heuristics approximately achieve goal generalization adaptation experiments gene name recognition using real data set effectiveness framework heuristics

unsupervised relation identification task automatically discovering relations entities text corpora relations identified clustering frequently co occurring pairs entities pairs occurring similar contexts belonging clusters paper compare clustering setups novel tried setups include feature extraction selection methods clustering algorithms comparison develop clustering evaluation metric specifically adapted relation identification task experiments demonstrate significant superiority single linkage hierarchical clustering novel threshold selection technique tested clustering algorithms experiments indicate successful relation identification rich complex features kinds features test relation slots relation features features test slot entity features found using kinds features algorithms produces precision results significantly improving previous

database summarization system coined lt scp gt aint lt scp gt lt scp gt ti lt scp gt provides multi resolution summaries structured data stored acentralized database summaries computed online conceptual hierarchical clustering algorithm companies distributed legacy environments consequently current centralized version lt scp gt aint lt scp gt lt scp gt ti lt scp gt feasible privacy preserving desirable resource limitations address propose algorithms generate single summary hierarchy distinct hierarchies scanning raw data greedy merging algorithm gma takes leaves hierarchies generates optimal partitioning considered data set regards cost function compactness separation hierarchical organization summaries built agglomerating dividing clusters cost function emphasize local global patterns data obtain hierarchies according performed optimisation approach breaks due exponential time complexity alternative approaches constant time complexity w.r.t data items proposed tackle called merge incorporation algorithm mia relies lt scp gt aint lt scp gt lt scp gt ti lt scp gt engine whereas approach named merge alignment algorithm maa consists rearranging summaries levels top manner compare approaches using original quality measure quantify merged hierarchies finally experimental study using real data sets merging processes mia maa efficient terms computational time

seek leverage knowledge information organization domain effectively efficiently meet targeted information expert users semantic components model represents document content manner complementary text keyword indexing semantic component instances segments text particular aspect main topic document correspond structural elements document paper describes semantic components model experimental evidence interactive searching study semantic components supplement text keyword indexing extend query language enhanced retrieval domain specific documents response realistic queries posed real users

paper studies effect latent semantic analysis lsa tasks multimedia document retrieval mdr automatic image annotation aia contributions paper twofold knowledge study influence lsa retrieval significant multimedia documents i.e collection 20000 tourist images image representations region based keypoint based combined lsa improve automatic image annotation document collections experiments corel photo collection imageclef 2006 collection

paper describes koru search interface offers effective domain independent knowledge based information retrieval koru exhibits understanding topics queries documents allows expand queries automatically help guide user evolve queries interactively understanding mined vast investment manual effort judgment wikipedia constantly evolving encyclopedia yield inexpensive knowledge structures specifically tailored expose topics terminology semantics individual document collections conducted detailed user study 12 participants 10 topics 2005 trec hard track found koru underlying knowledge base offers significant advantages traditional keyword search capable lending assistance query issued entry efficient improving relevance documents return narrowing gap expert novice seekers

increasingly visible demands structured unstructured information integration advanced analytics conventional database technology able robust practical implementation truly integrated architecture purposes industrial applications particular healthcare life sciences identified fundamental issues technical approaches tackle issues paper propose data representations algebraic operations integrating semantic information e.g ontologies olap systems allow analyze huge set textual documents underlying semantic information performance prototype implementation evaluated using real world datasets scalability flexibility approach confirmed respect computation time

online analytical processing database paradigm provides rich analysis multi dimensional data olap supported logical structure cube supporting efficient olap query resolution enterprise scale environments issue considerable complexity practice difficulty exacerbated existence dimension hierarchies sub divide core dimensions aggregation layers varying granularity common hierarchy sensitive query operations rollup drilldown costly cubes moreover facilities representation complex hierarchical relationships supported conventional techniques paper robust hierarchy infrastructure called mapgraph supports efficient transparent manipulation attribute hierarchies olap environments experimental results verify compared alternatives little additional overhead introduced advanced functionality exploited

database query engines typically rely query size estimators evaluate potential cost alternate query plans multi dimensional database systems typically found data warehousing environments selectivity estimators form multi dimensional histograms single dimensional histograms proven accurate presence data skew multi dimensional variations reliable paper histogram model based tree space partitioning localization tree boxes controlled hilbert space filling curve series efficient equalization heuristics restructures initial boxes provide improved bucket representation experimental results demonstrate significantly improved estimation accuracy relative art alternatives superior consistency variety record distributions

challenging implicit reliance document collections paper discusses pros cons using query logs document collections self contained sources data textual information extraction differences quantified scale study extracting prominent attributes quantifiable properties classes e.g top speed price fuel consumption carmodel unstructured text head head qualitative comparison lightweight extraction method produces class attributes 45 accurate average acquired query logs web documents

conceptual indexing produce index multilingual documents inter media conceptual indexing promotes common concepts media single index media paper explore advance indexing view benefit automatic conceptual indexing texts extension text image documents tests conducted multilingual image text medical document corpus clef initiative obtain results text 2005 2006 promising results images results combination image text

researchers shown various natural language processing techniques document analysis impact search performance examined analysis system performance characteristics leveraged improve document passage search results previously shown semantic queries utilize named entity relation information extracted corpus lead significant improvement search performance paper extend previous efforts examine search performance degrades imperfect named entity relation extraction study carried developing gold standard annotated corpora applying error models gold standard annotations simulate errors automatic recognizers identify automatic recognizer characteristics amenable search tasks recognizer recall significant impact semantic search performance precision demonstrate significant improvement map exact precision scores achieved adopting automatic named entity relation recognizers near art performance

activity centric collaboration environments help knowledge workers manage context shared activities providing representation activity resources activity management systems provide structure organization email execute shared activity shared activities increases difficult users focus activities attention paper describes personalized activity prioritization approach implemented top lotus connections activities management system prototype implementation allows user view activities predicted priorities predictions using ranking support vector machine model trained user's past interactions activities system describe prioritization interface results offline experiment based data 13 users 6 months results feature set derived shared activity structures significantly increase prediction accuracy compared recency baseline

user distinct background specific goal searching information web goal web search personalization tailor search results particular user based user's preferences effective personalization information access involves challenges accurately identifying user context organizing information matches particular context approach personalized search involves building models user context ontological profiles assigning implicitly derived scores existing concepts domain ontology spreading activation algorithm maintain scores based user's ongoing behavior experiments re ranking search results based scores semantic evidence ontological user profile effective relevant results user

world wide web growing changing astonishing rate web information systems search engines growth change web due resource constraints search engines usually difficulties keeping local database completely synchronized web paper study tomake limited system resource detect changes towards goal crawler web search engine able predict change behavior webpages propose applying clustering based sampling approach specifically local webpages clusters cluster contains webpages similar change pattern sample webpages cluster estimate change frequency webpages cluster finally crawler re visit cluster containing webpages change frequency probability evaluate performance incremental crawler web search engine measure freshness quality query results provided search engine run extensive experiments real web data set 300,000 distinct urls distributed 210 websites results demonstrate clustering algorithm effectively clusters pages similar change patterns solution significantly outperforms existing methods detect changed webpages improve quality user experience query search engine

pagerank algorithm web information retrieval calculate single list popularity scores page web popularity scores rank query results user using structure entire web calculate score document calculating popularity score particular community pagerank scores suited queries paper introduce form pagerank using web multi resolution community based popularity scores document obtains popularity score dependent web community query related specific community choose associated set popularity scores query results accordingly using web community based popularity scores achieved 11 increase precision pagerank

query biased web page summarization summarization web page reflecting relevance specific query plays role search results representation web search engines paper propose learning based query biased web page summarization method summarization solved typical sentence selection framework existing web page summarization methods page content link context considered sources sentences existing learning based summarization methods treat summarization sentence classification train classifier discriminate extracted sentences extracted sentences training documents basic assumption methods sentences documents comparable respect class information contrast classification scheme ranking scheme introduced rank extracted sentences extracted sentences training document underlying assumption sentences document comparable weaker reasonable assumption classification based scheme extensive results using intrinsic evaluation metrics gauge aspects proposed method

social networks orkut www.orkut.com portion user queries refer names people indeed 50 queries orkut names users average 1.8 terms query users usually search people whom maintain relationships network relationships modelled edges friendship graph graph nodes represent users context search ranking modelled function depends distances users graph specifically shortest paths friendship graph application idea ranking straightforward size modern social networks dozens millions users prevents efficient computation shortest paths query time overcome designing ranking formula strikes balance producing results reducing query processing time using data orkut social network includes 40 million users ranking augmented signal produces quality results maintaining query processing time

link graph analysis tools devices boost richness information retrieval systems internet existing social networking portals couple situations tools beneficial enriching users analysts integrating data sources performance generic tools odds continuously growing size data repositories paper propose evaluate dex performance graph database querying system allows integration multiple data sources dex makes graph querying flavors including link analysis social network analysis pattern recognition keyword search richness dex experiments carried internet movie database imdb variety complex analytical queries dex generic efficient tool graph databases

heterogeneous entities objects common usually interrelated scenarios example typical web search activities involve multiple types interrelated entities users web pages search queries paper define study novel lt ul gt lt ul gt earch lt ul gt lt ul gt eterogeneous lt ul gt lt ul gt terrelated lt ul gt lt ul gt ntities shine shine query type entities task shine retrieve multiple types related entities answer query contrast traditional search deals single type entities e.g web pages advantages shine include 1 feasible users specify information dimensions accepting queries types 2 answering query multiple types entities provides informative context users understand search results facilitate information exploration 3 multiple relations heterogeneous entities utilized improve ranking particular type entities attain goal shine propose represent entities unified space utilizing interaction relationships approaches lsa vsm discussed compared paper experiments 3 data sets i.e literature data set search engine log data set recommendation data set effectiveness flexibility proposed methods

topological information plays fundamental role human perception spatial configurations thereby prominent geographical features natural language vagueness abounds geography flexible formalisms ability capture vague topological information practice formalisms introduced various authors complete reasoning procedures usually discussed paper reasoning tasks consistency checking entailment checking supported generalization rcc 8 calculus particular decision procedures based linear programming solving reasoning tasks furthermore deciding consistency vague topological information reduced consistency original rcc 8

queries issued casual users specialists exploring dataset subsets data clusters outliers meaningful features capturing caching queries henceforth called nuggets potential benefits including optimization system performance search experience users unfortunately current visual exploration systems tapped potential resource identifying sharing queries paper introduce query consolidation strategy aimed solving isolating queries potentially huge amount queries submitted solution clusters redundant queries caused exploration style query specification prevalent data exploration systems measure similarity queries designed effective distance metric incorporates query specification actual query result overcome complexity comparing queries result sets designed approximation method efficient providing excellent accuracy user study conducted multivariate data sets comparing proposed technique literature confirms proposed distance metric indeed matches users intuition proof feasibility integrated proposed query consolidation solution nugget management system nms framework 22 based visual exploration system xmdvtool user study indicates efficiency accuracy users visual exploration enhanced supported nms

methods improving sponsored search revenue tested deployed submarket larger marketplace applications ideal submarket contains nodes amount spending submarket amount spending leaving submarket introduce efficient algorithm finding submarkets optimal user specified tradeoff quantities apply algorithm submarkets dense isolated spending graph yahoo sponsored search

information retrieval ir researchers commonly tests statistical significance student's paired test wilcoxon signed rank test sign test researchers previously proposed using bootstrap fisher's randomization permutation test parametric significance tests ir tests seen little five tests ad hoc retrieval runs submitted trecs 3 5 8 pair runs measured statistical significance difference mean average precision discovered little practical difference randomization bootstrap tests wilcoxon sign test poor ability detect significance potential lead false detections significance wilcoxon sign tests simplified variants randomization test discontinued measuring significance difference means

recent shown average precision accurately estimated random sample judged documents unfortunately random pools evaluate retrieval measures standard estimates average precision accurately infer relevances remaining unjudged documents obtaining judged pool standard system evaluation kinds using trec data demonstrate inferred judged pools correlated assessor judgments demonstrate inferred pools accurately infer precision recall curves commonly measures retrieval performance

information retrieval experimentation proceeds cycle development evaluation hypothesis testing ideally evaluation testing phases short easy maximize amount time spent development recent reducing amount assessor effort evaluate retrieval systems investigated effects methods tests significance explore detail effects reduced sets judgments sign test demonstrate analytically empirically relationship power test topics evaluated judgments available using relationships determine topics judgments cost confidence significance evaluation specifically testing pairwise significance 192 topics fewer 5 judgments testing significance 25 topics average 166 judgments 85 effort producing additional errors

simple efficient external perfect hashing scheme referred eph algorithm static key sets techniques literature obtain novel scheme theoretically understood time achieves magnitude increase size solved compared previous practical methods demonstrate scalability algorithm constructing minimum perfect hash functions set 1.024 billion urls world wide web average length 64 characters approximately 62 minutes using commodity pc scheme produces minimal perfect hash functions using approximately 3.8 bits key perfect hash functions range 0 2n 1 space usage drops approximately 2.7 bits key main contribution algorithm experimentally proven practicality sets billions keys time space usage carefully analyzed unrealistic assumptions

promising technology unique properties efficiency scalability fault tolerance peer peer p2p technology underlying network build internet scale applications issues application example www distribution data popularities heavily tailed zipf distribution consideration skewed popularity adopt proactive caching approach handle challenge focus key i.e placement strategy replicas i.e degree replicas assigned specific content propose novel approach applied structured p2p networks solve optimization objectives related max_perf min_cost solution called lt gt popcache lt gt discover properties 1 replicas assigned content proportional popularity 2 derived optimal solutions related entropy popularity knowledge none previous mentioned results finally apply results popcache propose p2p base web caching called web popcache means web cache trace driven simulation extensive evaluation results demonstrate advantages popcache web popcache

recent study native twig join algorithms tree aware relational framework significantly outperform tree unaware approaches evaluating structural relationships xml twig queries paper efficient strategy evaluate selective twig queries containing parent child relationships tree unaware relational environment scheme built top lt scp gt ucxent lt scp gt system exploiting encoding scheme lt scp gt ucxent lt scp gt devise efficient strategy evaluating twig queries extensive performance studies various data sets queries approach performs representative tree unaware approach lt scp gt lobal lt scp gt lt scp gt rder lt scp gt art native twig join algorithm tjf lt scp gt ast lt scp gt benchmark queries observed gain factors 243 95 respectively additionally approach reduces significantly performance gap tree aware tree unaware approaches outperforms tree aware approach lt scp gt onet lt scp gt db xq lt scp gt uery lt scp gt selective twig queries report insights plan choices relational optimizer twig query evaluation visually characterizing behavior relational selectivity space

seed based framework textual information extraction allows weakly supervised extraction named entities anonymized web search queries extraction guided set seed named entities handcrafted extraction patterns domain specific knowledge allowing acquisition named entities pertaining various classes web search users inherently noisy search queries shown highly valuable albeit little explored resource web based named entity discovery

user centric entity detection system primary consumer detected entities person perform actions detected entities e.g perform search view map shop contrast machine centric detection systems primary consumer detected entities machine machine centric detection systems typically focus quantity detected entities measured precision recall metrics goal correctly identifying single entity document simple precision recall scores machine centric entity detection systems fail accurately reflect quality detected entities user centric systems users necessarily entity posit detected entities piece text necessarily relevant main topic text nor necessarily user warrant action detected entities user annoy user decides capability completely undesirable outcome propose measure quality utility user centric entity detection systems core dimensions accuracy interestingness relevance entities user leveraging surrounding context greatly improve performance systems dimensions employing novel algorithms generating concept vector finding concept extensions using search query logs extensively evaluate proposed algorithms contextual shortcuts scale user centric entity detection platform using 1,586 entities detected 1,519 documents results confirm importance using context user centric entity detection systems validate usefulness proposed algorithms improve overall entity detection quality contextual shortcuts

paper type nanotheories tn framework representing knowledge performing similarity comparisons pairs terms type tn methodology namely support outcomes introduced ir nlp applications redundancy factor increase confidence tn based comparisons determine redundancy simple string comparisons results include 14 increase confidence weighted score qa system 68 improvement baseline answer key equivalencing experiment

shown using phrases properly document retrieval leads retrieval effectiveness paper define types noun phrases algorithm recognizing phrases queries strengths existing tools combined phrase recognition algorithm tested using set 500 web queries query log set 238 trec queries experimental results algorithm yields phrase recognition accuracy baseline noun phrase recognition algorithm recognize phrases trec queries document retrieval experiment conducted using trec queries 1 phrases 2 phrases recognized baseline noun phrase recognition algorithm 3 phrases recognized algorithm respectively retrieval effectiveness 3 2 1 demonstrates utilizing phrases queries improve retrieval effectiveness noun phrase recognition yields retrieval performance

traditional adaptive filtering systems learn user's simple words relevant documents favored query model words irrelevant documents weighted biases query model towards specific words seen past causing system favor documents containing relevant redundant information documents previously unseen words denote news event paper proposes news generalizing relevance feedback augmenting traditional bag words query model named entity wildcards anchored context wildcards allows generalization beyond specific words contextual restrictions limit wildcard matching entities related user's query test approach nugget level adaptive filtering system evaluate terms relevance novelty information results indicate recall obtained lexical terms generalized using wildcards wildcards anchored context maintain precision context wildcard represented matched document plays crucial role performance retrieval system

aspect retrieving named entities retrieving documents items retrieved persons locations organizations indirectly described documents throughout collection dedicated finding references named entities particular named entity extraction disambiguation retrieval performance snippets text combined build named entity representations focus trec expert search task goal identify people knowledgeable specific topic existing language modeling techniques expert finding assume terms person entities conditionally independent document theoretical experimental evidence simplifying assumption ignores information named entities relate document content address issue propose document representation emphasizes text proximity entities incorporates sequential information implicit text experiments demonstrate proposed model significantly improves retrieval performance main contribution effective formal method explicitly modeling dependency named entities terms appear document

consider document indexing representation recently locality preserving indexing lpi proposed learning compact document subspace latent semantic indexing lsi optimal sense global euclidean structure lpi optimal sense local manifold structure lpi efficient time memory makes difficult applied data set specifically computation lpi involves eigen decompositions dense matrices expensive paper propose algorithm called regularized locality preserving indexing rlpi benefit recent progresses spectral graph analysis cast original lpi algorithm regression framework enable avoid eigen decomposition dense matrices regression based framework kinds regularizers naturally incorporated algorithm makes flexible extensive experimental results rlpi obtains similar results comparing lpi significantly faster makes efficient effective data preprocessing method scale text clustering classification retrieval

previous line index maintenance strategies mainly designed document insertions considering document deletions truly dynamic search environment documents added removed collection time paper examine issues line index maintenance support instantaneous document deletions insertions dbt merge strategy dynamically adjust sequence sub index merge operations index construction offers query processing performance previous methods providing equivalent level index maintenance performance document insertions deletions exist parallel using experiments 426 gb web data demonstrate efficiency method practice line index construction dynamic text collections performed efficiently fast growing text collections

index compression techniques substantially decrease storage requirements text retrieval system effect increase retrieval performance reducing disk overhead despite advantage developers sometimes choose store index data uncompressed form obstruct random access index term's postings list paper index compression harm random access performance demonstrate random access term's postings list realized efficiently list stored compressed form instead uncompressed regardless index stored disk main memory types storage hard drives ram support efficient random access

modern web search engines expected return top results efficiently query dynamic index pruning strategies proposed efficient top computation prone ignore especially factors ranking functions e.g term proximity distance relationship query terms document inclusion term proximity breaks monotonicity ranking functions leads additional challenges efficient query processing paper studies performance existing top computation approaches using term proximity enabled ranking functions investigation demonstrates term proximity incorporated ranking functions existing index structures top strategies inefficient according analysis experimental results propose index structures corresponding index pruning strategies structured hybrid performs settings moreover efficiency index building maintenance affected approaches

association rule mining achievements knowledge discovery quality extracted association rules concern quality extracted association rules huge size extracted rule set matter tens thousands association rules extracted redundant useless mining redundant rules promising approach solve min max exact basis proposed pasquier et al pasquier05 exciting results generating redundant rules paper propose relaxing definition redundancy min max exact basis contains redundant rules propose condensed representation called reliable exact basis exact association rules rules reliable exact basis redundant succinct rules min max exact basis prove redundancy eliminated reliable exact basis reduce belief reliable exact basis size reliable exact basis min max exact basis moreover prove exact association rules deduced reliable exact basis reliable exact basis lossless representation exact association rules experimental results reliable exact basis significantly reduces redundant rules

novel algorithm named dolphin detecting distance based outliers proposed algorithm performs sequential scans dataset store main memory portion dataset efficiently search neighbors prune inliers strategy pursued algorithm allows portion theoretical justification empirical evidence size stored data amounts percent dataset provided feature dolphin memory resident data indexed using suitable proximity search approach allows search nearest neighbors looking subset main memory stored data temporal spatial cost analysis novel algorithm achieves near linear cpu cost dolphin compared art methods outperforms existing ones

real life data stream usually contains dimensions dimensional values data elements missing effectively extract change data stream respect subsets dimensions data stream grid based subspace clustering algorithm proposed paper dimensional data stream distribution statistics data elements dimension data space firstly monitored list grid cells called sibling list dense grid cell level sibling list dense unit grid cell level sibling lists created child nodes trace cluster dimensional rectangular subspaces sibling tree grows sup th sup level dimensional subcluster found sup th sup level sibling tree proposed method comparatively analyzed series experiments identify various characteristics

method detecting distance based outliers data streams deal sliding window model outlier queries performed detect anomalies current window algorithms exactly answers outlier queries larger space requirements algorithm directly derived exact limited memory requirements returns approximate answer based accurate estimations statistical guarantee experiments accomplished confirming effectiveness proposed approach quality approximate solutions

news reports produced disseminated overwhelming volume difficult information previous research automatic news organization treated news topics flat list ignoring intrinsic connection individual reports argue contextual information topics benefit users news understanding process news organization infrastructure incident threading proposed article text snippets describing occurrence real world happening combined news incident network composed incidents interconnected links types limited vocabulary connection types defined corresponding rules established based human experience news understanding incident threading system implemented algorithms starts clustering text passages creates links pre built rules method defines global score function collection solves optimization simulated annealing former achieves accuracy identification incidents latter generates links preferred links formation incident network

opinion retrieval document retrieval process requires documents retrieved ranked according opinions query topic relevant document satisfy criteria relevant query topic contains opinions query matter positive negative paper describe opinion retrieval algorithm traditional information retrieval ir component topic relevant documents document set opinion classification component documents opinions results ir step component rank documents based relevance query degrees opinions query implemented algorithm system tested using trec 2006 blog track data automatic title runs result 28 32 improvements map score automatic runs 2006 track result 13 art opinion retrieval system tested data set

paper evaluates uptake efficacy unified approach phrasal query suggestions context precision search engine search engine performs ranked extended boolean searches proximity operator lt scp gt near lt scp gt default operation suggestions offered searcher length result list falls outside predefined bounds list engine suggests narrowing query super phrases list short engine suggests broadening query proximal subphrases evaluated uptake phrasal query suggestions analyzing search log data suggestion feature added commercial version search engine looked approximately 1.5 million queries found added suggestions represented nearly 30 total queries evaluated efficacy controlled study 24 participants performing nine searches using search engines found engine phrase suggestions precision recall search engine suggestions search engine similar interface using okapi bm25 ranking algorithm

describe architecture automatic domain specific web portal construction system system major components focused crawler collects domain specific pages web ii information extraction engine extracts useful fields web pages iii query engine allows typical keyword based queries pages advanced queries extracted data fields prototype system course homepages domain web user study prototype system approach produces quality results achieves precision figures typical keyword based search

paper proposes measure relevance likelihood derived specifically language models measure guide user browse list retrieved items pseudo relevance feedback derive measure assumption user seeking ideal usually existent document actual relevant documents collection contain fragments ideal document deriving measure propose novel capturing relevance language modelling

efficient realization following interactive search engine feature user typing query words related query word lead hits suggested selected hits realization building clusters related terms ii adding information artificial words index iii described feature reduces instance prefix search completion efficient solution latter provided completesearch engine integrated proposed feature building clusters related terms propose variant latent semantic indexing unlike standard approaches completely transparent user experiments test collections demonstrate feature provided slight increase query processing time index size

text categorization applications require representation beyond standard bag words paradigm kernel based learning approached considering information syntactic structure ii incorporating knowledge semantic similarity term features propose generalized framework consisting family kernels jointly incorporate syntactic semantic similarity demonstrate power approach series experiments

metaphor merely rhetorical device characteristic language fundamental feature human conceptual system metaphor understood finding analogy mapping domains paper argues analogy mappings facilitate conceptual modeling allowing designer reinterpret fragments familiar conceptual models contexts contributions paper expressed tradition entity relation model

paper define type cohesive subgroups called communities hypergraphs based edge connectivity subhypergraphs describe simple algorithm construction sets based examples image segmentation information retrieval useful analysis accessibility graphs hypergraphs

semiautomatic evaluation retrieval systems using document similarities

improving classification newsgroup messages social network analysis paper focus automatic classification message replies types representing messages consider rich feature sets combine standard author reply network properties features derived additional structures identified data 1 network authors participate threads 2 network authors post similar content 3 network threads sharing common authors 4 network content related threads selected newsgroups train linear svm classifiers identify agreement disagreement original message question answer patterns threads newly defined features substantially improves classification messages comparison svm model based standard reply network

searching organization's document repositories experts frequently faced intranet information management paper proposes candidate centered model referred candidate description document cdd based retrieval model expertise evidence expert candidate scattered repositories mined aggregated automatically form profile called candidate's cdd represents knowledge model foundations logical development argue favor model expert finding devise compare strategies exploring variety expertise evidence experiments trec enterprise corpora demonstrate cdd based model achieves significant consistent improvement performance comparative studies cdd methods

previous researches filed xml databases evaluate xml queries branches little examined efficient processing xml queries predicates methods process query nodes document dealing queries branch paper modification tjfast method propose manner answering various queries method processes nodes efficiently ideal obtain answer process node don't unreasonable processing node

paper novel technique performing document genre identification utilizing genre producing tailored summaries based user's information seeking genre oriented goal focused summarization plot opinion summary movie review create test corpus determine genre classification accuracy 16 genres examine performance various amounts training data machine learning algorithms random forests svm light na 239 ve bayes results random forests outperforms svm light na 239 ve bayes genre tag inform downstream summarization engine define types summaries 7 genres create ground truth corpus analyze results genre oriented goal focused summarization type user based summarization requires algorithms leading sentence baseline perform news articles

main objective xml retrieval select relevant elements xml document instead document issues appear considering relevance feedback rf xml documents mainly related form xml documents mix content structure information information granularity paper flexible method relevance feedback xml retrieval using sources evidence described propose context criterion select terms extend initial query generative structures express structural constraints approaches applied combined forms experiments carried inex evaluation campaign results effectiveness approach

propose model feature evaluation selection assesses propensity features support set classification item data set collection features induce ranking list remaining items evaluation criterion favors features result consistent discrimination relevant relevant items ranked lists discrimination boundaries single list determined combinatorially according degree correlation relevant sets model makes special assumptions nature data selection heuristic based model proposed using sequential forward generation experimental comparison unsupervised feature selection methods

existing research blogs focused posts ignoring comments user study conducted summarizing blog posts reading comments change one's understanding blog posts research aim extract representative sentences blog post represent topics discussed comments proposed solution derives representative words comments selects sentences containing representative words representativeness words measured using requt i.e reader quotation topic evaluated human labeled sentences requt summation based sentence selection promising results

increasing amount data stored xml format olap queries data olap queries studied relational database systems evaluation olap queries xml data trivial extension relational solutions especially schema available paper introduce ix cube iceberg xml cube xml data tackle extend olap operations xml data develop efficient approaches ix cube computation olap query evaluation using ix cubes

investigate subtle cues user identity exploited attacks privacy users web search query logs study application simple classifiers map sequence queries gender age location user issuing queries classifiers carefully combined multiple granularities map sequence queries set candidate users 300 600 times random chance allow approach remains accurate removing personally identifiable information names limiting size query log attack real world acquaintance user attempts identify user query log using personal information combinations pieces information terms user probably search highly effective identifying sessions user conclude schemes release heavily scrubbed query logs contain session information significant privacy risks

efficient management multiversion data branched evolution crucial applications requires database designers aware tradeoffs index structures policies paper defines framework analysis method understanding behavior indexing policies data query characteristics analysis allows determining suitable index structure analysis validated experimental study

question answer portals naver yahoo answers quickly becoming rich sources knowledge topics served web search engines unfortunately quality submitted answers uneven ranging excellent detailed answers snappy insulting remarks advertisements commercial content furthermore user feedback topics sparse insufficient reliably identify answers bad ones hence estimating authority users crucial task emerging domain potential applications answer ranking spam detection incentive mechanism design analysis link structure purpose question answering community discover authoritative users promising experimental results dataset 3 million answers popular community qa site describe structural differences question topics correlate success link analysis authority discovery

noabstract

recent bagging method applied learn bayesian networks bns especially limited datasets bns learned using bagging method limited datasets biased towards complex models efficient approach produce accurate bns limited datasets based markov condition bn learning proposed novel sampling method called root nodes based sampling rns bns fusion method experimental results reveal ensemble method achieve accurate results terms accuracy limited datasets

inspired search behavior human society propose cto self organized semantic overlay based concept tree p2p ir infrastructure efficient text search pure p2p environment central control powerful peer hub node especially cto performs searching unpopular resources shared peers experiment searching scarce documents shared peers cto achieves 80 recall rate search covers 5 peers overlay search latency cto low controlled range 5 12 hops

method parallel query processing allows scalable performance distributed inverted files method allows realization hybrid combines advantages document term partitioned inverted files

visualizing network data tree structures arbitrarily connected graphs difficult information visualization network data users visualize attributes specific data item links specifying items connected past approaches resolving difficulties focus zooming clustering filtering applying various methods laying nodes edges approaches focus optimizing network visualization single view limiting amount information shown explored parallel moreover past approaches allow users cross reference subsets aspects complex networks paper propose approach limitations using multiple coordinated views network illustrate approach implement tool called dualnet evaluate tool study using email communication network using multiple coordinated views improves navigation provides insight networks multiple node link properties types

emails commonly modern communication media days unsolicited emails obstruct otherwise fast convenient technology information exchange jeopardize continuity popular communication tool waste valuable resources time exposure offensive content arise result junk emails addition monetary cost processing junk emails reaches billions dollars absorbed public users internet service providers extensive past dedicated eradicate junk emails none existing junk email detection approaches highly successful solving spammers able infiltrate existing detection techniques paper tool junex relies content similarity emails eradicate junk emails junex compares incoming email core emails marked junk individual user identify unwanted emails reducing legitimate emails treated junk critical conducted experiments junex verify accuracy

consider distributed information system allows autonomous consumers query autonomous providers focus query allocation view considering consumers providers satisfaction addition query load define satisfaction run notion based consumers providers preferences propose validate mediation process called sbmediation compared capacity based query allocation experimental results sbmediation significantly outperforms capacity based confronted autonomous participants

applications create consume unstructured data grown scale storage requirements complexity search primitives consider applications exhaustive search integration structured unstructured data current block based storage systems incapable inefficient address challenges bought forth applications propose storage framework efficiently store search unstructured structured data controlling storage management costs experimental results based prototype proposed system provide impressive performance feature benefits

query unlively returns empty answer debugging database schema requires determining unlively queries fixing knowledge existing methods provide designer explanation query lively paper propose method computing explanations independent particular method determine liveliness provides levels search explanation maximal set overlapping explanations explanations levels require linear calls underlying method propose filter reduce calls experimentally compare method method finding unsatisfiable subsets constraints

corpus based approach class expansion task set seed entities co occurrence statistics text collection define membership function rank candidate entities inclusion set describe evaluation framework data wikipedia performance class extension method improves size text collection increases

trace changes association rules online data stream efficiently paper proposes methods generating association rules directly changing set currently frequent itemsets currently frequent itemsets monitored estdec method association rules frequent itemset prefix tree estdec method generated purpose traversal stack introduced efficiently enumerate association rules online methods avoid drawbacks conventional step approach line environment user finding association rules antecedents consequents fixed specific itemset generating association rules produce timely additional methods namely assoc assoc introduced finally proposed methods compared series experiments identify various characteristics

computing similarity unstructured records fundamental function multiple applications approximate string matching text retrieval techniques performance applied directly information limited unstructured records short record length paper propose novel probabilistic correlation based similarity measure simply conducting exact matching tokens records similarity evaluation enriches information records considering correlations tokens define probabilistic correlation tokens probability tokens appear records compute weight tokens discover correlations records based probabilistic correlations tokens finally extensive experimental results demonstrate effectiveness approach

opinion leaders bring information ideas opinions disseminate masses influence opinions decisions fashion word mouth opinion leaders capture representative opinions social network consequently understanding massive complex blogosphere paper propose novel algorithm called influencerank identify opinion leaders blogosphere influencerank algorithm ranks blogs according compared blogs novel information contribute network experimental results indicate proposed algorithm effective identifying influential opinion leaders

paper proposes semi supervised distance metric learning algorithm ranking instead giving computer factors affect final rank value ranked implicitly contain knowledge ranking factors computer automatically plenty unlabeded data learn informative metric ranking metric help regress observed data retrieve data querying test moreover lower rank distance metric visualize dimensional data application housing potential estimation shown algorithm efficient help consultants refine consulting

attempt tackle domain transfer combining domain labeled examples domain unlabeled ones basic idea domain trained classifier label informative unlabeled examples domain retrain base classifier selected examples experimental results demonstrate proposed scheme significantly boost accuracy base sentiment classifier domain

compression term frequency lists document id lists inverted file search engine examined compression schemes compared including elias 947 948 codes golomb encoding variable byte encoding class word based encoding schemes including simple 9 relative 10 carryover 12 shown compression methods suited compressing kinds lists tested carryover 12 preferred effective compression fast decompression novel technique sigma encoding prior compression proposed tested sigma encoding utilizes parameterized dictionary reduce bits store integer method 0.3 bit integer improvement carryover 12 costing 3 extra clock cycles integer decompress

ram dynamic pruning schemes reduce query evaluation times portion lists processed dynamic pruning current systems store entire inverted list cache paper investigate caching pieces inverted lists actually answer query dynamic pruning examine lru cache model recently proposed models introduce dynamic pruning scheme impact inverted lists using web collections corresponding query logs using lru cache pruning scheme reduces disk accesses query processing time 7 15 art impact baseline reducing answer quality surprisingly discover using pruning scheme makes little difference disk traffic sophisticated caching schemes employed

improve search accuracy difficult topics addressed research question paper consider scenario search results poor none top ranked documents relevant user's query propose exploit negative feedback improve retrieval accuracy difficult queries specifically propose learn top ranked relevant documents rerank rest unseen documents propose approaches penalizing documents similar relevant documents language modeling framework evaluate proposed methods adapt standard trec collections construct test collection containing difficult queries experiment results proposed approaches effective improving retrieval accuracy difficult queries

classic techniques image annotation language translation model views image document i.e set visual words obtained vector quatitizing image regions generated unsupervised image segmentation annotating images achieved translating visual words textual words translating document english document french paper view image document view annotation processes consecutive processes i.e document summarization translation document summarization process image document firstly summarized own visual language called visual topics translation process translates visual topics textual words compared original translation model visual topics learned probabilistic latent semantic analysis plsa approach provide intermediate abstract level visual description improved annotation performance corel image dataset

conventional question answering qa techniques independently process candidate bearing snippets select exact answer question candidate answers paper novel utilizing redundancy candidate bearing snippets help select exact answer question web qa system i.e cluster based language model clm unsupervised svm classifier svm techniques comparative experiments demonstrate proposed methods significantly outperform language model based lm supervised svm based svm techniques utilize redundancy candidate bearing snippets using clm top_1 score increased 36.03 lm 46.96 top_1 improvement svm svm 23 moreover cross model comparison performance ranking models svm gt clm lm gt lm gt svm gt retrieval based model

poster concerned exploring social annotations improving language models information retrieval denoted lmir properties social annotations namely keyword property structure property studied aim keyword property improves lmir concatenating annotations document generate summary document structure property boost lmir similarity annotations similarity documents consideration simultaneously properties social annotations leveraged language modeling mixture model named language annotation model denoted lam evaluations using del.icio.us data lam outperforms traditional lmir approaches significantly

keyword search xml documents based notion lowest common ancestors lcas modifications recently gained research 2 3 4 paper propose efficient algorithm called indexed stack answers keyword queries based xrank's semantics lca 2 complexity indexed stack algorithm kd s1 log keywords query depth tree s1 occurrence frequent keyword query comparison worst complexity core algorithms 2 kd analytically experimentally evaluate indexed stack algorithm core algorithms 2 results indexed stack algorithm outperforms terms cpu costs algorithms magnitude query contains low frequency keyword frequency keywords

link analysis key technology contemporary web search engines previous link analysis information snapshot web graph commercial search engines crawl web periodically naturally obtain time series data web graphs historical information contained series web graphs improve performance link analysis paper argue page importance dynamic quantity propose defining page importance function pagerank current web graph accumulated historical page importance previous web graphs specifically novel algorithm named temporalrank designed compute proposed page importance try kinetic model interpret page importance regarded solution ordinary differential equation experiments link analysis using web graph data five snapshots proposed algorithm outperform pagerank measures effectively filter newly appeared link spam websites

discuss ranking entities types particular deal heterogeneous set types generic specific discuss approaches exploiting entity containment graph ii using web search engine compute entity relevance evaluate approaches real task ranking wikipedia entities typed art named entity tagger results approaches greatly increase performance methods based passage retrieval

paper concerned name disambiguation name disambiguation mean distinguishing persons name critical knowledge management applications despite research conducted resolved serious particular popularity web 2.0 previously name disambiguation undertaken supervised unsupervised fashion paper constraint based probabilistic model semi supervised name disambiguation specifically focus investigating academic researcher social network http arnetminer.org framework combines constraints euclidean distance learning allows user refine disambiguation results experimental results researcher social network proposed framework significantly outperforms baseline method using unsupervised hierarchical clustering algorithm

propose efficient algorithm approximate biased quantile computation data streams algorithm computes decomposable biased quantile summaries fixed sized blocks dynamically maintains biased quantile summary entire stream exponential histogram block wise quantile summaries algorithm computationally efficient achieves amortized computational cost log sup 1 sup 8260 8712 log 8712 space requirement sup log3 sup 8712 8620 8712 algorithm assume prior knowledge stream sizes range data values streams practice algorithm able efficiently maintain summaries data streams tens millions observations achieves significant performance improvement prior algorithms

hidden markov model hmm frequently pinyin chinese conversion captures dependency preceding character markov models bring accuracy computationally unaffordable average pc settings propose segment based hidden markov model shmm magnitude complexity hmm generates decoding accuracy shmm tells word bigram connecting words assigns reasonable probability words powerful hmm decode words containing characters conduct comprehensive pinyin chinese conversion evaluation lancaster corpus experiment perfect sentence accuracy improved 34.7 hmm 43.3 shmm error sentence accuracy increased 72.7 78.3 furthermore shmm seamlessly integrate pinyin typing correction acronym pinyin input user defined words self adaptive learning commercial pinyin chinese conversion product improve efficiency pinyin input

noabstract

